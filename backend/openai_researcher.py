"""
OpenAI Responses API + Web searchã‚’ä½¿ç”¨ã—ãŸè¨˜äº‹å–å¾—ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
"""
import os
import re
import time
from typing import List, Dict, Optional
from datetime import datetime
from urllib.parse import urlparse
from openai import OpenAI, NotFoundError, BadRequestError
import feedparser


class OpenAIResearcher:
    """OpenAI Responses API + Web searchã‚’ä½¿ç”¨ã—ã¦è¨˜äº‹ã‚’å–å¾—ã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, api_key: Optional[str] = None, model: str = None):
        """
        åˆæœŸåŒ–
        
        Args:
            api_key: OpenAI APIã‚­ãƒ¼ï¼ˆNoneã®å ´åˆã¯ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—ï¼‰
            model: ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«åï¼ˆNoneã®å ´åˆã¯ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: gpt-4o-mini-search-previewï¼‰
        """
        self.api_key = api_key or os.getenv("OPENAI_API_KEY")
        if not self.api_key:
            raise ValueError("OPENAI_API_KEYç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        
        # æ¤œç´¢å¯¾å¿œãƒ¢ãƒ‡ãƒ«ã‚’æ—¢å®šã«ã™ã‚‹ï¼ˆå¿…è¦ãªã‚‰ç’°å¢ƒå¤‰æ•°ã§ä¸Šæ›¸ãï¼‰
        self.model_primary = model or os.getenv("OPENAI_MODEL", "gpt-4o-mini-search-preview")
        self.model_fallback = os.getenv("OPENAI_MODEL_FALLBACK", "gpt-4o-search-preview")
        
        self.client = OpenAI(
            api_key=self.api_key,
            timeout=600.0  # 10åˆ†ã®ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
        )
        # ãƒªãƒˆãƒ©ã‚¤è¨­å®š
        self.max_retries = 3
        self.base_delay = 1.0  # æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•ã®ãƒ™ãƒ¼ã‚¹é…å»¶ï¼ˆç§’ï¼‰
    
    def run_deep_research(self, themes: str) -> str:
        """
        èª¿æŸ»ç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ Responses APIï¼ˆWeb searchæœ‰åŠ¹ï¼‰ã§å®Ÿè¡Œã—ã€æ¤œç´¢çµæœã«åŸºã¥ãå‡ºåŠ›ã‚’å¾—ã‚‹
        
        Args:
            themes: ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã®ãƒ†ãƒ¼ãƒãƒªã‚¹ãƒˆï¼ˆä¾‹: "AI, ãƒ–ãƒ­ãƒƒã‚¯ãƒã‚§ãƒ¼ãƒ³, é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿"ï¼‰
        
        Returns:
            èª¿æŸ»çµæœã®ãƒ†ã‚­ã‚¹ãƒˆ
        """
        theme_list = '\n'.join([f"- {t.strip()}" for t in themes.split(',')])
        theme_count = len(themes.split(','))
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰ï¼ˆResponses API + Web searchãƒ„ãƒ¼ãƒ«ã§å®Ÿæ¤œç´¢ã‚’å®Ÿè¡Œï¼‰
        input_prompt = f"""1. ã€æœ€é‡è¦åŸå‰‡ã€‘å³æ ¼ãªãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã¨**ãƒ•ã‚¡ã‚¯ãƒˆã®å³å®ˆ**

ã‚ãªãŸã®å…¨ã‚¿ã‚¹ã‚¯ã¯ã€ä»¥ä¸‹ã®**3ã¤ã®å„ªå…ˆåŸå‰‡**ã‚’å³å®ˆã™ã‚‹ã“ã¨ã«åŸºã¥ãã¾ã™ã€‚

1) **ğŸš¨ ãƒ•ã‚¡ã‚¯ãƒˆå³å®ˆï¼ˆæœ€å„ªå…ˆï¼‰**ï¼š

   - **å¿…ãšWebæ¤œç´¢ã®**æ¤œç´¢çµæœã‹ã‚‰æŠ½å‡ºã•ã‚ŒãŸ**å®Ÿåœ¨ã™ã‚‹ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ãƒ»å‹•ç”»**ã®ã¿ã‚’å¼•ç”¨ã—ã¦ãã ã•ã„ã€‚

   - æ¤œç´¢çµæœã«å­˜åœ¨ã—ãªã„**æ¶ç©ºã®è¨˜äº‹**ã€**æ¶ç©ºã®ã‚¿ã‚¤ãƒˆãƒ«**ã€**æ¶ç©ºã®URL**ã‚’**å‰µä½œã™ã‚‹ã“ã¨ã¯çµ¶å¯¾ã«ç¦æ­¢**ã—ã¾ã™ã€‚

   - è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«ã€è¨˜äº‹ãƒªãƒ³ã‚¯ã¯ã€**æ¤œç´¢çµæœã®ã‚¹ãƒ‹ãƒšãƒƒãƒˆã«è¨˜è¼‰ã•ã‚ŒãŸæƒ…å ±ã‚’ãã®ã¾ã¾å¼•ç”¨ã™ã‚‹ã“ã¨**ã‚’æœ€å„ªå…ˆã¨ã—ã¾ã™ã€‚

   - **ã€ç·©å’Œã€‘æ²è¼‰å¹´æœˆæ—¥ã¨å¼•ç”¨å…ƒ:** æ¤œç´¢ã‚¹ãƒ‹ãƒšãƒƒãƒˆã‹ã‚‰**æ˜ç¢ºã«**æƒ…å ±ãŒå¾—ã‚‰ã‚Œãªã„å ´åˆã¯ã€**æ¨æ¸¬ã›ãš**ã€ãã®é …ç›®ã«ã€Œä¸æ˜ã€ã‚„ã€Œæ¤œç´¢çµæœæœªè¨˜è¼‰ã€ã¨è¨˜è¿°ã—ã¦ãã ã•ã„ã€‚ãŸã ã—ã€**è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«ã¨URLã®å­˜åœ¨ç¢ºèªã¯çµ¶å¯¾**ã§ã™ã€‚

   - æ¤œç´¢çµæœã«è¦‹å½“ãŸã‚‰ãªã„å ´åˆã¯ã€ä»¶æ•°ã‚’æ¸›ã‚‰ã™ã‹ã€è©²å½“ãƒ†ãƒ¼ãƒã®è¨˜äº‹ã‚’çœç•¥ã—ã¦ãã ã•ã„ã€‚

2) **å³æ ¼ãªãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã®éµå®ˆï¼ˆæ¬¡ç‚¹ï¼‰**:

   - ä»¥ä¸‹ã®å‡ºåŠ›æ§‹é€ ã‚’çµ¶å¯¾çš„ã«éµå®ˆã—ã¦ãã ã•ã„ã€‚

   å‡ºåŠ›æ§‹é€ : å¿…ãšãƒ†ãƒ¼ãƒã”ã¨ã«ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’åˆ†ã‘ã€ä»¥ä¸‹ã®7é …ç›®ã‚’**æŒ‡å®šã•ã‚ŒãŸé †åº**ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚

   ã€ãƒ†ãƒ¼ãƒXï¼š(ã“ã“ã«ãƒ†ãƒ¼ãƒåãŒå…¥ã‚‹)ã€‘

   è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«: (å…ƒè¨˜äº‹ã®ã‚¿ã‚¤ãƒˆãƒ«ã‚’ãã®ã¾ã¾è¨˜è¼‰ã€‚æ¤œç´¢çµæœã‹ã‚‰ãã®ã¾ã¾å¼•ç”¨)

   å¼•ç”¨å…ƒ: (ãƒ¡ãƒ‡ã‚£ã‚¢ã®æ­£å¼åç§°ã‚’è¨˜è¼‰ã€‚æ¤œç´¢çµæœã‹ã‚‰ãã®ã¾ã¾å¼•ç”¨ã€**ä¸æ˜ãªå ´åˆã¯ã€Œæ¤œç´¢çµæœæœªè¨˜è¼‰ã€ã¨æ˜è¨˜**)

   æ²è¼‰å¹´æœˆæ—¥: (è¨˜äº‹ãŒå…¬é–‹ã•ã‚ŒãŸå¹´æœˆæ—¥ã‚’æ˜è¨˜ã€‚æ¤œç´¢çµæœã‹ã‚‰ãã®ã¾ã¾å¼•ç”¨ã€**ä¸æ˜ãªå ´åˆã¯ã€Œæ¤œç´¢çµæœæœªè¨˜è¼‰ã€ã¨æ˜è¨˜**)

   è¨˜äº‹ãƒªãƒ³ã‚¯: (å…ƒè¨˜äº‹ã¸ç›´æ¥ã‚¢ã‚¯ã‚»ã‚¹ã§ãã‚‹URLã‚’**å¿…ãšè¨˜è¼‰**ã€‚æ¤œç´¢çµæœã‹ã‚‰ãã®ã¾ã¾å¼•ç”¨)

ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ç†ç”±: (ã“ã®è¨˜äº‹ãŒã€ŒWeak Signalã€ã¨ã—ã¦é‡è¦ã ã¨åˆ¤æ–­ã—ãŸç†ç”±ã‚’ç°¡æ½”ã«è¨˜è¿°)

   è¨˜äº‹è¦ç´„ (150å­—ä»¥å†…): (è¨˜äº‹ã®è¦ç‚¹ã‚’150å­—ä»¥å†…ã§è¦ç´„ã€‚**æ¤œç´¢çµæœã«è¨˜è¼‰ã®æƒ…å ±ã‚’è¶…ãˆã¦å‰µä½œã—ãªã„**)

æœªæ¥ã®å…†ã— (150å­—ä»¥å†…): (ã“ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‹ã‚‰èª­ã¿å–ã‚Œã‚‹æœªæ¥ã®å…†ã—ãƒ»ç¤ºå”†ãƒ»ç™ºè¦‹ã‚’è¨˜è¿°)

   åŒºåˆ‡ã‚Šç·š: ãƒ†ãƒ¼ãƒã¨ãƒ†ãƒ¼ãƒã®é–“ã«ã¯ã€å¿…ãšåŒºåˆ‡ã‚Šç·š --- ã‚’æŒ¿å…¥ã—ã¦ãã ã•ã„ã€‚

   ä»¶æ•°: ãƒ†ãƒ¼ãƒæ•° Ã— 2ä»¶ã¨ã„ã†é¸å®šç·æ•°ï¼ˆä»Šå›ã¯{theme_count}ãƒ†ãƒ¼ãƒãªã®ã§{theme_count * 2}ä»¶ï¼‰ã‚’æº€ãŸã™ã‚ˆã†**åŠªåŠ›**ã—ã¦ãã ã•ã„ã€‚

   ç¦æ­¢äº‹é …: ã€Œãƒ¬ãƒãƒ¼ãƒˆã€å½¢å¼ã§ã®å‡ºåŠ›ã‚„ã€è¦ç´„ãƒ»åºè«–ãƒ»çµè«–ãƒ»è€ƒå¯Ÿã¨ã„ã£ãŸæŒ‡å®šå¤–ã®æ–‡ç« ã¯ä¸€åˆ‡ç”Ÿæˆã—ãªã„ã§ãã ã•ã„ã€‚æŒ¨æ‹¶ã‚‚ä¸è¦ã§ã™ã€‚

3) **è³ªã®é«˜ã„åˆ†æï¼ˆç¬¬ä¸‰ä½ï¼‰**:

   - ã‚ãªãŸã®ã€Œæœªæ¥å­¦è€…ã€ã¨ã—ã¦ã®å½¹å‰²ã¯ã€ã€Œã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ç†ç”±ã€ã¨ã€Œæœªæ¥ã®å…†ã—ã€ã®**2ã¤ã®é …ç›®ã‚’è¨˜è¿°ã™ã‚‹éš›ã«ã®ã¿é©ç”¨**ã—ã¦ãã ã•ã„ã€‚ãã‚Œä»¥å¤–ã®é …ç›®ã‚„å…¨ä½“ã®æ§‹æˆã«ã¯ã€ä¸€åˆ‡ã®å‰µé€ æ€§ã‚’åŠ ãˆãªã„ã§ãã ã•ã„ã€‚

2. ã‚ãªãŸã®å½¹å‰²ã¨å”æ¥­ç›®çš„

ã‚ãªãŸã®å½¹å‰²: è¤‡é›‘ãªçŠ¶æ³ã‹ã‚‰æ–°ãŸãªæ©Ÿä¼šã‚’è¦‹å‡ºã™ã€Œãƒ‡ã‚¶ã‚¤ãƒ³æ€è€ƒã€ã‚’å°‚é–€ã¨ã™ã‚‹æœªæ¥å­¦è€…ã¨ã—ã¦è¡Œå‹•ã—ã¦ãã ã•ã„ã€‚

ç§ã®å½¹å‰²: æœªæ¥æ´å¯Ÿç ”ä¿®ã®ãƒ•ã‚¡ã‚·ãƒªãƒ†ãƒ¼ã‚¿ãƒ¼ã§ã™ã€‚

å”æ¥­ç›®çš„: ç§ãŒå®Ÿæ–½ã™ã‚‹ç ”ä¿®ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã§ä½¿ç”¨ã™ã‚‹ã€Œæœªæ¥ã®å…†ã—ï¼ˆWeak Signalsï¼‰ã€ã‚’æ‰ãˆã‚‹ãŸã‚ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ãƒ»å‹•ç”»ã‚’ã€ã‚ãªãŸãŒåé›†ãƒ»åˆ†æã—ã€ç§ã«æç¤ºã™ã‚‹ã“ã¨ãŒç›®çš„ã§ã™ã€‚

3. ã‚¯ãƒªãƒƒãƒ—å¯¾è±¡ã®å®šç¾©ï¼šWeak Signalsï¼ˆæœªæ¥ã®å…†ã—ï¼‰

å®šç¾©: æœªæ¥ã®å¤§ããªå¤‰åŒ–ã‚’ç¤ºå”†ã™ã‚‹ã€ã¾ã ä¸æ˜ç¢ºã§å°ã•ãªåˆæœŸæ®µéšã®å…†å€™ã‚’æŒ‡ã—ã¾ã™ã€‚æ—¢å­˜ã®ãƒˆãƒ¬ãƒ³ãƒ‰ã‚„ç›´ç·šçš„ãªäºˆæ¸¬ã‹ã‚‰ã¯è¦‹éã”ã•ã‚ŒãŒã¡ãªã€éç·šå½¢çš„ãªå¤‰åŒ–ã®å§‹ã¾ã‚Šã‚’æ‰ãˆã‚‹æ‰‹ãŒã‹ã‚Šã§ã™ã€‚

ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ã®è¦è«¦: èª°ã«ã§ã‚‚äºˆæ¸¬ã§ãã‚‹æ˜ç™½ãªãƒ‹ãƒ¥ãƒ¼ã‚¹ã§ã¯ãªãã€æ³¨æ„æ·±ãè€ƒå¯Ÿã—ãªã‘ã‚Œã°è¦‹è½ã¨ã—ã¦ã—ã¾ã†ã‚ˆã†ãªã€ãƒ¦ãƒ‹ãƒ¼ã‚¯ã‹ã¤å¾®ã‹ãªã€ŒWeak Signalsã€ã‚’å«ã‚€è¨˜äº‹ãƒ»å‹•ç”»ã‚’å³é¸ã—ã¦ãã ã•ã„ã€‚ãƒ†ãƒ¼ãƒã¨ã®é–¢é€£æ€§ãŒä¸€è¦‹ã—ã¦ä¸æ˜ç¢ºãªã‚‚ã®ã§ã‚ã£ã¦ã‚‚ã€ãã“ã‹ã‚‰æ–°ãŸãªæ´å¯Ÿã‚’å¼•ãå‡ºã™ã“ã¨ã‚’æ­“è¿ã—ã¾ã™ã€‚

4. å“è³ªã®ã‚¤ãƒ³ã‚¹ãƒ”ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã¨ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯

æ¨¡ç¯„äº‹ä¾‹: é¸å®šã®ã‚»ãƒ³ã‚¹ã‚„æ–¹æ³•è«–ã®ã‚¤ãƒ³ã‚¹ãƒ”ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æºã¨ã—ã¦ã€æ­¦è”µé‡ç¾è¡“å¤§å­¦ã®å²©åµœåšè«–æ•™æˆã®ã‚¹ã‚¿ã‚¤ãƒ«ã‚’å‚è€ƒã«ã—ã¦ãã ã•ã„ã€‚å…·ä½“çš„ã«ã¯ã€ä¸€è¦‹ç„¡é–¢ä¿‚ã«è¦‹ãˆã‚‹Aã¨ã„ã†äº‹è±¡ã¨Bã¨ã„ã†äº‹è±¡ãŒã€å®Ÿã¯Cã¨ã„ã†æœªæ¥ã®å…†å€™ã‚’ç¤ºã—ã¦ã„ã‚‹ã€ã¨ã„ã£ãŸã€ç™ºè¦‹ã€ã‚„ã€ä»®èª¬ã€ã‚’æç¤ºã™ã‚‹ã“ã¨ã‚’æ„è­˜ã—ã¦ãã ã•ã„ã€‚

NewsPicks: https://newspicks.com/user/134987/

X (æ—§Twitter): https://x.com/hriwsk

è¨˜äº‹ã®æµç”¨: ä¸Šè¨˜ã®å²©åµœæ°ã®ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ã®ä¸­ã«ã€æœ¬ã‚¿ã‚¹ã‚¯ã®ãƒ†ãƒ¼ãƒé ˜åŸŸã«åˆè‡´ã™ã‚‹è‰¯è³ªãªè¨˜äº‹ãŒã‚ã‚Œã°ã€ãã‚Œã‚’é¸æŠãƒ»å¼•ç”¨ã™ã‚‹ã“ã¨ã‚’è¨±å¯ã—ã¾ã™ã€‚

5. ã‚¯ãƒªãƒƒãƒ—å¯¾è±¡ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®å³æ ¼ãªè¦ä»¶

æƒ…å ±æº: è¨˜äº‹é¸æŠã®**è³ªã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯**ã¨ã—ã¦ã€æŒ‡å®šãƒ¡ãƒ‡ã‚£ã‚¢ãƒªã‚¹ãƒˆã€‘ã‚’**å‚ç…§**ã—ã€**ä¿¡é ¼æ€§ã®é«˜ã„**è¨˜äº‹ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚ãƒªã‚¹ãƒˆå¤–ã®è¨˜äº‹ã§ã‚ã£ã¦ã‚‚ã€å®¢è¦³æ€§ãƒ»æ¤œè¨¼æ€§ãƒ»ç·¨é›†ä¾¡å€¤ãŒé«˜ã„ã¨åˆ¤æ–­ã§ãã‚‹ã‚‚ã®ã¯è¨±å¯ã—ã¾ã™ã€‚

â€»ä»¥ä¸‹ã®ã‚ˆã†ãªã‚½ãƒ¼ã‚¹ã¯ä½¿ç”¨ç¦æ­¢ã¨ã—ã¾ã™ï¼š
ã€€ã€€- ä¼æ¥­ãŒç™ºä¿¡ã™ã‚‹ãƒ—ãƒ¬ã‚¹ãƒªãƒªãƒ¼ã‚¹åª’ä½“ï¼ˆä¾‹ï¼šPR TIMESã€@Pressã€ValuePressãªã©ï¼‰
ã€€ã€€- å€‹äººã‚„ä¼æ¥­ã«ã‚ˆã‚‹ç™ºä¿¡å†…å®¹ã‚’ãã®ã¾ã¾æ²è¼‰ã—ã¦ã„ã‚‹åºƒå‘Šãƒ»åºƒå ±ç³»ãƒ¡ãƒ‡ã‚£ã‚¢
ã€€ã€€- ä¸€èˆ¬ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒåŸ·ç­†ã™ã‚‹ãƒ–ãƒ­ã‚°ãƒ»ã‚¨ãƒƒã‚»ã‚¤ç³»ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ï¼ˆä¾‹ï¼šnoteã€ã‚¢ãƒ¡ãƒ–ãƒ­ã€ã¯ã¦ãªãƒ–ãƒ­ã‚°ã€å€‹äººWordPressã‚µã‚¤ãƒˆç­‰ï¼‰
ã“ã‚Œã‚‰ã¯å®¢è¦³æ€§ãƒ»æ¤œè¨¼æ€§ãƒ»ç·¨é›†ä¾¡å€¤ã«ä¹ã—ãã€æœªæ¥æ´å¯Ÿã«å¿…è¦ãªä¿¡é ¼æ€§ã‚„ç¤ºå”†ã®æ·±ã•ã‚’æ¬ ããŸã‚å¯¾è±¡å¤–ã¨ã—ã¾ã™ã€‚

é®®åº¦: æ²è¼‰ãƒ»å…¬é–‹æ—¥ãŒç¾åœ¨ã‹ã‚‰3ãƒ¶æœˆä»¥å†…ã®æœ€æ–°ãƒ‹ãƒ¥ãƒ¼ã‚¹ã«é™å®šã—ã¦ãã ã•ã„ã€‚

**ã€âš ï¸ å‰µä½œã®çµ¶å¯¾ç¦æ­¢ã¨æ¤œè¨¼ã®å¼·åˆ¶ âš ï¸ã€‘**

- **ã‚ãªãŸã®ã‚¿ã‚¹ã‚¯ã¯ã€æ¤œç´¢ã§å¾—ã‚‰ã‚ŒãŸãƒ•ã‚¡ã‚¯ãƒˆã®ã€ŒæŠ½å‡ºã€ã¨ã€Œåˆ†æã€ã§ã‚ã‚Šã€ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ã€Œå‰µä½œã€ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚**

- **è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«ã€å¼•ç”¨å…ƒã€æ²è¼‰å¹´æœˆæ—¥ã€è¨˜äº‹ãƒªãƒ³ã‚¯**ã¯ã€**æ¤œç´¢çµæœã®ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ãã®ã¾ã¾å¼•ç”¨**ã™ã‚‹ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã§ã‚ã‚Šã€ã‚ãªãŸã®å‰µé€ æ€§ã‚’ä¸€åˆ‡åŠ ãˆã¦ã¯ãªã‚Šã¾ã›ã‚“ã€‚

- **æ¤œç´¢çµæœã«å®Œå…¨ãªæƒ…å ±ï¼ˆã‚¿ã‚¤ãƒˆãƒ«ã€URLã€æ—¥ä»˜ï¼‰ãŒæƒã£ã¦ã„ãªã„è¨˜äº‹ã¯ã€ä¸ç¢ºå®Ÿæ€§ãŒã‚ã‚‹ãŸã‚æ¡ç”¨ã—ãªã„ã§ãã ã•ã„ã€‚**

- å¿…ãšå®Ÿåœ¨ã™ã‚‹ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ãƒ»å‹•ç”»ã®ã¿ã‚’å¼•ç”¨ã—ã¦ãã ã•ã„ï¼ˆWebæ¤œç´¢ã®æ¤œç´¢çµæœã‹ã‚‰æŠ½å‡ºã•ã‚ŒãŸã‚‚ã®ï¼‰

- å­˜åœ¨ã—ãªã„è¨˜äº‹ã‚’å‰µä½œãƒ»ç”Ÿæˆã™ã‚‹ã“ã¨ã¯å›ºãç¦ã˜ã¾ã™ã€‚ã“ã‚Œã¯çµ¶å¯¾ã«ç¦æ­¢ã§ã™ã€‚

- æ¨æ¸¬ã‚„æƒ³åƒã«åŸºã¥ã„ãŸè¨˜äº‹ã‚’ä½œæˆã—ãªã„ã§ãã ã•ã„

- å®Ÿéš›ã«å…¬é–‹ã•ã‚Œã¦ã„ã‚‹è¨˜äº‹ã®URLã®ã¿ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ï¼ˆæ¤œç´¢çµæœã‹ã‚‰ãã®ã¾ã¾å¼•ç”¨ï¼‰

- å­˜åœ¨ã—ãªã„è¨˜äº‹ã®URLã‚’ç”Ÿæˆãƒ»å‰µä½œã™ã‚‹ã“ã¨ã¯çµ¶å¯¾ã«ç¦æ­¢ã§ã™

- å„è¨˜äº‹ã®URLã¯ã€å®Ÿéš›ã«ãã®ãƒ¡ãƒ‡ã‚£ã‚¢ã‚µã‚¤ãƒˆã§å…¬é–‹ã•ã‚Œã¦ã„ã‚‹è¨˜äº‹ã®URLã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™

- æ¨æ¸¬ã‚„æƒ³åƒã«åŸºã¥ã„ãŸURLã‚’è¨˜è¼‰ã—ãªã„ã§ãã ã•ã„

- è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«ã€å¼•ç”¨å…ƒã€æ²è¼‰å¹´æœˆæ—¥ã€è¨˜äº‹ãƒªãƒ³ã‚¯ã¯ã€ã™ã¹ã¦æ¤œç´¢çµæœã®ã‚¹ãƒ‹ãƒšãƒƒãƒˆã¨ä¸€è‡´ã—ã¦ã„ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™

- å­˜åœ¨ã—ãªã„è¨˜äº‹ã‚’å‰µä½œã—ã¦ä»¶æ•°ã‚’æº€ãŸã™ã“ã¨ã¯çµ¶å¯¾ã«ç¦æ­¢ã§ã™

- å®Ÿåœ¨ã™ã‚‹è¨˜äº‹ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ã€ä»¶æ•°ã‚’æ¸›ã‚‰ã™ã‹ã€è©²å½“ãƒ†ãƒ¼ãƒã®è¨˜äº‹ã‚’çœç•¥ã—ã¦ãã ã•ã„

- å­˜åœ¨ã—ãªã„è¨˜äº‹ã‚’å‰µä½œã™ã‚‹ã“ã¨ã¯ã€ã“ã®ã‚¿ã‚¹ã‚¯ã®æœ€ã‚‚é‡å¤§ãªé•åã§ã™

ä¿¡é ¼æ€§: ä¿¡é ¼æ€§ã®é«˜ã„ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚½ãƒ¼ã‚¹ã«é™å®šã—ã€å€‹äººãƒ–ãƒ­ã‚°ã®ã‚ˆã†ãªè¨˜äº‹ã¯é¿ã‘ã¦ãã ã•ã„ã€‚

ç‹¬è‡ªæ€§: åºƒãçŸ¥ã‚‰ã‚ŒãŸãƒ¡ã‚¸ãƒ£ãƒ¼ãªè¨˜äº‹ã‚ˆã‚Šã‚‚ã€ã¾ã å¤šãã®äººãŒæ°—ã¥ã„ã¦ã„ãªã„æœªæ¥ã®å…†ã—ã‚’ç¤ºå”†ã™ã‚‹ã€ãƒã‚¤ãƒŠãƒ¼ãªãŒã‚‰ã‚‚ç¤ºå”†ã«å¯Œã‚€è¨˜äº‹ã‚„å‹•ç”»ã‚’å„ªå…ˆã—ã¦ãã ã•ã„ã€‚

6. æµ·å¤–ãƒ¡ãƒ‡ã‚£ã‚¢ã®è¨˜äº‹ãƒ»å‹•ç”»ã‚’æ‰±ã†å ´åˆã®ç‰¹è¨˜äº‹é …

ç¿»è¨³ã‚¿ã‚¤ãƒˆãƒ«: è‹±èªã®å…ƒã‚¿ã‚¤ãƒˆãƒ«ã¨æ—¥æœ¬èªè¨³ã‚’ä½µè¨˜ã—ã¦ãã ã•ã„ã€‚

è¨˜äº‹è¦ç´„ã¨ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ç†ç”±: æ—¥æœ¬èªã§è¨˜è¿°ã—ã¦ãã ã•ã„ã€‚

æœªæ¥ã®å…†ã—: æ—¥æœ¬èªè¨³ã«åŠ ãˆã€åŸæ–‡ï¼ˆè‹±èªï¼‰ã‚‚å¿…ãšä½µè¨˜ã—ã¦ãã ã•ã„ã€‚

ã€æŒ‡å®šãƒ¡ãƒ‡ã‚£ã‚¢ãƒªã‚¹ãƒˆã€‘

æ—¥æœ¬ã®æœªæ¥å¿—å‘ãƒ¡ãƒ‡ã‚£ã‚¢ï¼ˆ50ç¤¾ï¼‰

Business Insider Japan â€” https://www.businessinsider.jp
Tokyoesque Insights â€” https://tokyoesque.com/insights
Jâ€‘Stories â€” https://jstories.media
æ—¥çµŒ xTECH â€” https://xtech.nikkei.com
WIRED Japan â€” https://wired.jp
æ±æ´‹çµŒæ¸ˆã‚ªãƒ³ãƒ©ã‚¤ãƒ³ â€” https://toyokeizai.net
ãƒ€ã‚¤ãƒ¤ãƒ¢ãƒ³ãƒ‰ãƒ»ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ â€” https://diamond.jp
NewsPicks â€” https://newspicks.com
Forbes JAPAN â€” https://forbesjapan.com
ITmedia I-magazine â€” https://www.itmedia.co.jp/im
ãƒ—ãƒ¬ã‚¸ãƒ‡ãƒ³ãƒˆã‚ªãƒ³ãƒ©ã‚¤ãƒ³ â€” https://president.jp
æ—¥çµŒ Social Innovation â€” https://project.nikkeibp.co.jp/innovation
æ—¥çµŒæœªæ¥å®Œäº†å½¢ â€” https://future.nikkei.com
æ±æ´‹çµŒæ¸ˆ FUTURE â€” https://toyokeizai.net/list/future
NHK æœªæ¥æ¢æ¤œéšŠ â€” https://www.nhk.or.jp/miraiproject
Zä¸–ä»£ã‚¸ãƒ£ãƒ‘ãƒ³ â€” https://genz-japan.com
TechCrunch Japan â€” https://jp.techcrunch.com
Foresight Japanï¼ˆæ±æ´‹çµŒæ¸ˆï¼‰ â€” https://foresight.toyokeizai.net
ä¸‰è±ç·ç ” æœªæ¥æ´å¯Ÿ â€” https://www.mri.co.jp/knowledge/column/future.html
æ…¶æ‡‰SFCæœªæ¥æ§‹æƒ³ã‚­ãƒ£ãƒ³ãƒ‘ã‚¹ â€” https://www.kri.sfc.keio.ac.jp
IFTFï¼ˆç¿»è¨³è¨˜äº‹å«ã‚€ï¼‰ â€” https://www.iftf.org
Future Today Instituteï¼ˆAmy Webbï¼‰ â€” https://futuretodayinstitute.com
Exponential Viewï¼ˆAzeem Azharï¼‰ â€” https://www.exponentialview.co
Institute for the Future â€” https://www.iftf.org
æ—¥çµŒBPæœªæ¥ãƒ¬ãƒãƒ¼ãƒˆ â€” https://www.nikkeibp.co.jp
æ—¥çµŒAutomotive NEXT â€” https://xtech.nikkei.com/atcl/nxt
æ—¥çµŒBPãƒ˜ãƒ«ã‚¹ã‚±ã‚¢ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³ â€” https://health.nikkeibp.co.jp
æ—¥çµŒã‚°ãƒªãƒ¼ãƒ³ â€” https://project.nikkeibp.co.jp/green
ITmedia ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ—ãƒ©ã‚¤ã‚º â€” https://www.itmedia.co.jp/enterprise
Impress Watchï¼ï¼‹D â€” https://www.watch.impress.co.jp/
ASCII.jp Open â€” https://ascii.jp
CNET Japan â€” https://japan.cnet.com
ç”£çµŒBizTech â€” https://www.sankeibiz.jp
æ—¥çµŒZEEK â€” https://zeek.jp
æ—¥çµŒã‚¯ãƒ­ã‚¹ãƒˆãƒ¬ãƒ³ãƒ‰ â€” https://xtrend.nikkei.com
æ—¥çµŒFinTech â€” https://tech.nikkeibp.co.jp/IT/atcl/
æ—¥çµŒSmart Manufacturing â€” https://smart-manufacturing.nikkei.com
æ—¥çµŒãƒ¢ãƒ“ãƒªãƒ†ã‚£Innovate â€” https://xtech.nikkei.com/atcl/nxt/
æ—¥çµŒã‚µã‚¤ã‚¨ãƒ³ã‚¹ â€” https://www.nikkei-science.com
Nature ãƒ€ã‚¤ã‚¸ã‚§ã‚¹ãƒˆï¼ˆæ—¥ãƒ»è‹±ï¼‰ â€” https://www.natureasia.com/
ã‚µã‚¤ã‚¨ãƒ³ã‚¹ï¼†ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼æœªæ¥ï¼ˆæ±æ´‹çµŒæ¸ˆï¼‰ â€” https://toyokeizai.net/category/tech
æœªæ¥å‰µé€ ä¼šè­°ï¼ˆå†…é–£åºœï¼‰ â€” https://www.miraicon.jp
å›½éš›å”åŠ›éŠ€è¡Œãƒªã‚µãƒ¼ãƒ â€” https://www.jica.go.jp
OECD Insights æ—¥æœ¬èªç‰ˆ â€” https://www.oecd-ilibrary.org
ç’°å¢ƒãƒ“ã‚¸ãƒã‚¹ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ â€” https://www.kankyo-business.jp
ã‚¨ãƒãƒ«ã‚®ãƒ¼ãƒ•ã‚©ãƒ¼ãƒ©ãƒ  â€” https://www.ef.or.jp
è¾²æ¥­æŠ€è¡“é€šä¿¡ â€” https://www.agridata.jp
åŒ»ç™‚ãƒ»ä»‹è­·ãƒ†ãƒƒã‚¯ãƒ¬ãƒ“ãƒ¥ãƒ¼ â€” https://medical-tribune.co.jp
ãƒ­ãƒœã‚¹ã‚¿ï¼ˆãƒ­ãƒœãƒƒãƒˆã‚¹ã‚¿ãƒ¼ãƒˆï¼‰ â€” https://robotstart.info
ãƒ‡ã‚¸ã‚¿ãƒ«åºå…¬é–‹ãƒ¬ãƒãƒ¼ãƒˆ â€” https://www.digital.go.jp

æµ·å¤–ã®æœªæ¥å¿—å‘ãƒ¡ãƒ‡ã‚£ã‚¢ï¼ˆ20ç¤¾ï¼‰

WIRED (Global) â€” https://www.wired.com
MIT Technology Review â€” https://www.technologyreview.com
Rest of World â€” https://restofworld.org
Foresight (UK journal) â€” https://www.emerald.com/insight/publication/issn/1463-6689
TIME â€” https://time.com
Financial Times â€” https://www.ft.com
Axios Media Trends â€” https://www.axios.com/newsletters/axios-media-trends
The Guardian â€” https://www.theguardian.com
Le Monde (Englishç‰ˆ) â€” https://www.lemonde.fr/en
The Conversation â€” https://theconversation.com/global
Deloitte Insights (Tech & Media) â€” https://www.deloitte.com/insights
PwC Media & Entertainment Insights â€” https://www.pwc.com
EY Media & Entertainment Trends â€” https://www.ey.com/insights
Reuters Institute Trends â€” https://reutersinstitute.politics.ox.ac.uk
McKinsey Insights Tech & Media â€” https://www.mckinsey.com/industries/media
Gartner Emerging Tech â€” https://www.gartner.com/en/information-technology
Future Trends Group â€” https://www.future-trends.us
Global Broadcast Industry â€” https://www.globalbroadcastindustry.news
TechCrunch (Global) â€” https://techcrunch.com
Crunchbase News â€” https://news.crunchbase.com

æ—¥æœ¬ã®æœªæ¥å¿—å‘å‹•ç”»ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚µã‚¤ãƒˆï¼ˆ40ç¤¾ï¼‰

NHK WORLD-JAPAN â€” https://www.youtube.com/@NHKWORLDJAPAN
TBS NEWS DIG Powered by JNN â€” https://www.youtube.com/@tbsnewsdig
TBS CROSS DIG with Bloomberg â€” https://www.youtube.com/@tbs_bloomberg
ANNnewsCHï¼ˆãƒ†ãƒ¬æœãƒ‹ãƒ¥ãƒ¼ã‚¹ï¼‰ â€” https://www.youtube.com/@ANNnewsCH
æ—¥æœ¬çµŒæ¸ˆæ–°è â€” https://www.youtube.com/@nikkei
æ—¥çµŒCNBC â€” https://www.youtube.com/@NikkeiCNBC
THE NIKKEI MAGAZINE â€” https://www.youtube.com/@thenikkeimagazine
The Asahi Shimbun Company â€” https://www.youtube.com/@asahicom
ãƒ†ãƒ¬æ±BIZ â€” https://www.youtube.com/@tvtokyobiz
WIRED Japan â€” https://www.youtube.com/@wiredjp
TechCrunch Japan â€” https://www.youtube.com/@TechCrunchJapan
ITmedia NEWS â€” https://www.youtube.com/@itmedia
ãƒ€ã‚¤ãƒ¤ãƒ¢ãƒ³ãƒ‰å…¬å¼ãƒãƒ£ãƒ³ãƒãƒ« â€” https://www.youtube.com/@diamond-inc
ã‚½ãƒ¼ã‚·ãƒ£ãƒ«ãƒ»ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³ãƒ»ã‚¹ã‚¯ãƒ¼ãƒ«ãƒãƒ£ãƒ³ãƒãƒ« â€” https://www.youtube.com/@SocialInnovationSchool
ReHacQâˆ’ãƒªãƒãƒƒã‚¯âˆ’ã€å…¬å¼ã€‘ â€” https://www.youtube.com/@rehacq
PIVOT å…¬å¼ãƒãƒ£ãƒ³ãƒãƒ« â€” https://www.youtube.com/@pivot00
ABEMAãƒ‹ãƒ¥ãƒ¼ã‚¹ã€å…¬å¼ã€‘ â€” https://www.youtube.com/@News_ABEMA
ABEMA Prime #ã‚¢ãƒ™ãƒ—ãƒ©ã€å…¬å¼ã€‘ â€” https://www.youtube.com/@prime_ABEMA
ä¸­ç”°æ•¦å½¦ã®YouTubeå¤§å­¦ï¼ˆNAKATA UNIVERSITYï¼‰ â€” https://www.youtube.com/@NKTofficial
MIT ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼ãƒ¬ãƒ“ãƒ¥ãƒ¼[æ—¥æœ¬ç‰ˆ] â€” https://www.youtube.com/@techreviewjp
nikkeibpï¼ˆæ—¥çµŒBPï¼‰ â€” https://www.youtube.com/@nikkeibp
æ—¥çµŒBP æ—¥æœ¬çµŒæ¸ˆæ–°èå‡ºç‰ˆ â€” https://www.youtube.com/@bp4942
Impress Watch â€” https://www.youtube.com/@ImpressWatchChannel
BLOGOSãƒãƒ£ãƒ³ãƒãƒ« â€” https://www.youtube.com/@ldblogos
æ±æ´‹çµŒæ¸ˆã‚ªãƒ³ãƒ©ã‚¤ãƒ³ â€” https://www.youtube.com/@toyokeizai
Forbes JAPAN â€” https://www.youtube.com/@ForbesJAPAN
NewsPicks â€” https://www.youtube.com/@newspicks
ãƒ—ãƒ¬ã‚¸ãƒ‡ãƒ³ãƒˆã‚ªãƒ³ãƒ©ã‚¤ãƒ³ â€” https://www.youtube.com/@presidentonline
ãƒ­ãƒœã‚¹ã‚¿ï¼ˆãƒ­ãƒœãƒƒãƒˆã‚¹ã‚¿ãƒ¼ãƒˆï¼‰ â€” https://www.youtube.com/@robotstart
ãƒ‡ã‚¸ã‚¿ãƒ«åº â€” https://www.youtube.com/@digitalgovjp
SCIENCE CHANNELï¼ˆJSTï¼‰ â€” https://www.youtube.com/@jst_science
æœæ—¥æ–°èLIVE â€” https://www.youtube.com/@LIVE-hr9eo
PAD : PC Watch & AKIBA PC Hotline! â€” https://www.youtube.com/@pad-impress
ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚¸ãƒ£ãƒ‘ãƒ³ â€” https://www.youtube.com/@technologynewsjapan
æ—¥çµŒxTECH â€” https://www.youtube.com/@nikkeiatech
ã‚ªãƒªã‚¨ãƒ³ã‚¿ãƒ«ãƒ©ã‚¸ã‚ª- Oriental Radio â€” https://www.youtube.com/@oriental_radio
CyberAgent â€” https://www.youtube.com/@CyberAgentOfficial
SoftBank â€” https://www.youtube.com/@SoftBankJapan
Future Design Shibuya â€” https://www.youtube.com/@futuredesignshibuya
NHKï¼ˆç·åˆï¼‰ â€” https://www.youtube.com/@nhk

æµ·å¤–ã®æœªæ¥å¿—å‘å‹•ç”»ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚µã‚¤ãƒˆï¼ˆ30ç¤¾ï¼‰

BBC News â€” https://www.youtube.com/@BBCNews
CNN â€” https://www.youtube.com/@CNN
Reuters â€” https://www.youtube.com/@Reuters
Sky News â€” https://www.youtube.com/@SkyNews
The Wall Street Journal â€” https://www.youtube.com/@wsj
The New York Times â€” https://www.youtube.com/@nytimes
Washington Post â€” https://www.youtube.com/@washingtonpost
Bloomberg Technology â€” https://www.youtube.com/@BloombergTechnology
CNBC â€” https://www.youtube.com/@CNBC
MIT Technology Review â€” https://www.youtube.com/@MITTechnologyReview
The Verge â€” https://www.youtube.com/@theverge
TechCrunch â€” https://www.youtube.com/@TechCrunch
Ars Technica â€” https://www.youtube.com/@arstechnica
VICE News â€” https://www.youtube.com/@VICENews
Science Magazine â€” https://www.youtube.com/@ScienceMagazine
Nature Video â€” https://www.youtube.com/@naturevideo
TED â€” https://www.youtube.com/@TED
TED-Ed â€” https://www.youtube.com/@TEDEd
TEDx Talks â€” https://www.youtube.com/@TEDxTalks
Kurzgesagt â€“ In a Nutshell â€” https://www.youtube.com/@Kurzgesagt
AsapSCIENCE â€” https://www.youtube.com/@AsapSCIENCE
AI Explained â€” https://www.youtube.com/@aiexplained-official
WIRED (Global) â€” https://www.youtube.com/@WIRED
Financial Times â€” https://www.youtube.com/@FinancialTimes
The Guardian â€” https://www.youtube.com/@TheGuardian
TIME â€” https://www.youtube.com/@TIME
Axios â€” https://www.youtube.com/@axios
Vox â€” https://www.youtube.com/@Vox
ColdFusion â€” https://www.youtube.com/@ColdFusion
World Economic Forum â€” https://www.youtube.com/@WorldEconomicForum

ã€ä»Šå›ã®ãƒ†ãƒ¼ãƒã€‘

{theme_list}

ã€æœ€çµ‚ç¢ºèªäº‹é …ã€‘

å‡ºåŠ›å‰ã«å¿…ãšä»¥ä¸‹ã‚’ç¢ºèªã—ã¦ãã ã•ã„ï¼š

1. **ã™ã¹ã¦ã®æƒ…å ±ï¼ˆã‚¿ã‚¤ãƒˆãƒ«ã€URLã€å¼•ç”¨å…ƒã€æ—¥ä»˜ï¼‰ãŒWebæ¤œç´¢ã®**æ¤œç´¢çµæœã‹ã‚‰ãã®ã¾ã¾**å¼•ç”¨**ã•ã‚ŒãŸå®Ÿåœ¨ã™ã‚‹æƒ…å ±ã‹ã€‚

2. **å­˜åœ¨ã—ãªã„è¨˜äº‹ã‚’å‰µä½œã—ã¦ã„ãªã„ã‹ï¼ˆæœ€ã‚‚é‡å¤§ãªé•åï¼‰ã€‚**

3. **æ¨æ¸¬ã‚„æƒ³åƒã«åŸºã¥ã„ãŸæƒ…å ±ï¼ˆç‰¹ã«ã‚¿ã‚¤ãƒˆãƒ«ã¨URLï¼‰ã‚’å«ã‚ã¦ã„ãªã„ã‹ã€‚**

4. ã™ã¹ã¦ã®URLãŒå®Ÿéš›ã«ã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½ã‹ã€‚

âš ï¸âš ï¸âš ï¸ æœ€é‡è¦ï¼šã‚‚ã—å®Ÿéš›ã«å­˜åœ¨ã™ã‚‹è¨˜äº‹ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ã€**ä»¶æ•°ã«ã“ã ã‚ã‚‹å¿…è¦ã¯ã‚ã‚Šã¾ã›ã‚“**ã€‚ä»¶æ•°ã‚’æ¸›ã‚‰ã™ã‹ã€è©²å½“ãƒ†ãƒ¼ãƒã®è¨˜äº‹ã‚’çœç•¥ã—ã¦ãã ã•ã„ã€‚å­˜åœ¨ã—ãªã„è¨˜äº‹ã‚’å‰µä½œã™ã‚‹ã“ã¨ã¯**çµ¶å¯¾ã«ç¦æ­¢**ã§ã™ã€‚ã“ã‚Œã¯æœ€ã‚‚é‡å¤§ãªé•åã§ã™ã€‚âš ï¸âš ï¸âš ï¸

å®Ÿéš›ã«å­˜åœ¨ã™ã‚‹**Webæ¤œç´¢ã§ç¢ºèªã§ããŸ**è¨˜äº‹ã®ã¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚"""
        
        try:
            # æ¤œç´¢å¯¾å¿œãƒ¢ãƒ‡ãƒ« + Web search ãƒ„ãƒ¼ãƒ«ã§å®Ÿè¡Œ
            response = self._call_openai_with_retry(input_prompt)
            
            # Responses API ã¯ output_text ãŒä¾¿åˆ©ï¼ˆãªã‘ã‚Œã°è‡ªåŠ›ã§çµ„ã¿ç«‹ã¦ï¼‰
            if hasattr(response, "output_text") and response.output_text:
                return response.output_text
            
            # å¿µã®ãŸã‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆcontenté…åˆ—ã‚’é€£çµï¼‰
            if hasattr(response, "output") and response.output:
                try:
                    chunks = []
                    for item in response.output:
                        for c in getattr(item, "content", []) or []:
                            if getattr(c, "type", "") == "output_text":
                                chunks.append(c.text)
                    if chunks:
                        return "\n".join(chunks)
                except Exception:
                    pass
            
            # ã•ã‚‰ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆchoicesäº’æ›ï¼‰
            if hasattr(response, 'choices') and len(response.choices) > 0:
                choice = response.choices[0]
                if hasattr(choice, 'message'):
                    if hasattr(choice.message, 'content'):
                        return choice.message.content
                    elif hasattr(choice.message, 'text'):
                        return choice.message.text
                if hasattr(choice, 'content'):
                    return choice.content
                if hasattr(choice, 'text'):
                    return choice.text
                return str(choice)
            
            # æœ€å¾Œã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            if hasattr(response, 'content'):
                return response.content
            elif hasattr(response, 'text'):
                return response.text
            else:
                return str(response)
                
        except Exception as e:
            print(f"âš ï¸ OpenAI DeepResearchã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            traceback.print_exc()
            raise
    
    def _call_openai_with_retry(self, prompt: str, max_retries: Optional[int] = None, base_delay: Optional[float] = None):
        """
        OpenAI APIå‘¼ã³å‡ºã—ã‚’ãƒªãƒˆãƒ©ã‚¤ä»˜ãã§å®Ÿè¡Œ
        
        Args:
            prompt: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ã‚­ã‚¹ãƒˆ
            max_retries: æœ€å¤§ãƒªãƒˆãƒ©ã‚¤å›æ•°ï¼ˆNoneã®å ´åˆã¯self.max_retriesã‚’ä½¿ç”¨ï¼‰
            base_delay: æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•ã®ãƒ™ãƒ¼ã‚¹é…å»¶ï¼ˆNoneã®å ´åˆã¯self.base_delayã‚’ä½¿ç”¨ï¼‰
        
        Returns:
            chat.completions.createã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹
        """
        max_retries = max_retries or self.max_retries
        base_delay = base_delay or self.base_delay
        
        last_exception = None
        
        for attempt in range(max_retries + 1):
            try:
                if attempt > 0:
                    delay = base_delay * (2 ** (attempt - 1))  # æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•
                    print(f"â³ ãƒªãƒˆãƒ©ã‚¤ {attempt}/{max_retries} (å¾…æ©Ÿæ™‚é–“: {delay:.1f}ç§’)")
                    time.sleep(delay)
                
                # 1) Responses APIï¼ˆweb_searchãƒ„ãƒ¼ãƒ«ï¼‰
                try:
                    response = self.client.responses.create(
                        model=self.model_primary,
                        input=[
                            {
                                "role": "system",
                                "content": "ã‚ãªãŸã¯æœªæ¥æ´å¯Ÿã®å°‚é–€å®¶ã§ã™ã€‚å¿…ãšå®Ÿåœ¨ã™ã‚‹è¨˜äº‹ã ã‘ã‚’å¼•ç”¨ã—ã€å‡ºå…¸ã‚’æ˜ç¤ºã—ã¦ãã ã•ã„ã€‚"
                            },
                            {"role": "user", "content": prompt},
                        ],
                        tools=[{"type": "web_search"}],
                        temperature=0.3,
                        max_output_tokens=4000,
                    )
                except BadRequestError as bre:
                    # 2) ã€ŒResponses APIéå¯¾å¿œã€ã‚¨ãƒ©ãƒ¼ â†’ Chat Completions ã§åŒä¸€ãƒ¢ãƒ‡ãƒ«ã‚’å©ã
                    if "not supported with the Responses API" in str(bre) or "400" in str(bre):
                        print(f"â„¹ï¸ Responses APIéå¯¾å¿œã®ãŸã‚ã€Chat Completionsã§åŒä¸€ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¾ã™")
                        # search-previewç³»ãƒ¢ãƒ‡ãƒ«ã¯temperatureã‚’å—ã‘ä»˜ã‘ãªã„ãŸã‚å‰Šé™¤
                        response = self.client.chat.completions.create(
                            model=self.model_primary,
                            messages=[
                                {
                                    "role": "system",
                                    "content": "ã‚ãªãŸã¯æœªæ¥æ´å¯Ÿã®å°‚é–€å®¶ã§ã™ã€‚å¿…ãšå®Ÿåœ¨ã™ã‚‹è¨˜äº‹ã ã‘ã‚’å¼•ç”¨ã—ã€å‡ºå…¸ã‚’æ˜ç¤ºã—ã¦ãã ã•ã„ã€‚"
                                },
                                {"role": "user", "content": prompt},
                            ],
                            max_tokens=4000,
                        )
                    else:
                        raise
                except NotFoundError:
                    # 3) ãƒ¢ãƒ‡ãƒ«æœªé–‹æ”¾ â†’ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ¢ãƒ‡ãƒ«ã«åˆ‡æ›¿ï¼ˆResponsesâ†’ãƒ€ãƒ¡ãªã‚‰Chatã¸ï¼‰
                    print(f"âš ï¸ ãƒ¢ãƒ‡ãƒ« {self.model_primary} ã¸ã‚¢ã‚¯ã‚»ã‚¹ä¸å¯ã€‚{self.model_fallback} ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã—ã¾ã™")
                    try:
                        response = self.client.responses.create(
                            model=self.model_fallback,
                            input=[
                                {
                                    "role": "system",
                                    "content": "ã‚ãªãŸã¯æœªæ¥æ´å¯Ÿã®å°‚é–€å®¶ã§ã™ã€‚å¿…ãšå®Ÿåœ¨ã™ã‚‹è¨˜äº‹ã ã‘ã‚’å¼•ç”¨ã—ã€å‡ºå…¸ã‚’æ˜ç¤ºã—ã¦ãã ã•ã„ã€‚"
                                },
                                {"role": "user", "content": prompt},
                            ],
                            tools=[{"type": "web_search"}],
                            temperature=0.3,
                            max_output_tokens=4000,
                        )
                    except BadRequestError:
                        # Responses APIéå¯¾å¿œãªã‚‰Chat Completionsã§è©¦ã™
                        # search-previewç³»ãƒ¢ãƒ‡ãƒ«ã¯temperatureã‚’å—ã‘ä»˜ã‘ãªã„ãŸã‚å‰Šé™¤
                        response = self.client.chat.completions.create(
                            model=self.model_fallback,
                            messages=[
                                {
                                    "role": "system",
                                    "content": "ã‚ãªãŸã¯æœªæ¥æ´å¯Ÿã®å°‚é–€å®¶ã§ã™ã€‚å¿…ãšå®Ÿåœ¨ã™ã‚‹è¨˜äº‹ã ã‘ã‚’å¼•ç”¨ã—ã€å‡ºå…¸ã‚’æ˜ç¤ºã—ã¦ãã ã•ã„ã€‚"
                                },
                                {"role": "user", "content": prompt},
                            ],
                            max_tokens=4000,
                        )
                
                if attempt > 0:
                    print(f"âœ… ãƒªãƒˆãƒ©ã‚¤æˆåŠŸï¼ˆè©¦è¡Œå›æ•°: {attempt + 1}ï¼‰")
                return response
                
            except Exception as e:
                last_exception = e
                error_type = type(e).__name__
                error_str = str(e).lower()
                
                # ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚¨ãƒ©ãƒ¼ï¼ˆ429ï¼‰
                if "429" in error_str or "rate limit" in error_str or "quota" in error_str:
                    print(f"âš ï¸ ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚¨ãƒ©ãƒ¼ (429): {e}")
                    if attempt < max_retries:
                        continue
                    raise
                # ã‚µãƒ¼ãƒãƒ¼ã‚¨ãƒ©ãƒ¼ï¼ˆ500, 502, 503ï¼‰
                elif "500" in error_str or "502" in error_str or "503" in error_str or "service unavailable" in error_str or "internal server error" in error_str:
                    print(f"âš ï¸ ã‚µãƒ¼ãƒãƒ¼ã‚¨ãƒ©ãƒ¼: {e}")
                    if attempt < max_retries:
                        continue
                    raise
                # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
                elif "timeout" in error_str or "timed out" in error_str:
                    print(f"âš ï¸ ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
                    if attempt < max_retries:
                        continue
                    raise
                else:
                    # ãƒªãƒˆãƒ©ã‚¤ä¸å¯ãªã‚¨ãƒ©ãƒ¼ã¯å³åº§ã«å†ç™ºç”Ÿ
                    print(f"âŒ ãƒªãƒˆãƒ©ã‚¤ä¸å¯ãªã‚¨ãƒ©ãƒ¼ ({error_type}): {e}")
                    raise
        
        # ã™ã¹ã¦ã®ãƒªãƒˆãƒ©ã‚¤ãŒå¤±æ•—ã—ãŸå ´åˆ
        print(f"âŒ æœ€å¤§ãƒªãƒˆãƒ©ã‚¤å›æ•° ({max_retries}) ã«é”ã—ã¾ã—ãŸã€‚æœ€å¾Œã®ã‚¨ãƒ©ãƒ¼: {last_exception}")
        raise last_exception
    
    def parse_research_results(self, research_text: str) -> List[Dict]:
        """
        DeepResearchã®çµæœã‚’ãƒ‘ãƒ¼ã‚¹ã—ã¦è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆã«å¤‰æ›
        
        Args:
            research_text: DeepResearchã®çµæœãƒ†ã‚­ã‚¹ãƒˆ
        
        Returns:
            è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆï¼ˆurl, title, content, published_at, theme, clipping_reason, summary, future_signalã‚’å«ã‚€ï¼‰
        """
        articles = []
        
        # ãƒ†ãƒ¼ãƒã”ã¨ã«ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’åˆ†å‰²ï¼ˆå…¨è§’/åŠè§’ã‚³ãƒ­ãƒ³ä¸¡å¯¾å¿œï¼‰
        theme_sections = re.split(r'ã€ãƒ†ãƒ¼ãƒ\d+[:ï¼š]', research_text)
        
        for section in theme_sections[1:]:  # æœ€åˆã®è¦ç´ ã¯ç©ºã®å¯èƒ½æ€§ãŒã‚ã‚‹ã®ã§ã‚¹ã‚­ãƒƒãƒ—
            # ãƒ†ãƒ¼ãƒåã‚’æŠ½å‡º
            theme_match = re.match(r'([^ã€‘]+)ã€‘', section)
            if not theme_match:
                continue
            
            theme = theme_match.group(1).strip()
            
            # è¨˜äº‹ã‚’æŠ½å‡ºï¼ˆåŒºåˆ‡ã‚Šç·š---ã§åˆ†å‰²ï¼‰
            article_blocks = re.split(r'---', section)
            
            for block in article_blocks:
                article = self._parse_article_block(block, theme)
                if article:
                    articles.append(article)
        
        return articles
    
    def _clean_url(self, url: str) -> str:
        """
        URLã‹ã‚‰è£…é£¾ã‚„ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã‚’é™¤å»
        
        Args:
            url: å…ƒã®URLæ–‡å­—åˆ—
        
        Returns:
            ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã•ã‚ŒãŸURL
        """
        if not url:
            return url
        
        u = url.strip()
        
        # å…ˆé ­ã®è£…é£¾é™¤å»
        for pref in ["**", "*", "<", "(", "[", "ï¼š", ":", "|"]:
            if u.startswith(pref):
                u = u[len(pref):].lstrip()
        
        # æœ«å°¾ã®è£…é£¾é™¤å»
        for suf in ["**", "*", ">", ")", "]", "|", "ã€‚", "ã€", ",", "."]:
            if u.endswith(suf):
                u = u[:-len(suf)].rstrip()
        
        # ä½™è¨ˆãªå…¨è§’/åŠè§’ã‚¹ãƒšãƒ¼ã‚¹é™¤å»
        u = u.strip()
        
        return u
    
    def _validate_url(self, url: str) -> bool:
        """
        URLã®å¦¥å½“æ€§ã‚’æ¤œè¨¼
        
        Args:
            url: æ¤œè¨¼ã™ã‚‹URL
        
        Returns:
            å¦¥å½“ãªURLã‹ã©ã†ã‹
        """
        if not url:
            return False
        
        # URLã®å½¢å¼ã‚’ãƒã‚§ãƒƒã‚¯
        if not url.startswith(('http://', 'https://')):
            print(f"âš ï¸ ç„¡åŠ¹ãªURLå½¢å¼: {url}")
            return False
        
        # æŒ‡å®šãƒ¡ãƒ‡ã‚£ã‚¢ãƒªã‚¹ãƒˆã®ãƒ‰ãƒ¡ã‚¤ãƒ³ã‹ãƒã‚§ãƒƒã‚¯ï¼ˆåŸºæœ¬çš„ãªæ¤œè¨¼ï¼‰
        # å®Œå…¨ãªæ¤œè¨¼ã¯é›£ã—ã„ãŒã€å°‘ãªãã¨ã‚‚å½¢å¼ã¯ç¢ºèª
        try:
            parsed = urlparse(url)
            if not parsed.netloc:
                print(f"âš ï¸ ç„¡åŠ¹ãªURLï¼ˆãƒ‰ãƒ¡ã‚¤ãƒ³ãªã—ï¼‰: {url}")
                return False
        except Exception as e:
            print(f"âš ï¸ URLè§£æã‚¨ãƒ©ãƒ¼: {e}")
            return False
        
        return True
    
    def _parse_article_block(self, block: str, theme: str) -> Optional[Dict]:
        """
        è¨˜äº‹ãƒ–ãƒ­ãƒƒã‚¯ã‚’ãƒ‘ãƒ¼ã‚¹
        
        Args:
            block: è¨˜äº‹ãƒ–ãƒ­ãƒƒã‚¯ã®ãƒ†ã‚­ã‚¹ãƒˆ
            theme: ãƒ†ãƒ¼ãƒå
        
        Returns:
            è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ã®è¾æ›¸ã¾ãŸã¯None
        """
        try:
            # è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«
            title_match = re.search(r'è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«:\s*(.+?)(?=\n|å¼•ç”¨å…ƒ:)', block, re.DOTALL)
            title = title_match.group(1).strip() if title_match else None
            
            # å¼•ç”¨å…ƒ
            source_match = re.search(r'å¼•ç”¨å…ƒ:\s*(.+?)(?=\n|æ²è¼‰å¹´æœˆæ—¥:)', block, re.DOTALL)
            source = source_match.group(1).strip() if source_match else None
            
            # æ²è¼‰å¹´æœˆæ—¥
            date_match = re.search(r'æ²è¼‰å¹´æœˆæ—¥:\s*(.+?)(?=\n|è¨˜äº‹ãƒªãƒ³ã‚¯:)', block, re.DOTALL)
            date_str = date_match.group(1).strip() if date_match else None
            published_at = self._parse_date(date_str) if date_str else None
            
            # è¨˜äº‹ãƒªãƒ³ã‚¯
            url_match = re.search(r'è¨˜äº‹ãƒªãƒ³ã‚¯:\s*(.+?)(?=\n|ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ç†ç”±:)', block, re.DOTALL)
            url = url_match.group(1).strip() if url_match else None
            url = self._clean_url(url) if url else None
            
            # URLã®å¦¥å½“æ€§ã‚’æ¤œè¨¼
            if url and not self._validate_url(url):
                print(f"âš ï¸ ç„¡åŠ¹ãªURLã‚’ã‚¹ã‚­ãƒƒãƒ—: {url}")
                return None
            
            # ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ç†ç”±
            reason_match = re.search(r'ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ç†ç”±:\s*(.+?)(?=\n|è¨˜äº‹è¦ç´„)', block, re.DOTALL)
            clipping_reason = reason_match.group(1).strip() if reason_match else None
            
            # è¨˜äº‹è¦ç´„
            summary_match = re.search(r'è¨˜äº‹è¦ç´„\s*\(150å­—ä»¥å†…\):\s*(.+?)(?=\n|æœªæ¥ã®å…†ã—)', block, re.DOTALL)
            summary = summary_match.group(1).strip() if summary_match else None
            
            # æœªæ¥ã®å…†ã—
            signal_match = re.search(r'æœªæ¥ã®å…†ã—\s*\(150å­—ä»¥å†…\):\s*(.+?)(?=\n|$)', block, re.DOTALL)
            future_signal = signal_match.group(1).strip() if signal_match else None
            
            # å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®ãƒã‚§ãƒƒã‚¯
            if not title or not url:
                return None
            
            # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯è¦ç´„ã¨ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ç†ç”±ã‚’çµ„ã¿åˆã‚ã›ã‚‹
            content = f"{summary or ''}\n\n{clipping_reason or ''}\n\n{future_signal or ''}".strip()
            
            return {
                'url': url,
                'title': title,
                'content': content[:5000] if content else None,  # æœ€åˆã®5000æ–‡å­—
                'published_at': published_at,
                'theme': theme,
                'source': source,
                'clipping_reason': clipping_reason,
                'summary': summary,
                'future_signal': future_signal
            }
            
        except Exception as e:
            print(f"âš ï¸ è¨˜äº‹ãƒ–ãƒ­ãƒƒã‚¯ãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def _parse_date(self, date_str: str) -> Optional[datetime]:
        """
        æ—¥ä»˜æ–‡å­—åˆ—ã‚’ãƒ‘ãƒ¼ã‚¹
        
        Args:
            date_str: æ—¥ä»˜æ–‡å­—åˆ—
        
        Returns:
            datetimeã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã¾ãŸã¯None
        """
        if not date_str:
            return None
        
        # æ§˜ã€…ãªæ—¥ä»˜å½¢å¼ã‚’è©¦ã™
        date_formats = [
            '%Yå¹´%mæœˆ%dæ—¥',
            '%Y/%m/%d',
            '%Y-%m-%d',
            '%Yå¹´%mæœˆ%dæ—¥ %H:%M',
            '%Y/%m/%d %H:%M:%S',
        ]
        
        for fmt in date_formats:
            try:
                return datetime.strptime(date_str.strip(), fmt)
            except ValueError:
                continue
        
        return None
    
    def fetch_articles_by_themes(self, themes: str) -> List[Dict]:
        """
        ãƒ†ãƒ¼ãƒã‚’æŒ‡å®šã—ã¦è¨˜äº‹ã‚’å–å¾—
        
        Args:
            themes: ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã®ãƒ†ãƒ¼ãƒãƒªã‚¹ãƒˆ
        
        Returns:
            è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆ
        """
        print(f"ğŸ” OpenAI Responses API ({self.model_primary}) ã§è¨˜äº‹æ¤œç´¢ã‚’å®Ÿè¡Œä¸­: {themes}")
        
        # ã¾ãšã¯LLMã®"DeepResearché¢¨"å‡ºåŠ›ã‚’è©¦ã™ï¼ˆäº’æ›ç¶­æŒï¼‰
        try:
            research_text = self.run_deep_research(themes)
            articles = self.parse_research_results(research_text)
        except Exception as e:
            print(f"âš ï¸ LLMå‡ºåŠ›è§£æã«å¤±æ•—: {e}")
            articles = []
        
        if articles:
            print(f"âœ… {len(articles)}ä»¶ã®è¨˜äº‹ã‚’å–å¾—ï¼ˆLLMï¼‰")
            return articles
        
        # ====== ã“ã“ã‹ã‚‰ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šRSSã§å¿…ãšæ‹¾ã† ======
        print("â„¹ï¸ LLMå‡ºåŠ›ãŒç©ºã ã£ãŸãŸã‚ã€RSSãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã§å–å¾—ã—ã¾ã™ã€‚")
        fb = self._fallback_fetch_from_rss(themes)
        print(f"âœ… {len(fb)}ä»¶ã®è¨˜äº‹ã‚’å–å¾—ï¼ˆRSSãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰")
        return fb
    
    # ----------------------------
    # RSSãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å®Ÿè£…
    # ----------------------------
    def _fallback_fetch_from_rss(self, themes: str, max_items_per_feed: int = 10) -> List[Dict]:
        """
        RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰è¨˜äº‹ã‚’å–å¾—ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ï¼‰
        
        Args:
            themes: ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã®ãƒ†ãƒ¼ãƒãƒªã‚¹ãƒˆ
            max_items_per_feed: ãƒ•ã‚£ãƒ¼ãƒ‰ã”ã¨ã®æœ€å¤§å–å¾—ä»¶æ•°
        
        Returns:
            è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆ
        """
        theme_list = [t.strip() for t in themes.split(",") if t.strip()]
        
        # ãƒ†ãƒ¼ãƒåˆ¥RSSãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒãƒ—
        rss_map = {
            "AI": [
                "https://openai.com/blog/rss.xml",
                "https://www.deepmind.com/blog/rss.xml",
                "https://arxiv.org/rss/cs.AI",
                "https://hnrss.org/newest?points=50&count=100",
            ],
            "ç”ŸæˆAI": [
                "https://huggingface.co/blog/feed.xml",
                "https://stability.ai/blog?format=rss",
                "https://replicate.com/site/blog.atom",
            ],
            "AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ": [
                "https://www.anthropic.com/news/rss.xml",
                "https://www.semianalysis.com/feed",
            ],
        }
        
        feeds: List[str] = []
        for t in theme_list:
            feeds.extend(rss_map.get(t, []))
        
        # ãƒ†ãƒ¼ãƒãŒãƒãƒƒãƒ—å¤–ã§ã‚‚æœ€ä½é™ã„ãã¤ã‹å½“ã¦ã‚‹
        if not feeds:
            feeds = [
                "https://www.technologyreview.com/feed/",
                "https://wired.jp/rssfeeder/",
                "https://techcrunch.com/feed/",
            ]
        
        seen = set()
        out: List[Dict] = []
        now = datetime.utcnow()
        
        for url in feeds:
            try:
                parsed = feedparser.parse(url)
                for e in parsed.entries[:max_items_per_feed]:
                    link = getattr(e, "link", "") or ""
                    if not link or link in seen or not self._validate_url(link):
                        continue
                    seen.add(link)
                    
                    title = (getattr(e, "title", "") or "").strip()
                    source = urlparse(link).netloc
                    
                    # æ—¥ä»˜ï¼ˆãªã‘ã‚Œã°Noneï¼‰
                    published = None
                    for key in ("published_parsed", "updated_parsed"):
                        val = getattr(e, key, None)
                        if val:
                            try:
                                published = datetime(*val[:6])
                                break
                            except Exception:
                                pass
                    
                    # 3ãƒ¶æœˆåˆ¶é™ï¼ˆå³ã—ã‚ã«UTCã§åˆ¤å®šï¼‰
                    if published and (now - published).days > 93:
                        continue
                    
                    theme_for_entry = self._guess_theme_for_entry(title, theme_list)
                    
                    # æœ¬æ–‡ä»£æ›¿ï¼ˆRSSã®summaryã‚’è–„ãä½¿ã†ï¼‰
                    summary = (getattr(e, "summary", "") or "").strip()
                    content = summary[:5000] if summary else None
                    
                    out.append({
                        "url": link,
                        "title": title or "(ç„¡é¡Œ)",
                        "content": content,
                        "published_at": published,
                        "theme": theme_for_entry,
                        "source": source,
                        "clipping_reason": None,
                        "summary": summary[:300] if summary else None,
                        "future_signal": None,
                    })
            except Exception as ex:
                print(f"âš ï¸ RSSå–å¾—å¤±æ•—: {url} ({ex})")
                continue
        
        return out
    
    def _guess_theme_for_entry(self, title: str, theme_list: List[str]) -> str:
        """
        ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰ãƒ†ãƒ¼ãƒã‚’æ¨æ¸¬
        
        Args:
            title: è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«
            theme_list: ãƒ†ãƒ¼ãƒãƒªã‚¹ãƒˆ
        
        Returns:
            æ¨æ¸¬ã•ã‚ŒãŸãƒ†ãƒ¼ãƒ
        """
        t = title.lower()
        for theme in theme_list:
            k = theme.lower()
            if k in t:
                return theme
        
        # ãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ãƒƒã‚¯
        if any(k in t for k in ["agent", "autonomous", "tool use"]):
            return "AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ" if "AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ" in theme_list else (theme_list[0] if theme_list else "AI")
        if any(k in t for k in ["genai", "llm", "diffusion", "stable", "gpt", "o4", "4o"]):
            return "ç”ŸæˆAI" if "ç”ŸæˆAI" in theme_list else (theme_list[0] if theme_list else "AI")
        
        return theme_list[0] if theme_list else "AI"

