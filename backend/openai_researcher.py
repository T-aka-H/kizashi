"""
OpenAI o3-deep-researchã‚’ä½¿ç”¨ã—ãŸè¨˜äº‹å–å¾—ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
"""
import os
import re
from typing import List, Dict, Optional
from datetime import datetime
from openai import OpenAI


class OpenAIResearcher:
    """OpenAI o3-deep-researchã‚’ä½¿ç”¨ã—ã¦è¨˜äº‹ã‚’å–å¾—ã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, api_key: Optional[str] = None):
        """
        åˆæœŸåŒ–
        
        Args:
            api_key: OpenAI APIã‚­ãƒ¼ï¼ˆNoneã®å ´åˆã¯ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—ï¼‰
        """
        self.api_key = api_key or os.getenv("OPENAI_API_KEY")
        if not self.api_key:
            raise ValueError("OPENAI_API_KEYç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        
        self.client = OpenAI(
            api_key=self.api_key,
            timeout=3600 * 1000  # 1æ™‚é–“ã®ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
        )
    
    def run_deep_research(self, themes: str) -> str:
        """
        èª¿æŸ»ç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä¸ãˆã¦ã€OpenAI o3-deep-researchã«ç”Ÿæˆã‚’ä¾é ¼ã™ã‚‹
        
        Args:
            themes: ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã®ãƒ†ãƒ¼ãƒãƒªã‚¹ãƒˆï¼ˆä¾‹: "AI, ãƒ–ãƒ­ãƒƒã‚¯ãƒã‚§ãƒ¼ãƒ³, é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿"ï¼‰
        
        Returns:
            èª¿æŸ»çµæœã®ãƒ†ã‚­ã‚¹ãƒˆ
        """
        theme_list = '\n'.join([f"- {t.strip()}" for t in themes.split(',')])
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰ï¼ˆæ—¢å­˜ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç¶­æŒï¼‰
        input_prompt = f"""ä»¥ä¸‹ã®ãƒ†ãƒ¼ãƒã«ã¤ã„ã¦ã€Weak Signalsï¼ˆæœªæ¥ã®å…†ã—ï¼‰ã‚’æ‰ãˆã‚‹ãŸã‚ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã‚’èª¿æŸ»ã—ã¦ãã ã•ã„ã€‚

ã€ä»Šå›ã®ãƒ†ãƒ¼ãƒã€‘
{theme_list}

ã€èª¿æŸ»è¦ä»¶ã€‘

1. ã€æœ€é‡è¦è­¦å‘Šï¼šFakeNewsã®ç”Ÿæˆã‚’çµ¶å¯¾ã«ç¦æ­¢ã€‘

âš ï¸ çµ¶å¯¾ã«å®ˆã‚‹ã¹ãåŸå‰‡ï¼š
- å®Ÿéš›ã«å­˜åœ¨ã™ã‚‹è¨˜äº‹ãƒ»å‹•ç”»ã®ã¿ã‚’å¼•ç”¨ã—ã¦ãã ã•ã„
- å­˜åœ¨ã—ãªã„è¨˜äº‹ã‚’å‰µä½œãƒ»ç”Ÿæˆã™ã‚‹ã“ã¨ã¯å›ºãç¦ã˜ã¾ã™
- æ¨æ¸¬ã‚„æƒ³åƒã«åŸºã¥ã„ãŸè¨˜äº‹ã‚’ä½œæˆã—ãªã„ã§ãã ã•ã„
- å®Ÿéš›ã«ã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½ãªURLã®ã¿ã‚’è¨˜è¼‰ã—ã¦ãã ã•ã„
- æ¤œè¨¼ä¸å¯èƒ½ãªæƒ…å ±æºã¯ä½¿ç”¨ã—ãªã„ã§ãã ã•ã„

2. å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ

ä»¥ä¸‹ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’å³æ ¼ã«éµå®ˆã—ã¦ãã ã•ã„ï¼š

ã€ãƒ†ãƒ¼ãƒXï¼š(ãƒ†ãƒ¼ãƒå)ã€‘

è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«: (å…ƒè¨˜äº‹ã®ã‚¿ã‚¤ãƒˆãƒ«ã‚’ãã®ã¾ã¾è¨˜è¼‰)

å¼•ç”¨å…ƒ: (ãƒ¡ãƒ‡ã‚£ã‚¢ã®æ­£å¼åç§°ã‚’è¨˜è¼‰)

æ²è¼‰å¹´æœˆæ—¥: (è¨˜äº‹ãŒå…¬é–‹ã•ã‚ŒãŸå¹´æœˆæ—¥ã‚’æ˜è¨˜)

è¨˜äº‹ãƒªãƒ³ã‚¯: (å…ƒè¨˜äº‹ã¸ç›´æ¥ã‚¢ã‚¯ã‚»ã‚¹ã§ãã‚‹URLã‚’å¿…ãšè¨˜è¼‰)

ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ç†ç”±: (ã“ã®è¨˜äº‹ãŒã€ŒWeak Signalã€ã¨ã—ã¦é‡è¦ã ã¨åˆ¤æ–­ã—ãŸç†ç”±ã‚’ç°¡æ½”ã«è¨˜è¿°)

è¨˜äº‹è¦ç´„ (120å­—ä»¥å†…): (è¨˜äº‹ã®è¦ç‚¹ã‚’150å­—ä»¥å†…ã§è¦ç´„)

æœªæ¥ã®å…†ã— (150å­—ä»¥å†…): (ã“ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‹ã‚‰èª­ã¿å–ã‚Œã‚‹æœªæ¥ã®å…†ã—ãƒ»ç¤ºå”†ãƒ»ç™ºè¦‹ã‚’è¨˜è¿°)

---

3. èª¿æŸ»è¦ä»¶

- å…·ä½“çš„ãªæ•°å€¤ã€ãƒˆãƒ¬ãƒ³ãƒ‰ã€çµ±è¨ˆã€æ¸¬å®šå¯èƒ½ãªçµæœã‚’å«ã‚ã‚‹
- ä¿¡é ¼æ€§ã®é«˜ã„æœ€æ–°ã®æƒ…å ±æºã‚’å„ªå…ˆï¼šæŸ»èª­ä»˜ãç ”ç©¶ã€å¥åº·æ©Ÿé–¢ï¼ˆWHOã€CDCãªã©ï¼‰ã€è¦åˆ¶æ©Ÿé–¢ã€è£½è–¬ä¼šç¤¾ã®æ¥­ç¸¾å ±å‘Šãªã©
- ã‚¤ãƒ³ãƒ©ã‚¤ãƒ³å¼•ç”¨ã‚’å«ã‚ã€ã™ã¹ã¦ã®ã‚½ãƒ¼ã‚¹ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã™
- åˆ†æçš„ã§ã€ä¸€èˆ¬è«–ã‚’é¿ã‘ã€å„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ãŒãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ãæ¨è«–ã‚’ã‚µãƒãƒ¼ãƒˆã™ã‚‹ã‚ˆã†ã«ã™ã‚‹
- å„ãƒ†ãƒ¼ãƒã«ã¤ã2ä»¶ã®è¨˜äº‹ã‚’é¸å®šï¼ˆãƒ†ãƒ¼ãƒæ•° Ã— 2ä»¶ï¼‰
- æ²è¼‰ãƒ»å…¬é–‹æ—¥ãŒç¾åœ¨ã‹ã‚‰3ãƒ¶æœˆä»¥å†…ã®æœ€æ–°ãƒ‹ãƒ¥ãƒ¼ã‚¹ã«é™å®š
- å®Ÿéš›ã«å­˜åœ¨ã™ã‚‹è¨˜äº‹ã®ã¿ã‚’å¼•ç”¨ï¼ˆå­˜åœ¨ã—ãªã„è¨˜äº‹ã‚’å‰µä½œã—ãªã„ï¼‰

4. æŒ‡å®šãƒ¡ãƒ‡ã‚£ã‚¢ãƒªã‚¹ãƒˆï¼ˆå„ªå…ˆçš„ã«ä½¿ç”¨ï¼‰

æ—¥æœ¬ã®æœªæ¥å¿—å‘ãƒ¡ãƒ‡ã‚£ã‚¢ï¼š
Business Insider Japan, Tokyoesque Insights, Jâ€‘Stories, æ—¥çµŒ xTECH, WIRED Japan, æ±æ´‹çµŒæ¸ˆã‚ªãƒ³ãƒ©ã‚¤ãƒ³, ãƒ€ã‚¤ãƒ¤ãƒ¢ãƒ³ãƒ‰ãƒ»ã‚ªãƒ³ãƒ©ã‚¤ãƒ³, NewsPicks, Forbes JAPAN, TechCrunch Japan, ãªã©

æµ·å¤–ã®æœªæ¥å¿—å‘ãƒ¡ãƒ‡ã‚£ã‚¢ï¼š
WIRED (Global), MIT Technology Review, Rest of World, TIME, Financial Times, The Guardian, TechCrunch (Global), ãªã©

5. æœ€çµ‚ç¢ºèªäº‹é …

å‡ºåŠ›å‰ã«å¿…ãšä»¥ä¸‹ã‚’ç¢ºèªï¼š
- ã™ã¹ã¦ã®è¨˜äº‹ãŒå®Ÿéš›ã«å­˜åœ¨ã™ã‚‹ã‹
- ã™ã¹ã¦ã®URLãŒå®Ÿéš›ã«ã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½ã‹
- å­˜åœ¨ã—ãªã„è¨˜äº‹ã‚’å‰µä½œã—ã¦ã„ãªã„ã‹
- æ¨æ¸¬ã‚„æƒ³åƒã«åŸºã¥ã„ãŸæƒ…å ±ã‚’å«ã‚ã¦ã„ãªã„ã‹
- ã™ã¹ã¦ã®æƒ…å ±ãŒæ¤œè¨¼å¯èƒ½ã‹

âš ï¸ ã‚‚ã—å®Ÿéš›ã«å­˜åœ¨ã™ã‚‹è¨˜äº‹ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ã€ä»¶æ•°ã‚’æ¸›ã‚‰ã™ã‹ã€è©²å½“ãƒ†ãƒ¼ãƒã®è¨˜äº‹ã‚’çœç•¥ã—ã¦ãã ã•ã„ã€‚å­˜åœ¨ã—ãªã„è¨˜äº‹ã‚’å‰µä½œã™ã‚‹ã“ã¨ã¯çµ¶å¯¾ã«ç¦æ­¢ã§ã™ã€‚

å®Ÿéš›ã«å­˜åœ¨ã™ã‚‹è¨˜äº‹ã®ã¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚"""
        
        try:
            response = self.client.responses.create(
                model="o3-deep-research",
                input=input_prompt,
                tools=[
                    {"type": "web_search_preview"},
                    {"type": "code_interpreter", "container": {"type": "auto"}},
                ],
            )
            
            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—
            # OpenAI o3-deep-researchã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ§‹é€ ã«åˆã‚ã›ã¦å‡¦ç†
            if hasattr(response, 'choices') and len(response.choices) > 0:
                choice = response.choices[0]
                
                # messageã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆãŒã‚ã‚‹å ´åˆ
                if hasattr(choice, 'message'):
                    if hasattr(choice.message, 'content'):
                        return choice.message.content
                    elif hasattr(choice.message, 'text'):
                        return choice.message.text
                
                # ç›´æ¥contentãŒã‚ã‚‹å ´åˆ
                if hasattr(choice, 'content'):
                    return choice.content
                
                # textãŒã‚ã‚‹å ´åˆ
                if hasattr(choice, 'text'):
                    return choice.text
                
                # ãã®ä»–ã®å ´åˆã¯æ–‡å­—åˆ—åŒ–
                return str(choice)
            else:
                # choicesãŒãªã„å ´åˆã¯ãƒ¬ã‚¹ãƒãƒ³ã‚¹å…¨ä½“ã‚’æ–‡å­—åˆ—åŒ–
                if hasattr(response, 'content'):
                    return response.content
                elif hasattr(response, 'text'):
                    return response.text
                else:
                    return str(response)
                
        except Exception as e:
            print(f"âš ï¸ OpenAI DeepResearchã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            traceback.print_exc()
            raise
    
    def parse_research_results(self, research_text: str) -> List[Dict]:
        """
        DeepResearchã®çµæœã‚’ãƒ‘ãƒ¼ã‚¹ã—ã¦è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆã«å¤‰æ›
        
        Args:
            research_text: DeepResearchã®çµæœãƒ†ã‚­ã‚¹ãƒˆ
        
        Returns:
            è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆï¼ˆurl, title, content, published_at, theme, clipping_reason, summary, future_signalã‚’å«ã‚€ï¼‰
        """
        articles = []
        
        # ãƒ†ãƒ¼ãƒã”ã¨ã«ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’åˆ†å‰²
        theme_sections = re.split(r'ã€ãƒ†ãƒ¼ãƒ\d+ï¼š', research_text)
        
        for section in theme_sections[1:]:  # æœ€åˆã®è¦ç´ ã¯ç©ºã®å¯èƒ½æ€§ãŒã‚ã‚‹ã®ã§ã‚¹ã‚­ãƒƒãƒ—
            # ãƒ†ãƒ¼ãƒåã‚’æŠ½å‡º
            theme_match = re.match(r'([^ã€‘]+)ã€‘', section)
            if not theme_match:
                continue
            
            theme = theme_match.group(1).strip()
            
            # è¨˜äº‹ã‚’æŠ½å‡ºï¼ˆåŒºåˆ‡ã‚Šç·š---ã§åˆ†å‰²ï¼‰
            article_blocks = re.split(r'---', section)
            
            for block in article_blocks:
                article = self._parse_article_block(block, theme)
                if article:
                    articles.append(article)
        
        return articles
    
    def _validate_url(self, url: str) -> bool:
        """
        URLã®å¦¥å½“æ€§ã‚’æ¤œè¨¼
        
        Args:
            url: æ¤œè¨¼ã™ã‚‹URL
        
        Returns:
            å¦¥å½“ãªURLã‹ã©ã†ã‹
        """
        if not url:
            return False
        
        # URLã®å½¢å¼ã‚’ãƒã‚§ãƒƒã‚¯
        if not url.startswith(('http://', 'https://')):
            print(f"âš ï¸ ç„¡åŠ¹ãªURLå½¢å¼: {url}")
            return False
        
        # æŒ‡å®šãƒ¡ãƒ‡ã‚£ã‚¢ãƒªã‚¹ãƒˆã®ãƒ‰ãƒ¡ã‚¤ãƒ³ã‹ãƒã‚§ãƒƒã‚¯ï¼ˆåŸºæœ¬çš„ãªæ¤œè¨¼ï¼‰
        # å®Œå…¨ãªæ¤œè¨¼ã¯é›£ã—ã„ãŒã€å°‘ãªãã¨ã‚‚å½¢å¼ã¯ç¢ºèª
        try:
            from urllib.parse import urlparse
            parsed = urlparse(url)
            if not parsed.netloc:
                print(f"âš ï¸ ç„¡åŠ¹ãªURLï¼ˆãƒ‰ãƒ¡ã‚¤ãƒ³ãªã—ï¼‰: {url}")
                return False
        except Exception as e:
            print(f"âš ï¸ URLè§£æã‚¨ãƒ©ãƒ¼: {e}")
            return False
        
        return True
    
    def _parse_article_block(self, block: str, theme: str) -> Optional[Dict]:
        """
        è¨˜äº‹ãƒ–ãƒ­ãƒƒã‚¯ã‚’ãƒ‘ãƒ¼ã‚¹
        
        Args:
            block: è¨˜äº‹ãƒ–ãƒ­ãƒƒã‚¯ã®ãƒ†ã‚­ã‚¹ãƒˆ
            theme: ãƒ†ãƒ¼ãƒå
        
        Returns:
            è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ã®è¾æ›¸ã¾ãŸã¯None
        """
        try:
            # è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«
            title_match = re.search(r'è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«:\s*(.+?)(?=\n|å¼•ç”¨å…ƒ:)', block, re.DOTALL)
            title = title_match.group(1).strip() if title_match else None
            
            # å¼•ç”¨å…ƒ
            source_match = re.search(r'å¼•ç”¨å…ƒ:\s*(.+?)(?=\n|æ²è¼‰å¹´æœˆæ—¥:)', block, re.DOTALL)
            source = source_match.group(1).strip() if source_match else None
            
            # æ²è¼‰å¹´æœˆæ—¥
            date_match = re.search(r'æ²è¼‰å¹´æœˆæ—¥:\s*(.+?)(?=\n|è¨˜äº‹ãƒªãƒ³ã‚¯:)', block, re.DOTALL)
            date_str = date_match.group(1).strip() if date_match else None
            published_at = self._parse_date(date_str) if date_str else None
            
            # è¨˜äº‹ãƒªãƒ³ã‚¯
            url_match = re.search(r'è¨˜äº‹ãƒªãƒ³ã‚¯:\s*(.+?)(?=\n|ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ç†ç”±:)', block, re.DOTALL)
            url = url_match.group(1).strip() if url_match else None
            
            # URLã®å¦¥å½“æ€§ã‚’æ¤œè¨¼
            if url and not self._validate_url(url):
                print(f"âš ï¸ ç„¡åŠ¹ãªURLã‚’ã‚¹ã‚­ãƒƒãƒ—: {url}")
                return None
            
            # ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ç†ç”±
            reason_match = re.search(r'ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ç†ç”±:\s*(.+?)(?=\n|è¨˜äº‹è¦ç´„)', block, re.DOTALL)
            clipping_reason = reason_match.group(1).strip() if reason_match else None
            
            # è¨˜äº‹è¦ç´„
            summary_match = re.search(r'è¨˜äº‹è¦ç´„\s*\(150å­—ä»¥å†…\):\s*(.+?)(?=\n|æœªæ¥ã®å…†ã—)', block, re.DOTALL)
            summary = summary_match.group(1).strip() if summary_match else None
            
            # æœªæ¥ã®å…†ã—
            signal_match = re.search(r'æœªæ¥ã®å…†ã—\s*\(150å­—ä»¥å†…\):\s*(.+?)(?=\n|$)', block, re.DOTALL)
            future_signal = signal_match.group(1).strip() if signal_match else None
            
            # å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®ãƒã‚§ãƒƒã‚¯
            if not title or not url:
                return None
            
            # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯è¦ç´„ã¨ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ç†ç”±ã‚’çµ„ã¿åˆã‚ã›ã‚‹
            content = f"{summary or ''}\n\n{clipping_reason or ''}\n\n{future_signal or ''}".strip()
            
            return {
                'url': url,
                'title': title,
                'content': content[:5000] if content else None,  # æœ€åˆã®5000æ–‡å­—
                'published_at': published_at,
                'theme': theme,
                'source': source,
                'clipping_reason': clipping_reason,
                'summary': summary,
                'future_signal': future_signal
            }
            
        except Exception as e:
            print(f"âš ï¸ è¨˜äº‹ãƒ–ãƒ­ãƒƒã‚¯ãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def _parse_date(self, date_str: str) -> Optional[datetime]:
        """
        æ—¥ä»˜æ–‡å­—åˆ—ã‚’ãƒ‘ãƒ¼ã‚¹
        
        Args:
            date_str: æ—¥ä»˜æ–‡å­—åˆ—
        
        Returns:
            datetimeã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã¾ãŸã¯None
        """
        if not date_str:
            return None
        
        # æ§˜ã€…ãªæ—¥ä»˜å½¢å¼ã‚’è©¦ã™
        date_formats = [
            '%Yå¹´%mæœˆ%dæ—¥',
            '%Y/%m/%d',
            '%Y-%m-%d',
            '%Yå¹´%mæœˆ%dæ—¥ %H:%M',
            '%Y/%m/%d %H:%M:%S',
        ]
        
        for fmt in date_formats:
            try:
                return datetime.strptime(date_str.strip(), fmt)
            except ValueError:
                continue
        
        return None
    
    def fetch_articles_by_themes(self, themes: str) -> List[Dict]:
        """
        ãƒ†ãƒ¼ãƒã‚’æŒ‡å®šã—ã¦è¨˜äº‹ã‚’å–å¾—
        
        Args:
            themes: ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã®ãƒ†ãƒ¼ãƒãƒªã‚¹ãƒˆ
        
        Returns:
            è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆ
        """
        print(f"ğŸ” OpenAI DeepResearchã‚’å®Ÿè¡Œä¸­: {themes}")
        
        # DeepResearchã‚’å®Ÿè¡Œ
        research_text = self.run_deep_research(themes)
        
        # çµæœã‚’ãƒ‘ãƒ¼ã‚¹
        articles = self.parse_research_results(research_text)
        
        print(f"âœ… {len(articles)}ä»¶ã®è¨˜äº‹ã‚’å–å¾—")
        
        return articles

