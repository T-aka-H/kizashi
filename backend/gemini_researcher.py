"""
Gemini Groundingï¼ˆGoogle Searchï¼‰ã‚’ä½¿ç”¨ã—ãŸè¨˜äº‹å–å¾—ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
"""
import os
import re
from typing import List, Dict, Optional
from datetime import datetime
import google.generativeai as genai


class GeminiResearcher:
    """Gemini Groundingï¼ˆGoogle Searchï¼‰ã‚’ä½¿ç”¨ã—ã¦è¨˜äº‹ã‚’å–å¾—ã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, api_key: Optional[str] = None, model: str = "gemini-2.5-flash"):
        """
        åˆæœŸåŒ–
        
        Args:
            api_key: Gemini APIã‚­ãƒ¼ï¼ˆNoneã®å ´åˆã¯ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—ï¼‰
            model: ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«å
        """
        self.api_key = api_key or os.getenv("GEMINI_API_KEY")
        if not self.api_key:
            raise ValueError("GEMINI_API_KEYç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        
        genai.configure(api_key=self.api_key)
        self.model = genai.GenerativeModel(model)
    
    def run_deep_research(self, themes: str) -> Dict:
        """
        èª¿æŸ»ç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä¸ãˆã¦ã€Gemini APIã«ç”Ÿæˆã‚’ä¾é ¼ã™ã‚‹ï¼ˆGoogle Search Groundingä½¿ç”¨ï¼‰
        
        Args:
            themes: ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã®ãƒ†ãƒ¼ãƒãƒªã‚¹ãƒˆï¼ˆä¾‹: "AI, ãƒ–ãƒ­ãƒƒã‚¯ãƒã‚§ãƒ¼ãƒ³, é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿"ï¼‰
        
        Returns:
            èª¿æŸ»çµæœã®è¾æ›¸ï¼ˆsummary, sourcesã‚’å«ã‚€ï¼‰
        """
        theme_list = '\n'.join([f"- {t.strip()}" for t in themes.split(',')])
        theme_count = len(themes.split(','))
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰ï¼ˆæä¾›ã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½¿ç”¨ï¼‰
        prompt = f"""1. ã€æœ€é‡è¦æŒ‡ç¤ºã€‘ã‚¿ã‚¹ã‚¯ã®åŸºæœ¬åŸå‰‡ã¨å„ªå…ˆé †ä½

ã‚ãªãŸã®ã‚¿ã‚¹ã‚¯ã¯ã€ä»¥ä¸‹ã®2ã¤ã®åŸå‰‡ã§æ§‹æˆã•ã‚Œã¾ã™ã€‚AIã¨ã—ã¦ã€ã„ã‹ãªã‚‹å ´åˆã‚‚ã“ã®å„ªå…ˆé †ä½ã‚’å³å®ˆã—ã¦ãã ã•ã„ã€‚

1) å³æ ¼ãªãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã®éµå®ˆï¼ˆæœ€å„ªå…ˆï¼‰:

ã‚ãªãŸã®å…¨ã‚¿ã‚¹ã‚¯ã¯ã€ä»¥ä¸‹ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’çµ¶å¯¾çš„ã«éµå®ˆã™ã‚‹ã“ã¨ã«åŸºã¥ãã¾ã™ã€‚ã„ã‹ãªã‚‹è§£é‡ˆã‚ˆã‚Šã‚‚ã“ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆãŒå„ªå…ˆã•ã‚Œã¾ã™ã€‚

å‡ºåŠ›æ§‹é€ : å¿…ãšãƒ†ãƒ¼ãƒã”ã¨ã«ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’åˆ†ã‘ã€ä»¥ä¸‹ã®7é …ç›®ã‚’æŒ‡å®šã•ã‚ŒãŸé †åºã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚

ã€ãƒ†ãƒ¼ãƒXï¼š(ã“ã“ã«ãƒ†ãƒ¼ãƒåãŒå…¥ã‚‹)ã€‘

è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«: (å…ƒè¨˜äº‹ã®ã‚¿ã‚¤ãƒˆãƒ«ã‚’ãã®ã¾ã¾è¨˜è¼‰)

å¼•ç”¨å…ƒ: (ãƒ¡ãƒ‡ã‚£ã‚¢ã®æ­£å¼åç§°ã‚’è¨˜è¼‰)

æ²è¼‰å¹´æœˆæ—¥: (è¨˜äº‹ãŒå…¬é–‹ã•ã‚ŒãŸå¹´æœˆæ—¥ã‚’æ˜è¨˜)

è¨˜äº‹ãƒªãƒ³ã‚¯: (å…ƒè¨˜äº‹ã¸ç›´æ¥ã‚¢ã‚¯ã‚»ã‚¹ã§ãã‚‹URLã‚’å¿…ãšè¨˜è¼‰)

ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ç†ç”±: (ã“ã®è¨˜äº‹ãŒã€ŒWeak Signalã€ã¨ã—ã¦é‡è¦ã ã¨åˆ¤æ–­ã—ãŸç†ç”±ã‚’ç°¡æ½”ã«è¨˜è¿°)

è¨˜äº‹è¦ç´„ (150å­—ä»¥å†…): (è¨˜äº‹ã®è¦ç‚¹ã‚’150å­—ä»¥å†…ã§è¦ç´„)

æœªæ¥ã®å…†ã— (150å­—ä»¥å†…): (ã“ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‹ã‚‰èª­ã¿å–ã‚Œã‚‹æœªæ¥ã®å…†ã—ãƒ»ç¤ºå”†ãƒ»ç™ºè¦‹ã‚’è¨˜è¿°)

åŒºåˆ‡ã‚Šç·š: ãƒ†ãƒ¼ãƒã¨ãƒ†ãƒ¼ãƒã®é–“ã«ã¯ã€å¿…ãšåŒºåˆ‡ã‚Šç·š --- ã‚’æŒ¿å…¥ã—ã¦ãã ã•ã„ã€‚

ä»¶æ•°: ãƒ†ãƒ¼ãƒæ•° Ã— 2ä»¶ã¨ã„ã†é¸å®šç·æ•°ï¼ˆä»Šå›ã¯{theme_count}ãƒ†ãƒ¼ãƒãªã®ã§{theme_count * 2}ä»¶ï¼‰ã‚’å³å®ˆã—ã¦ãã ã•ã„ã€‚

ç¦æ­¢äº‹é …: ã€Œãƒ¬ãƒãƒ¼ãƒˆã€å½¢å¼ã§ã®å‡ºåŠ›ã‚„ã€è¦ç´„ãƒ»åºè«–ãƒ»çµè«–ãƒ»è€ƒå¯Ÿã¨ã„ã£ãŸæŒ‡å®šå¤–ã®æ–‡ç« ã¯ä¸€åˆ‡ç”Ÿæˆã—ãªã„ã§ãã ã•ã„ã€‚æŒ¨æ‹¶ã‚‚ä¸è¦ã§ã™ã€‚

2) è³ªã®é«˜ã„åˆ†æã®å®Ÿè·µ: ä¸Šè¨˜ã®å³æ ¼ãªãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã¨ã„ã†æ çµ„ã¿ã®ä¸­ã§ã€ã‚ãªãŸã®èƒ½åŠ›ã‚’æœ€å¤§é™ã«ç™ºæ®ã—ã¦ãã ã•ã„ã€‚ç‰¹ã«ã€ã‚ãªãŸã®ã€Œæœªæ¥å­¦è€…ã€ã¨ã—ã¦ã®å½¹å‰²ã¯ã€ã€Œã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ç†ç”±ã€ã¨ã€Œæœªæ¥ã®å…†ã—ã€ã®2ã¤ã®é …ç›®ã‚’è¨˜è¿°ã™ã‚‹éš›ã«ã®ã¿é©ç”¨ã—ã¦ãã ã•ã„ã€‚ãã‚Œä»¥å¤–ã®é …ç›®ã‚„å…¨ä½“ã®æ§‹æˆã«ã¯ã€ä¸€åˆ‡ã®å‰µé€ æ€§ã‚’åŠ ãˆãªã„ã§ãã ã•ã„ã€‚

2. ã‚ãªãŸã®å½¹å‰²ã¨å”æ¥­ç›®çš„

ã‚ãªãŸã®å½¹å‰²: è¤‡é›‘ãªçŠ¶æ³ã‹ã‚‰æ–°ãŸãªæ©Ÿä¼šã‚’è¦‹å‡ºã™ã€Œãƒ‡ã‚¶ã‚¤ãƒ³æ€è€ƒã€ã‚’å°‚é–€ã¨ã™ã‚‹æœªæ¥å­¦è€…ã¨ã—ã¦è¡Œå‹•ã—ã¦ãã ã•ã„ã€‚

ç§ã®å½¹å‰²: æœªæ¥æ´å¯Ÿç ”ä¿®ã®ãƒ•ã‚¡ã‚·ãƒªãƒ†ãƒ¼ã‚¿ãƒ¼ã§ã™ã€‚

å”æ¥­ç›®çš„: ç§ãŒå®Ÿæ–½ã™ã‚‹ç ”ä¿®ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã§ä½¿ç”¨ã™ã‚‹ã€Œæœªæ¥ã®å…†ã—ï¼ˆWeak Signalsï¼‰ã€ã‚’æ‰ãˆã‚‹ãŸã‚ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ãƒ»å‹•ç”»ã‚’ã€ã‚ãªãŸãŒåé›†ãƒ»åˆ†æã—ã€ç§ã«æç¤ºã™ã‚‹ã“ã¨ãŒç›®çš„ã§ã™ã€‚

3. ã‚¯ãƒªãƒƒãƒ—å¯¾è±¡ã®å®šç¾©ï¼šWeak Signalsï¼ˆæœªæ¥ã®å…†ã—ï¼‰

å®šç¾©: æœªæ¥ã®å¤§ããªå¤‰åŒ–ã‚’ç¤ºå”†ã™ã‚‹ã€ã¾ã ä¸æ˜ç¢ºã§å°ã•ãªåˆæœŸæ®µéšã®å…†å€™ã‚’æŒ‡ã—ã¾ã™ã€‚æ—¢å­˜ã®ãƒˆãƒ¬ãƒ³ãƒ‰ã‚„ç›´ç·šçš„ãªäºˆæ¸¬ã‹ã‚‰ã¯è¦‹éã”ã•ã‚ŒãŒã¡ãªã€éç·šå½¢çš„ãªå¤‰åŒ–ã®å§‹ã¾ã‚Šã‚’æ‰ãˆã‚‹æ‰‹ãŒã‹ã‚Šã§ã™ã€‚

ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ã®è¦è«¦: èª°ã«ã§ã‚‚äºˆæ¸¬ã§ãã‚‹æ˜ç™½ãªãƒ‹ãƒ¥ãƒ¼ã‚¹ã§ã¯ãªãã€æ³¨æ„æ·±ãè€ƒå¯Ÿã—ãªã‘ã‚Œã°è¦‹è½ã¨ã—ã¦ã—ã¾ã†ã‚ˆã†ãªã€ãƒ¦ãƒ‹ãƒ¼ã‚¯ã‹ã¤å¾®ã‹ãªã€ŒWeak Signalsã€ã‚’å«ã‚€è¨˜äº‹ãƒ»å‹•ç”»ã‚’å³é¸ã—ã¦ãã ã•ã„ã€‚ãƒ†ãƒ¼ãƒã¨ã®é–¢é€£æ€§ãŒä¸€è¦‹ã—ã¦ä¸æ˜ç¢ºãªã‚‚ã®ã§ã‚ã£ã¦ã‚‚ã€ãã“ã‹ã‚‰æ–°ãŸãªæ´å¯Ÿã‚’å¼•ãå‡ºã™ã“ã¨ã‚’æ­“è¿ã—ã¾ã™ã€‚

4. å“è³ªã®ã‚¤ãƒ³ã‚¹ãƒ”ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã¨ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯

æ¨¡ç¯„äº‹ä¾‹: é¸å®šã®ã‚»ãƒ³ã‚¹ã‚„æ–¹æ³•è«–ã®ã‚¤ãƒ³ã‚¹ãƒ”ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æºã¨ã—ã¦ã€æ­¦è”µé‡ç¾è¡“å¤§å­¦ã®å²©åµœåšè«–æ•™æˆã®ã‚¹ã‚¿ã‚¤ãƒ«ã‚’å‚è€ƒã«ã—ã¦ãã ã•ã„ã€‚å…·ä½“çš„ã«ã¯ã€ä¸€è¦‹ç„¡é–¢ä¿‚ã«è¦‹ãˆã‚‹Aã¨ã„ã†äº‹è±¡ã¨Bã¨ã„ã†äº‹è±¡ãŒã€å®Ÿã¯Cã¨ã„ã†æœªæ¥ã®å…†å€™ã‚’ç¤ºã—ã¦ã„ã‚‹ã€ã¨ã„ã£ãŸã€ç™ºè¦‹ã€ã‚„ã€ä»®èª¬ã€ã‚’æç¤ºã™ã‚‹ã“ã¨ã‚’æ„è­˜ã—ã¦ãã ã•ã„ã€‚

NewsPicks: https://newspicks.com/user/134987/

X (æ—§Twitter): https://x.com/hriwsk

è¨˜äº‹ã®æµç”¨: ä¸Šè¨˜ã®å²©åµœæ°ã®ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ã®ä¸­ã«ã€æœ¬ã‚¿ã‚¹ã‚¯ã®ãƒ†ãƒ¼ãƒé ˜åŸŸã«åˆè‡´ã™ã‚‹è‰¯è³ªãªè¨˜äº‹ãŒã‚ã‚Œã°ã€ãã‚Œã‚’é¸æŠãƒ»å¼•ç”¨ã™ã‚‹ã“ã¨ã‚’è¨±å¯ã—ã¾ã™ã€‚

5. ã‚¯ãƒªãƒƒãƒ—å¯¾è±¡ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®å³æ ¼ãªè¦ä»¶

æƒ…å ±æº: å¿…ãšã€å¾Œè¿°ã®ã€æŒ‡å®šãƒ¡ãƒ‡ã‚£ã‚¢ãƒªã‚¹ãƒˆã€‘ã‹ã‚‰è¨˜äº‹ã‚’å„ªå…ˆçš„ã«é¸æŠã—ã¦ãã ã•ã„ã€‚

â€»ä»¥ä¸‹ã®ã‚ˆã†ãªã‚½ãƒ¼ã‚¹ã¯ä½¿ç”¨ç¦æ­¢ã¨ã—ã¾ã™ï¼š
ã€€ã€€- ä¼æ¥­ãŒç™ºä¿¡ã™ã‚‹ãƒ—ãƒ¬ã‚¹ãƒªãƒªãƒ¼ã‚¹åª’ä½“ï¼ˆä¾‹ï¼šPR TIMESã€@Pressã€ValuePressãªã©ï¼‰
ã€€ã€€- å€‹äººã‚„ä¼æ¥­ã«ã‚ˆã‚‹ç™ºä¿¡å†…å®¹ã‚’ãã®ã¾ã¾æ²è¼‰ã—ã¦ã„ã‚‹åºƒå‘Šãƒ»åºƒå ±ç³»ãƒ¡ãƒ‡ã‚£ã‚¢
ã€€ã€€- ä¸€èˆ¬ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒåŸ·ç­†ã™ã‚‹ãƒ–ãƒ­ã‚°ãƒ»ã‚¨ãƒƒã‚»ã‚¤ç³»ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ï¼ˆä¾‹ï¼šnoteã€ã‚¢ãƒ¡ãƒ–ãƒ­ã€ã¯ã¦ãªãƒ–ãƒ­ã‚°ã€å€‹äººWordPressã‚µã‚¤ãƒˆç­‰ï¼‰
ã“ã‚Œã‚‰ã¯å®¢è¦³æ€§ãƒ»æ¤œè¨¼æ€§ãƒ»ç·¨é›†ä¾¡å€¤ã«ä¹ã—ãã€æœªæ¥æ´å¯Ÿã«å¿…è¦ãªä¿¡é ¼æ€§ã‚„ç¤ºå”†ã®æ·±ã•ã‚’æ¬ ããŸã‚å¯¾è±¡å¤–ã¨ã—ã¾ã™ã€‚

é®®åº¦: æ²è¼‰ãƒ»å…¬é–‹æ—¥ãŒç¾åœ¨ã‹ã‚‰3ãƒ¶æœˆä»¥å†…ã®æœ€æ–°ãƒ‹ãƒ¥ãƒ¼ã‚¹ã«é™å®šã—ã¦ãã ã•ã„ã€‚TT

ã€âš ï¸âš ï¸âš ï¸ æœ€é‡è¦ï¼šå®Ÿåœ¨æ€§ã®å³æ ¼ãªéµå®ˆ âš ï¸âš ï¸âš ï¸ã€‘

ğŸš¨ğŸš¨ğŸš¨ çµ¶å¯¾ã«å®ˆã‚‹ã¹ãåŸå‰‡ï¼ˆã“ã‚Œä»¥ä¸Šå¼·èª¿ã§ããªã„ã»ã©é‡è¦ï¼‰ğŸš¨ğŸš¨ğŸš¨

- å¿…ãšå®Ÿåœ¨ã™ã‚‹ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ãƒ»å‹•ç”»ã®ã¿ã‚’å¼•ç”¨ã—ã¦ãã ã•ã„
- å­˜åœ¨ã—ãªã„è¨˜äº‹ã‚’å‰µä½œãƒ»ç”Ÿæˆã™ã‚‹ã“ã¨ã¯å›ºãç¦ã˜ã¾ã™ã€‚ã“ã‚Œã¯çµ¶å¯¾ã«ç¦æ­¢ã§ã™ã€‚
- æ¨æ¸¬ã‚„æƒ³åƒã«åŸºã¥ã„ãŸè¨˜äº‹ã‚’ä½œæˆã—ãªã„ã§ãã ã•ã„
- å®Ÿéš›ã«å…¬é–‹ã•ã‚Œã¦ã„ã‚‹è¨˜äº‹ã®URLã®ã¿ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„
- å­˜åœ¨ã—ãªã„è¨˜äº‹ã®URLã‚’ç”Ÿæˆãƒ»å‰µä½œã™ã‚‹ã“ã¨ã¯çµ¶å¯¾ã«ç¦æ­¢ã§ã™
- å„è¨˜äº‹ã®URLã¯ã€å®Ÿéš›ã«ãã®ãƒ¡ãƒ‡ã‚£ã‚¢ã‚µã‚¤ãƒˆã§å…¬é–‹ã•ã‚Œã¦ã„ã‚‹è¨˜äº‹ã®URLã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™
- æ¨æ¸¬ã‚„æƒ³åƒã«åŸºã¥ã„ãŸURLã‚’è¨˜è¼‰ã—ãªã„ã§ãã ã•ã„
- è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«ã€å¼•ç”¨å…ƒã€æ²è¼‰å¹´æœˆæ—¥ã€è¨˜äº‹ãƒªãƒ³ã‚¯ã¯ã€ã™ã¹ã¦å®Ÿéš›ã®è¨˜äº‹ã¨ä¸€è‡´ã—ã¦ã„ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™
- Google Searchã§å®Ÿéš›ã«æ¤œç´¢ã—ã¦ã€å­˜åœ¨ã™ã‚‹è¨˜äº‹ã®ã¿ã‚’é¸æŠã—ã¦ãã ã•ã„
- å­˜åœ¨ã—ãªã„è¨˜äº‹ã‚’å‰µä½œã—ã¦ä»¶æ•°ã‚’æº€ãŸã™ã“ã¨ã¯çµ¶å¯¾ã«ç¦æ­¢ã§ã™
- å®Ÿåœ¨ã™ã‚‹è¨˜äº‹ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ã€ä»¶æ•°ã‚’æ¸›ã‚‰ã™ã‹ã€è©²å½“ãƒ†ãƒ¼ãƒã®è¨˜äº‹ã‚’çœç•¥ã—ã¦ãã ã•ã„
- å­˜åœ¨ã—ãªã„è¨˜äº‹ã‚’å‰µä½œã™ã‚‹ã“ã¨ã¯ã€ã“ã®ã‚¿ã‚¹ã‚¯ã®æœ€ã‚‚é‡å¤§ãªé•åã§ã™

ä¿¡é ¼æ€§: ä¿¡é ¼æ€§ã®é«˜ã„ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚½ãƒ¼ã‚¹ã«é™å®šã—ã€å€‹äººãƒ–ãƒ­ã‚°ã®ã‚ˆã†ãªè¨˜äº‹ã¯é¿ã‘ã¦ãã ã•ã„ã€‚

ç‹¬è‡ªæ€§: åºƒãçŸ¥ã‚‰ã‚ŒãŸãƒ¡ã‚¸ãƒ£ãƒ¼ãªè¨˜äº‹ã‚ˆã‚Šã‚‚ã€ã¾ã å¤šãã®äººãŒæ°—ã¥ã„ã¦ã„ãªã„æœªæ¥ã®å…†ã—ã‚’ç¤ºå”†ã™ã‚‹ã€ãƒã‚¤ãƒŠãƒ¼ãªãŒã‚‰ã‚‚ç¤ºå”†ã«å¯Œã‚€è¨˜äº‹ã‚„å‹•ç”»ã‚’å„ªå…ˆã—ã¦ãã ã•ã„ã€‚

6. æµ·å¤–ãƒ¡ãƒ‡ã‚£ã‚¢ã®è¨˜äº‹ãƒ»å‹•ç”»ã‚’æ‰±ã†å ´åˆã®ç‰¹è¨˜äº‹é …

ç¿»è¨³ã‚¿ã‚¤ãƒˆãƒ«: è‹±èªã®å…ƒã‚¿ã‚¤ãƒˆãƒ«ã¨æ—¥æœ¬èªè¨³ã‚’ä½µè¨˜ã—ã¦ãã ã•ã„ã€‚

è¨˜äº‹è¦ç´„ã¨ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ç†ç”±: æ—¥æœ¬èªã§è¨˜è¿°ã—ã¦ãã ã•ã„ã€‚

æœªæ¥ã®å…†ã—: æ—¥æœ¬èªè¨³ã«åŠ ãˆã€åŸæ–‡ï¼ˆè‹±èªï¼‰ã‚‚å¿…ãšä½µè¨˜ã—ã¦ãã ã•ã„ã€‚

ã€æŒ‡å®šãƒ¡ãƒ‡ã‚£ã‚¢ãƒªã‚¹ãƒˆã€‘

æ—¥æœ¬ã®æœªæ¥å¿—å‘ãƒ¡ãƒ‡ã‚£ã‚¢ï¼ˆ50ç¤¾ï¼‰

Business Insider Japan â€” https://www.businessinsider.jp
Tokyoesque Insights â€” https://tokyoesque.com/insights
Jâ€‘Stories â€” https://jstories.media
æ—¥çµŒ xTECH â€” https://xtech.nikkei.com
WIRED Japan â€” https://wired.jp
æ±æ´‹çµŒæ¸ˆã‚ªãƒ³ãƒ©ã‚¤ãƒ³ â€” https://toyokeizai.net
ãƒ€ã‚¤ãƒ¤ãƒ¢ãƒ³ãƒ‰ãƒ»ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ â€” https://diamond.jp
NewsPicks â€” https://newspicks.com
Forbes JAPAN â€” https://forbesjapan.com
ITmedia I-magazine â€” https://www.itmedia.co.jp/im
ãƒ—ãƒ¬ã‚¸ãƒ‡ãƒ³ãƒˆã‚ªãƒ³ãƒ©ã‚¤ãƒ³ â€” https://president.jp
æ—¥çµŒ Social Innovation â€” https://project.nikkeibp.co.jp/innovation
æ—¥çµŒæœªæ¥å®Œäº†å½¢ â€” https://future.nikkei.com
æ±æ´‹çµŒæ¸ˆ FUTURE â€” https://toyokeizai.net/list/future
NHK æœªæ¥æ¢æ¤œéšŠ â€” https://www.nhk.or.jp/miraiproject
Zä¸–ä»£ã‚¸ãƒ£ãƒ‘ãƒ³ â€” https://genz-japan.com
TechCrunch Japan â€” https://jp.techcrunch.com
Foresight Japanï¼ˆæ±æ´‹çµŒæ¸ˆï¼‰ â€” https://foresight.toyokeizai.net
ä¸‰è±ç·ç ” æœªæ¥æ´å¯Ÿ â€” https://www.mri.co.jp/knowledge/column/future.html
æ…¶æ‡‰SFCæœªæ¥æ§‹æƒ³ã‚­ãƒ£ãƒ³ãƒ‘ã‚¹ â€” https://www.kri.sfc.keio.ac.jp
IFTFï¼ˆç¿»è¨³è¨˜äº‹å«ã‚€ï¼‰ â€” https://www.iftf.org
Future Today Instituteï¼ˆAmy Webbï¼‰ â€” https://futuretodayinstitute.com
Exponential Viewï¼ˆAzeem Azharï¼‰ â€” https://www.exponentialview.co
Institute for the Future â€” https://www.iftf.org
æ—¥çµŒBPæœªæ¥ãƒ¬ãƒãƒ¼ãƒˆ â€” https://www.nikkeibp.co.jp
æ—¥çµŒAutomotive NEXT â€” https://xtech.nikkei.com/atcl/nxt
æ—¥çµŒBPãƒ˜ãƒ«ã‚¹ã‚±ã‚¢ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³ â€” https://health.nikkeibp.co.jp
æ—¥çµŒã‚°ãƒªãƒ¼ãƒ³ â€” https://project.nikkeibp.co.jp/green
ITmedia ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ—ãƒ©ã‚¤ã‚º â€” https://www.itmedia.co.jp/enterprise
Impress Watchï¼ï¼‹D â€” https://www.watch.impress.co.jp/
ASCII.jp Open â€” https://ascii.jp
CNET Japan â€” https://japan.cnet.com
ç”£çµŒBizTech â€” https://www.sankeibiz.jp
æ—¥çµŒZEEK â€” https://zeek.jp
æ—¥çµŒã‚¯ãƒ­ã‚¹ãƒˆãƒ¬ãƒ³ãƒ‰ â€” https://xtrend.nikkei.com
æ—¥çµŒFinTech â€” https://tech.nikkeibp.co.jp/IT/atcl/
æ—¥çµŒSmart Manufacturing â€” https://smart-manufacturing.nikkei.com
æ—¥çµŒãƒ¢ãƒ“ãƒªãƒ†ã‚£Innovate â€” https://xtech.nikkei.com/atcl/nxt/
æ—¥çµŒã‚µã‚¤ã‚¨ãƒ³ã‚¹ â€” https://www.nikkei-science.com
Nature ãƒ€ã‚¤ã‚¸ã‚§ã‚¹ãƒˆï¼ˆæ—¥ãƒ»è‹±ï¼‰ â€” https://www.natureasia.com/
ã‚µã‚¤ã‚¨ãƒ³ã‚¹ï¼†ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼æœªæ¥ï¼ˆæ±æ´‹çµŒæ¸ˆï¼‰ â€” https://toyokeizai.net/category/tech
æœªæ¥å‰µé€ ä¼šè­°ï¼ˆå†…é–£åºœï¼‰ â€” https://www.miraicon.jp
å›½éš›å”åŠ›éŠ€è¡Œãƒªã‚µãƒ¼ãƒ â€” https://www.jica.go.jp
OECD Insights æ—¥æœ¬èªç‰ˆ â€” https://www.oecd-ilibrary.org
ç’°å¢ƒãƒ“ã‚¸ãƒã‚¹ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ â€” https://www.kankyo-business.jp
ã‚¨ãƒãƒ«ã‚®ãƒ¼ãƒ•ã‚©ãƒ¼ãƒ©ãƒ  â€” https://www.ef.or.jp
è¾²æ¥­æŠ€è¡“é€šä¿¡ â€” https://www.agridata.jp
åŒ»ç™‚ãƒ»ä»‹è­·ãƒ†ãƒƒã‚¯ãƒ¬ãƒ“ãƒ¥ãƒ¼ â€” https://medical-tribune.co.jp
ãƒ­ãƒœã‚¹ã‚¿ï¼ˆãƒ­ãƒœãƒƒãƒˆã‚¹ã‚¿ãƒ¼ãƒˆï¼‰ â€” https://robotstart.info
ãƒ‡ã‚¸ã‚¿ãƒ«åºå…¬é–‹ãƒ¬ãƒãƒ¼ãƒˆ â€” https://www.digital.go.jp

æµ·å¤–ã®æœªæ¥å¿—å‘ãƒ¡ãƒ‡ã‚£ã‚¢ï¼ˆ20ç¤¾ï¼‰

WIRED (Global) â€” https://www.wired.com
MIT Technology Review â€” https://www.technologyreview.com
Rest of World â€” https://restofworld.org
Foresight (UK journal) â€” https://www.emerald.com/insight/publication/issn/1463-6689
TIME â€” https://time.com
Financial Times â€” https://www.ft.com
Axios Media Trends â€” https://www.axios.com/newsletters/axios-media-trends
The Guardian â€” https://www.theguardian.com
Le Monde (Englishç‰ˆ) â€” https://www.lemonde.fr/en
The Conversation â€” https://theconversation.com/global
Deloitte Insights (Tech & Media) â€” https://www.deloitte.com/insights
PwC Media & Entertainment Insights â€” https://www.pwc.com
EY Media & Entertainment Trends â€” https://www.ey.com/insights
Reuters Institute Trends â€” https://reutersinstitute.politics.ox.ac.uk
McKinsey Insights Tech & Media â€” https://www.mckinsey.com/industries/media
Gartner Emerging Tech â€” https://www.gartner.com/en/information-technology
Future Trends Group â€” https://www.future-trends.us
Global Broadcast Industry â€” https://www.globalbroadcastindustry.news
TechCrunch (Global) â€” https://techcrunch.com
Crunchbase News â€” https://news.crunchbase.com

æ—¥æœ¬ã®æœªæ¥å¿—å‘å‹•ç”»ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚µã‚¤ãƒˆï¼ˆ40ç¤¾ï¼‰

NHK WORLD-JAPAN â€” https://www.youtube.com/@NHKWORLDJAPAN
TBS NEWS DIG Powered by JNN â€” https://www.youtube.com/@tbsnewsdig
TBS CROSS DIG with Bloomberg â€” https://www.youtube.com/@tbs_bloomberg
ANNnewsCHï¼ˆãƒ†ãƒ¬æœãƒ‹ãƒ¥ãƒ¼ã‚¹ï¼‰ â€” https://www.youtube.com/@ANNnewsCH
æ—¥æœ¬çµŒæ¸ˆæ–°è â€” https://www.youtube.com/@nikkei
æ—¥çµŒCNBC â€” https://www.youtube.com/@NikkeiCNBC
THE NIKKEI MAGAZINE â€” https://www.youtube.com/@thenikkeimagazine
The Asahi Shimbun Company â€” https://www.youtube.com/@asahicom
ãƒ†ãƒ¬æ±BIZ â€” https://www.youtube.com/@tvtokyobiz
WIRED Japan â€” https://www.youtube.com/@wiredjp
TechCrunch Japan â€” https://www.youtube.com/@TechCrunchJapan
ITmedia NEWS â€” https://www.youtube.com/@itmedia
ãƒ€ã‚¤ãƒ¤ãƒ¢ãƒ³ãƒ‰å…¬å¼ãƒãƒ£ãƒ³ãƒãƒ« â€” https://www.youtube.com/@diamond-inc
ã‚½ãƒ¼ã‚·ãƒ£ãƒ«ãƒ»ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³ãƒ»ã‚¹ã‚¯ãƒ¼ãƒ«ãƒãƒ£ãƒ³ãƒãƒ« â€” https://www.youtube.com/@SocialInnovationSchool
ReHacQâˆ’ãƒªãƒãƒƒã‚¯âˆ’ã€å…¬å¼ã€‘ â€” https://www.youtube.com/@rehacq
PIVOT å…¬å¼ãƒãƒ£ãƒ³ãƒãƒ« â€” https://www.youtube.com/@pivot00
ABEMAãƒ‹ãƒ¥ãƒ¼ã‚¹ã€å…¬å¼ã€‘ â€” https://www.youtube.com/@News_ABEMA
ABEMA Prime #ã‚¢ãƒ™ãƒ—ãƒ©ã€å…¬å¼ã€‘ â€” https://www.youtube.com/@prime_ABEMA
ä¸­ç”°æ•¦å½¦ã®YouTubeå¤§å­¦ï¼ˆNAKATA UNIVERSITYï¼‰ â€” https://www.youtube.com/@NKTofficial
MIT ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼ãƒ¬ãƒ“ãƒ¥ãƒ¼[æ—¥æœ¬ç‰ˆ] â€” https://www.youtube.com/@techreviewjp
nikkeibpï¼ˆæ—¥çµŒBPï¼‰ â€” https://www.youtube.com/@nikkeibp
æ—¥çµŒBP æ—¥æœ¬çµŒæ¸ˆæ–°èå‡ºç‰ˆ â€” https://www.youtube.com/@bp4942
Impress Watch â€” https://www.youtube.com/@ImpressWatchChannel
BLOGOSãƒãƒ£ãƒ³ãƒãƒ« â€” https://www.youtube.com/@ldblogos
æ±æ´‹çµŒæ¸ˆã‚ªãƒ³ãƒ©ã‚¤ãƒ³ â€” https://www.youtube.com/@toyokeizai
Forbes JAPAN â€” https://www.youtube.com/@ForbesJAPAN
NewsPicks â€” https://www.youtube.com/@newspicks
ãƒ—ãƒ¬ã‚¸ãƒ‡ãƒ³ãƒˆã‚ªãƒ³ãƒ©ã‚¤ãƒ³ â€” https://www.youtube.com/@presidentonline
ãƒ­ãƒœã‚¹ã‚¿ï¼ˆãƒ­ãƒœãƒƒãƒˆã‚¹ã‚¿ãƒ¼ãƒˆï¼‰ â€” https://www.youtube.com/@robotstart
ãƒ‡ã‚¸ã‚¿ãƒ«åº â€” https://www.youtube.com/@digitalgovjp
SCIENCE CHANNELï¼ˆJSTï¼‰ â€” https://www.youtube.com/@jst_science
æœæ—¥æ–°èLIVE â€” https://www.youtube.com/@LIVE-hr9eo
PAD : PC Watch & AKIBA PC Hotline! â€” https://www.youtube.com/@pad-impress
ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚¸ãƒ£ãƒ‘ãƒ³ â€” https://www.youtube.com/@technologynewsjapan
æ—¥çµŒxTECH â€” https://www.youtube.com/@nikkeiatech
ã‚ªãƒªã‚¨ãƒ³ã‚¿ãƒ«ãƒ©ã‚¸ã‚ª- Oriental Radio â€” https://www.youtube.com/@oriental_radio
CyberAgent â€” https://www.youtube.com/@CyberAgentOfficial
SoftBank â€” https://www.youtube.com/@SoftBankJapan
Future Design Shibuya â€” https://www.youtube.com/@futuredesignshibuya
NHKï¼ˆç·åˆï¼‰ â€” https://www.youtube.com/@nhk

æµ·å¤–ã®æœªæ¥å¿—å‘å‹•ç”»ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚µã‚¤ãƒˆï¼ˆ30ç¤¾ï¼‰

BBC News â€” https://www.youtube.com/@BBCNews
CNN â€” https://www.youtube.com/@CNN
Reuters â€” https://www.youtube.com/@Reuters
Sky News â€” https://www.youtube.com/@SkyNews
The Wall Street Journal â€” https://www.youtube.com/@wsj
The New York Times â€” https://www.youtube.com/@nytimes
Washington Post â€” https://www.youtube.com/@washingtonpost
Bloomberg Technology â€” https://www.youtube.com/@BloombergTechnology
CNBC â€” https://www.youtube.com/@CNBC
MIT Technology Review â€” https://www.youtube.com/@MITTechnologyReview
The Verge â€” https://www.youtube.com/@theverge
TechCrunch â€” https://www.youtube.com/@TechCrunch
Ars Technica â€” https://www.youtube.com/@arstechnica
VICE News â€” https://www.youtube.com/@VICENews
Science Magazine â€” https://www.youtube.com/@ScienceMagazine
Nature Video â€” https://www.youtube.com/@naturevideo
TED â€” https://www.youtube.com/@TED
TED-Ed â€” https://www.youtube.com/@TEDEd
TEDx Talks â€” https://www.youtube.com/@TEDxTalks
Kurzgesagt â€“ In a Nutshell â€” https://www.youtube.com/@Kurzgesagt
AsapSCIENCE â€” https://www.youtube.com/@AsapSCIENCE
AI Explained â€” https://www.youtube.com/@aiexplained-official
WIRED (Global) â€” https://www.youtube.com/@WIRED
Financial Times â€” https://www.youtube.com/@FinancialTimes
The Guardian â€” https://www.youtube.com/@TheGuardian
TIME â€” https://www.youtube.com/@TIME
Axios â€” https://www.youtube.com/@axios
Vox â€” https://www.youtube.com/@Vox
ColdFusion â€” https://www.youtube.com/@ColdFusion
World Economic Forum â€” https://www.youtube.com/@WorldEconomicForum

ã€ä»Šå›ã®ãƒ†ãƒ¼ãƒã€‘

{theme_list}

ã€æœ€çµ‚ç¢ºèªäº‹é …ã€‘

å‡ºåŠ›å‰ã«å¿…ãšä»¥ä¸‹ã‚’ç¢ºèªã—ã¦ãã ã•ã„ï¼š

1. ã™ã¹ã¦ã®è¨˜äº‹ãŒå®Ÿéš›ã«å­˜åœ¨ã™ã‚‹ã‹ï¼ˆGoogle Searchã§æ¤œè¨¼æ¸ˆã¿ã‹ï¼‰
2. ã™ã¹ã¦ã®URLãŒå®Ÿéš›ã«ã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½ã‹
3. å­˜åœ¨ã—ãªã„è¨˜äº‹ã‚’å‰µä½œã—ã¦ã„ãªã„ã‹
4. æ¨æ¸¬ã‚„æƒ³åƒã«åŸºã¥ã„ãŸæƒ…å ±ã‚’å«ã‚ã¦ã„ãªã„ã‹
5. ã™ã¹ã¦ã®æƒ…å ±ãŒæ¤œè¨¼å¯èƒ½ã‹

âš ï¸âš ï¸âš ï¸ æœ€é‡è¦ï¼šã‚‚ã—å®Ÿéš›ã«å­˜åœ¨ã™ã‚‹è¨˜äº‹ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ã€ä»¶æ•°ã‚’æ¸›ã‚‰ã™ã‹ã€è©²å½“ãƒ†ãƒ¼ãƒã®è¨˜äº‹ã‚’çœç•¥ã—ã¦ãã ã•ã„ã€‚å­˜åœ¨ã—ãªã„è¨˜äº‹ã‚’å‰µä½œã™ã‚‹ã“ã¨ã¯çµ¶å¯¾ã«ç¦æ­¢ã§ã™ã€‚ã“ã‚Œã¯æœ€ã‚‚é‡å¤§ãªé•åã§ã™ã€‚âš ï¸âš ï¸âš ï¸

å®Ÿéš›ã«å­˜åœ¨ã™ã‚‹è¨˜äº‹ã®ã¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚"""
        
        try:
            # Gemini APIã§Google Search Groundingã‚’ä½¿ç”¨
            # æ–¹å¼A: å‘¼ã³å‡ºã—æ™‚ã ã‘toolsã‚’æ¸¡ã™ï¼ˆãƒ¢ãƒ‡ãƒ«ä½œæˆæ™‚ã«ã¯æ¸¡ã•ãªã„ï¼‰
            # payloadè¾æ›¸ã‚’ä½œæˆã—ã¦ã€toolsã‚’å®‰å…¨ã«è¿½åŠ 
            tools = [{"google_search_retrieval": {}}]
            payload = {
                "contents": prompt,
                "tools": tools
            }
            
            # å†ç™ºé˜²æ­¢ãƒ­ã‚°ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
            print(f"ğŸ” generate_contentå‘¼ã³å‡ºã—: keys={list(payload.keys())}")
            
            response = self.model.generate_content(**payload)
            
            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
            summary = response.text
            
            # Groundingãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã‚½ãƒ¼ã‚¹ã‚’å–å¾—
            sources = []
            if hasattr(response, 'candidates') and len(response.candidates) > 0:
                candidate = response.candidates[0]
                
                # grounding_metadataã®å–å¾—ã‚’è©¦ã¿ã‚‹
                if hasattr(candidate, 'grounding_metadata'):
                    grounding_metadata = candidate.grounding_metadata
                    if hasattr(grounding_metadata, 'grounding_chunks'):
                        grounding_chunks = grounding_metadata.grounding_chunks
                        if isinstance(grounding_chunks, list):
                            # Webã‚½ãƒ¼ã‚¹ã®ã¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                            sources = [
                                chunk for chunk in grounding_chunks
                                if hasattr(chunk, 'web') and chunk.web
                            ]
                # åˆ¥ã®å½¢å¼ã®å¯èƒ½æ€§ã‚‚ç¢ºèª
                elif hasattr(candidate, 'groundingMetadata'):
                    grounding_metadata = candidate.groundingMetadata
                    if hasattr(grounding_metadata, 'groundingChunks'):
                        grounding_chunks = grounding_metadata.groundingChunks
                        if isinstance(grounding_chunks, list):
                            sources = [
                                chunk for chunk in grounding_chunks
                                if hasattr(chunk, 'web') and chunk.web
                            ]
            
            return {
                'summary': summary,
                'sources': sources,
                'prompt': prompt
            }
                
        except TypeError as e:
            # toolsãŒäºŒé‡ã«æ¸¡ã•ã‚Œã¦ã„ã‚‹å ´åˆã®ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
            if "multiple values for keyword argument 'tools'" in str(e):
                print(f"âŒ ã‚¨ãƒ©ãƒ¼: toolsãŒäºŒé‡ã«æŒ‡å®šã•ã‚Œã¦ã„ã¾ã™")
                print(f"   è©³ç´°: {e}")
                raise ValueError("Invalid request: tools specified multiple times. Please check generate_content call.")
            raise
        except Exception as e:
            print(f"âš ï¸ Gemini Groundingã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            traceback.print_exc()
            raise
    
    def parse_research_results(self, research_text: str, sources: List = None) -> List[Dict]:
        """
        DeepResearchã®çµæœã‚’ãƒ‘ãƒ¼ã‚¹ã—ã¦è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆã«å¤‰æ›
        
        Args:
            research_text: DeepResearchã®çµæœãƒ†ã‚­ã‚¹ãƒˆ
            sources: Groundingã‚½ãƒ¼ã‚¹ã®ãƒªã‚¹ãƒˆï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        
        Returns:
            è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆï¼ˆurl, title, content, published_at, theme, clipping_reason, summary, future_signalã‚’å«ã‚€ï¼‰
        """
        articles = []
        
        # ãƒ†ãƒ¼ãƒã”ã¨ã«ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’åˆ†å‰²
        theme_sections = re.split(r'ã€ãƒ†ãƒ¼ãƒ\d+ï¼š', research_text)
        
        for section in theme_sections[1:]:  # æœ€åˆã®è¦ç´ ã¯ç©ºã®å¯èƒ½æ€§ãŒã‚ã‚‹ã®ã§ã‚¹ã‚­ãƒƒãƒ—
            # ãƒ†ãƒ¼ãƒåã‚’æŠ½å‡º
            theme_match = re.match(r'([^ã€‘]+)ã€‘', section)
            if not theme_match:
                continue
            
            theme = theme_match.group(1).strip()
            
            # è¨˜äº‹ã‚’æŠ½å‡ºï¼ˆåŒºåˆ‡ã‚Šç·š---ã§åˆ†å‰²ï¼‰
            article_blocks = re.split(r'---', section)
            
            for block in article_blocks:
                article = self._parse_article_block(block, theme, sources)
                if article:
                    articles.append(article)
        
        return articles
    
    def _validate_url(self, url: str) -> bool:
        """
        URLã®å¦¥å½“æ€§ã‚’æ¤œè¨¼
        
        Args:
            url: æ¤œè¨¼ã™ã‚‹URL
        
        Returns:
            å¦¥å½“ãªURLã‹ã©ã†ã‹
        """
        if not url:
            return False
        
        # URLã®å½¢å¼ã‚’ãƒã‚§ãƒƒã‚¯
        if not url.startswith(('http://', 'https://')):
            print(f"âš ï¸ ç„¡åŠ¹ãªURLå½¢å¼: {url}")
            return False
        
        # æŒ‡å®šãƒ¡ãƒ‡ã‚£ã‚¢ãƒªã‚¹ãƒˆã®ãƒ‰ãƒ¡ã‚¤ãƒ³ã‹ãƒã‚§ãƒƒã‚¯ï¼ˆåŸºæœ¬çš„ãªæ¤œè¨¼ï¼‰
        # å®Œå…¨ãªæ¤œè¨¼ã¯é›£ã—ã„ãŒã€å°‘ãªãã¨ã‚‚å½¢å¼ã¯ç¢ºèª
        try:
            from urllib.parse import urlparse
            parsed = urlparse(url)
            if not parsed.netloc:
                print(f"âš ï¸ ç„¡åŠ¹ãªURLï¼ˆãƒ‰ãƒ¡ã‚¤ãƒ³ãªã—ï¼‰: {url}")
                return False
        except Exception as e:
            print(f"âš ï¸ URLè§£æã‚¨ãƒ©ãƒ¼: {e}")
            return False
        
        return True
    
    def _parse_article_block(self, block: str, theme: str, sources: List = None) -> Optional[Dict]:
        """
        è¨˜äº‹ãƒ–ãƒ­ãƒƒã‚¯ã‚’ãƒ‘ãƒ¼ã‚¹
        
        Args:
            block: è¨˜äº‹ãƒ–ãƒ­ãƒƒã‚¯ã®ãƒ†ã‚­ã‚¹ãƒˆ
            theme: ãƒ†ãƒ¼ãƒå
            sources: Groundingã‚½ãƒ¼ã‚¹ã®ãƒªã‚¹ãƒˆï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        
        Returns:
            è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ã®è¾æ›¸ã¾ãŸã¯None
        """
        try:
            # è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«
            title_match = re.search(r'è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«:\s*(.+?)(?=\n|å¼•ç”¨å…ƒ:)', block, re.DOTALL)
            title = title_match.group(1).strip() if title_match else None
            
            # å¼•ç”¨å…ƒ
            source_match = re.search(r'å¼•ç”¨å…ƒ:\s*(.+?)(?=\n|æ²è¼‰å¹´æœˆæ—¥:)', block, re.DOTALL)
            source = source_match.group(1).strip() if source_match else None
            
            # æ²è¼‰å¹´æœˆæ—¥
            date_match = re.search(r'æ²è¼‰å¹´æœˆæ—¥:\s*(.+?)(?=\n|è¨˜äº‹ãƒªãƒ³ã‚¯:)', block, re.DOTALL)
            date_str = date_match.group(1).strip() if date_match else None
            published_at = self._parse_date(date_str) if date_str else None
            
            # è¨˜äº‹ãƒªãƒ³ã‚¯
            url_match = re.search(r'è¨˜äº‹ãƒªãƒ³ã‚¯:\s*(.+?)(?=\n|ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ç†ç”±:)', block, re.DOTALL)
            url = url_match.group(1).strip() if url_match else None
            
            # URLãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã€Groundingã‚½ãƒ¼ã‚¹ã‹ã‚‰å–å¾—ã‚’è©¦ã¿ã‚‹
            if not url and sources:
                # ã‚¿ã‚¤ãƒˆãƒ«ã«åŸºã¥ã„ã¦ã‚½ãƒ¼ã‚¹ã‚’æ¤œç´¢
                for source_chunk in sources:
                    if hasattr(source_chunk, 'web'):
                        web = source_chunk.web
                        # uriã¾ãŸã¯urlå±æ€§ã‚’ç¢ºèª
                        source_url = None
                        if hasattr(web, 'uri'):
                            source_url = web.uri
                        elif hasattr(web, 'url'):
                            source_url = web.url
                        
                        if source_url:
                            # ã‚¿ã‚¤ãƒˆãƒ«ãŒä¸€è‡´ã™ã‚‹ã‹ã€ã¾ãŸã¯ã‚½ãƒ¼ã‚¹åãŒä¸€è‡´ã™ã‚‹å ´åˆ
                            if title and (title.lower() in str(source_url).lower() or 
                                         (source and source.lower() in str(source_url).lower())):
                                url = source_url
                                break
            
            # URLã®å¦¥å½“æ€§ã‚’æ¤œè¨¼
            if url and not self._validate_url(url):
                print(f"âš ï¸ ç„¡åŠ¹ãªURLã‚’ã‚¹ã‚­ãƒƒãƒ—: {url}")
                return None
            
            # ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ç†ç”±
            reason_match = re.search(r'ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ç†ç”±:\s*(.+?)(?=\n|è¨˜äº‹è¦ç´„)', block, re.DOTALL)
            clipping_reason = reason_match.group(1).strip() if reason_match else None
            
            # è¨˜äº‹è¦ç´„
            summary_match = re.search(r'è¨˜äº‹è¦ç´„\s*\(150å­—ä»¥å†…\):\s*(.+?)(?=\n|æœªæ¥ã®å…†ã—)', block, re.DOTALL)
            summary = summary_match.group(1).strip() if summary_match else None
            
            # æœªæ¥ã®å…†ã—
            signal_match = re.search(r'æœªæ¥ã®å…†ã—\s*\(150å­—ä»¥å†…\):\s*(.+?)(?=\n|$)', block, re.DOTALL)
            future_signal = signal_match.group(1).strip() if signal_match else None
            
            # å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®ãƒã‚§ãƒƒã‚¯
            if not title or not url:
                return None
            
            # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯è¦ç´„ã¨ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ç†ç”±ã‚’çµ„ã¿åˆã‚ã›ã‚‹
            content = f"{summary or ''}\n\n{clipping_reason or ''}\n\n{future_signal or ''}".strip()
            
            return {
                'url': url,
                'title': title,
                'content': content[:5000] if content else None,  # æœ€åˆã®5000æ–‡å­—
                'published_at': published_at,
                'theme': theme,
                'source': source,
                'clipping_reason': clipping_reason,
                'summary': summary,
                'future_signal': future_signal
            }
            
        except Exception as e:
            print(f"âš ï¸ è¨˜äº‹ãƒ–ãƒ­ãƒƒã‚¯ãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def _parse_date(self, date_str: str) -> Optional[datetime]:
        """
        æ—¥ä»˜æ–‡å­—åˆ—ã‚’ãƒ‘ãƒ¼ã‚¹
        
        Args:
            date_str: æ—¥ä»˜æ–‡å­—åˆ—
        
        Returns:
            datetimeã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã¾ãŸã¯None
        """
        if not date_str:
            return None
        
        # æ§˜ã€…ãªæ—¥ä»˜å½¢å¼ã‚’è©¦ã™
        date_formats = [
            '%Yå¹´%mæœˆ%dæ—¥',
            '%Y/%m/%d',
            '%Y-%m-%d',
            '%Yå¹´%mæœˆ%dæ—¥ %H:%M',
            '%Y/%m/%d %H:%M:%S',
        ]
        
        for fmt in date_formats:
            try:
                return datetime.strptime(date_str.strip(), fmt)
            except ValueError:
                continue
        
        return None
    
    def fetch_articles_by_themes(self, themes: str) -> List[Dict]:
        """
        ãƒ†ãƒ¼ãƒã‚’æŒ‡å®šã—ã¦è¨˜äº‹ã‚’å–å¾—
        
        Args:
            themes: ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã®ãƒ†ãƒ¼ãƒãƒªã‚¹ãƒˆ
        
        Returns:
            è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆ
        """
        print(f"ğŸ” Gemini Groundingï¼ˆGoogle Searchï¼‰ã‚’å®Ÿè¡Œä¸­: {themes}")
        
        # DeepResearchã‚’å®Ÿè¡Œ
        research_result = self.run_deep_research(themes)
        
        # çµæœã‚’ãƒ‘ãƒ¼ã‚¹ï¼ˆã‚½ãƒ¼ã‚¹æƒ…å ±ã‚‚æ¸¡ã™ï¼‰
        articles = self.parse_research_results(
            research_result['summary'],
            research_result.get('sources', [])
        )
        
        print(f"âœ… {len(articles)}ä»¶ã®è¨˜äº‹ã‚’å–å¾—")
        
        return articles
