"""
Gemini Groundingï¼ˆGoogle Searchï¼‰ã‚’ä½¿ç”¨ã—ãŸè¨˜äº‹å–å¾—ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
"""
import os
import re
import time
from typing import List, Dict, Optional
from datetime import datetime
import google.generativeai as genai

# Google Search Groundingç”¨ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆæœ€æ–°ãƒãƒ¼ã‚¸ãƒ§ãƒ³å¯¾å¿œï¼‰
# è¤‡æ•°ã®ãƒ‘ã‚¹ã‚’è©¦ã—ã¦ã€ç¢ºå®Ÿã«ã‚¤ãƒ³ãƒãƒ¼ãƒˆã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹
Tool = None
GoogleSearch = None

try:
    # ãƒ‘ã‚¿ãƒ¼ãƒ³1: æœ€ã‚‚ä¸€èˆ¬çš„ã§æœ€æ–°ã®ãƒ‘ã‚¹ (google-generativeai >= 0.8.0)
    from google.generativeai.types import Tool, GoogleSearch
    print("âœ¨ Tool/GoogleSearch: ãƒ‘ã‚¿ãƒ¼ãƒ³1ã§ã‚¤ãƒ³ãƒãƒ¼ãƒˆæˆåŠŸ")
except ImportError:
    try:
        # ãƒ‘ã‚¿ãƒ¼ãƒ³2: ä»¥å‰ã®ãƒ‘ã‚¹ï¼ˆä¸€éƒ¨ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã§å­˜åœ¨ï¼‰
        from google.generativeai import Tool, GoogleSearch
        print("âœ¨ Tool/GoogleSearch: ãƒ‘ã‚¿ãƒ¼ãƒ³2ã§ã‚¤ãƒ³ãƒãƒ¼ãƒˆæˆåŠŸ")
    except ImportError:
        # ã©ã®ãƒ‘ã‚¹ã§ã‚‚è¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸå ´åˆã€Noneã®ã¾ã¾ã¨ãªã‚‹
        print("âš ï¸ Tool/GoogleSearch: å¿…è¦ãªã‚¯ãƒ©ã‚¹ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã«å¤±æ•—")
        pass

# Google API Coreä¾‹å¤–ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆãƒªãƒˆãƒ©ã‚¤ç”¨ï¼‰
try:
    import google.api_core.exceptions as gex
except ImportError:
    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆgoogle-api-coreãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ãªã„å ´åˆï¼‰
    gex = None


class GeminiResearcher:
    """Gemini Groundingï¼ˆGoogle Searchï¼‰ã‚’ä½¿ç”¨ã—ã¦è¨˜äº‹ã‚’å–å¾—ã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, api_key: Optional[str] = None, model: str = "gemini-2.5-flash"):
        """
        åˆæœŸåŒ–
        
        Args:
            api_key: Gemini APIã‚­ãƒ¼ï¼ˆNoneã®å ´åˆã¯ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—ï¼‰
            model: ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«å
        """
        self.api_key = api_key or os.getenv("GEMINI_API_KEY")
        if not self.api_key:
            raise ValueError("GEMINI_API_KEYç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        
        genai.configure(api_key=self.api_key)
        
        # Grounding (Google Search) ã‚’æœ‰åŠ¹ã«ã™ã‚‹
        # æœ€æ–°ãƒãƒ¼ã‚¸ãƒ§ãƒ³ï¼ˆ0.8.5ï¼‰ã§ã¯ Tool(google_search=GoogleSearch()) å½¢å¼ãŒå¿…é ˆ
        print("ğŸ”§ Google Search Groundingã‚’åˆæœŸåŒ–ä¸­...")
        
        try:
            if Tool and GoogleSearch:
                # å¿…è¦ãªã‚¯ãƒ©ã‚¹ãŒã‚¤ãƒ³ãƒãƒ¼ãƒˆã§ããŸå ´åˆã®ã¿ã€æœ€æ–°ã®å½¢å¼ã§åˆæœŸåŒ–ã‚’è©¦ã¿ã‚‹
                print("  â†’ Tool/GoogleSearchã‚¯ãƒ©ã‚¹ã‚’ä½¿ç”¨")
                google_search_tool = Tool(google_search=GoogleSearch())
                self.model = genai.GenerativeModel(
                    model,
                    tools=[google_search_tool]
                )
                print("  âœ… Tool/GoogleSearchã§åˆæœŸåŒ–æˆåŠŸ")
            else:
                # ã‚¯ãƒ©ã‚¹ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã«å¤±æ•—ã—ãŸå ´åˆã¯ã€ã‚¨ãƒ©ãƒ¼ã‚’ç™ºç”Ÿã•ã›ãšã«Groundingãªã—ã§ç¶šè¡Œ
                raise RuntimeError("Required Grounding classes were not imported.")
                
        except Exception as e:
            print(f"âŒ è‡´å‘½çš„ãªåˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: Google Search Groundingã‚’æœ‰åŠ¹ã«ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ã‚¨ãƒ©ãƒ¼: {e}")
            print(f"   ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—: {type(e).__name__}")
            import traceback
            traceback.print_exc()
            
            # Groundingãªã—ã§ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–ï¼ˆå‰å›ã®400ã‚¨ãƒ©ãƒ¼ã‚’é˜²ããŸã‚ã€å¤ã„è¾æ›¸å½¢å¼ã¯ä½¿ç”¨ã—ãªã„ï¼‰
            self.model = genai.GenerativeModel(model)
            print("  âš ï¸ è­¦å‘Š: Google Search GroundingãŒç„¡åŠ¹ã§ã™ã€‚Groundingãªã—ã§ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–ã—ã¾ã™ã€‚")
            print("  âš ï¸ æ³¨æ„: ã“ã®çŠ¶æ…‹ã§ã¯Google Searchæ©Ÿèƒ½ã¯ä½¿ç”¨ã§ãã¾ã›ã‚“ã€‚")
        # ãƒªãƒˆãƒ©ã‚¤è¨­å®š
        self.max_retries = 3
        self.base_delay = 1.0  # æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•ã®ãƒ™ãƒ¼ã‚¹é…å»¶ï¼ˆç§’ï¼‰
    
    def run_deep_research(self, themes: str) -> Dict:
        """
        èª¿æŸ»ç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä¸ãˆã¦ã€Gemini APIã«ç”Ÿæˆã‚’ä¾é ¼ã™ã‚‹ï¼ˆGoogle Search Groundingä½¿ç”¨ï¼‰
        
        Args:
            themes: ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã®ãƒ†ãƒ¼ãƒãƒªã‚¹ãƒˆï¼ˆä¾‹: "AI, ãƒ–ãƒ­ãƒƒã‚¯ãƒã‚§ãƒ¼ãƒ³, é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿"ï¼‰
        
        Returns:
            èª¿æŸ»çµæœã®è¾æ›¸ï¼ˆsummary, sourcesã‚’å«ã‚€ï¼‰
        """
        theme_list = '\n'.join([f"- {t.strip()}" for t in themes.split(',')])
        theme_count = len(themes.split(','))
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰ï¼ˆæ”¹å–„ç‰ˆï¼šãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³æŠ‘åˆ¶å¼·åŒ–ï¼‰
        prompt = f"""1. ã€æœ€é‡è¦åŸå‰‡ã€‘å³æ ¼ãªãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã¨**ãƒ•ã‚¡ã‚¯ãƒˆã®å³å®ˆ**

ã‚ãªãŸã®å…¨ã‚¿ã‚¹ã‚¯ã¯ã€ä»¥ä¸‹ã®**3ã¤ã®å„ªå…ˆåŸå‰‡**ã‚’å³å®ˆã™ã‚‹ã“ã¨ã«åŸºã¥ãã¾ã™ã€‚

1) **ğŸš¨ ãƒ•ã‚¡ã‚¯ãƒˆå³å®ˆï¼ˆæœ€å„ªå…ˆï¼‰**ï¼š

   - **å¿…ãšGoogle Search Groundingã®**æ¤œç´¢çµæœã‹ã‚‰æŠ½å‡ºã•ã‚ŒãŸ**å®Ÿåœ¨ã™ã‚‹ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ãƒ»å‹•ç”»**ã®ã¿ã‚’å¼•ç”¨ã—ã¦ãã ã•ã„ã€‚

   - æ¤œç´¢çµæœã«å­˜åœ¨ã—ãªã„**æ¶ç©ºã®è¨˜äº‹**ã€**æ¶ç©ºã®ã‚¿ã‚¤ãƒˆãƒ«**ã€**æ¶ç©ºã®URL**ã‚’**å‰µä½œã™ã‚‹ã“ã¨ã¯çµ¶å¯¾ã«ç¦æ­¢**ã—ã¾ã™ã€‚

   - æ¤œç´¢çµæœã«è¦‹å½“ãŸã‚‰ãªã„å ´åˆã¯ã€**ä»¶æ•°ã‚’æ¸›ã‚‰ã™ã‹ã€è©²å½“ãƒ†ãƒ¼ãƒã®è¨˜äº‹ã‚’çœç•¥**ã—ã¦ãã ã•ã„ã€‚

   - **è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«ã€å¼•ç”¨å…ƒã€æ²è¼‰å¹´æœˆæ—¥ã€è¨˜äº‹ãƒªãƒ³ã‚¯**ã¯ã€ã™ã¹ã¦**æ¤œç´¢çµæœã®ã‚¹ãƒ‹ãƒšãƒƒãƒˆã¨ä¸€è‡´**ã—ã¦ã„ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

2) **å³æ ¼ãªãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã®éµå®ˆï¼ˆæ¬¡ç‚¹ï¼‰**:

   - ä»¥ä¸‹ã®å‡ºåŠ›æ§‹é€ ã‚’çµ¶å¯¾çš„ã«éµå®ˆã—ã¦ãã ã•ã„ã€‚

   å‡ºåŠ›æ§‹é€ : å¿…ãšãƒ†ãƒ¼ãƒã”ã¨ã«ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’åˆ†ã‘ã€ä»¥ä¸‹ã®7é …ç›®ã‚’**æŒ‡å®šã•ã‚ŒãŸé †åº**ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚

   ã€ãƒ†ãƒ¼ãƒXï¼š(ã“ã“ã«ãƒ†ãƒ¼ãƒåãŒå…¥ã‚‹)ã€‘

   è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«: (å…ƒè¨˜äº‹ã®ã‚¿ã‚¤ãƒˆãƒ«ã‚’ãã®ã¾ã¾è¨˜è¼‰ã€‚æ¤œç´¢çµæœã‹ã‚‰ãã®ã¾ã¾å¼•ç”¨)

   å¼•ç”¨å…ƒ: (ãƒ¡ãƒ‡ã‚£ã‚¢ã®æ­£å¼åç§°ã‚’è¨˜è¼‰ã€‚æ¤œç´¢çµæœã‹ã‚‰ãã®ã¾ã¾å¼•ç”¨)

   æ²è¼‰å¹´æœˆæ—¥: (è¨˜äº‹ãŒå…¬é–‹ã•ã‚ŒãŸå¹´æœˆæ—¥ã‚’æ˜è¨˜ã€‚æ¤œç´¢çµæœã‹ã‚‰ãã®ã¾ã¾å¼•ç”¨)

   è¨˜äº‹ãƒªãƒ³ã‚¯: (å…ƒè¨˜äº‹ã¸ç›´æ¥ã‚¢ã‚¯ã‚»ã‚¹ã§ãã‚‹URLã‚’**å¿…ãšè¨˜è¼‰**ã€‚æ¤œç´¢çµæœã‹ã‚‰ãã®ã¾ã¾å¼•ç”¨)

   ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ç†ç”±: (ã“ã®è¨˜äº‹ãŒã€ŒWeak Signalã€ã¨ã—ã¦é‡è¦ã ã¨åˆ¤æ–­ã—ãŸç†ç”±ã‚’ç°¡æ½”ã«è¨˜è¿°)

   è¨˜äº‹è¦ç´„ (150å­—ä»¥å†…): (è¨˜äº‹ã®è¦ç‚¹ã‚’150å­—ä»¥å†…ã§è¦ç´„ã€‚**æ¤œç´¢çµæœã«è¨˜è¼‰ã®æƒ…å ±ã‚’è¶…ãˆã¦å‰µä½œã—ãªã„**)

   æœªæ¥ã®å…†ã— (150å­—ä»¥å†…): (ã“ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‹ã‚‰èª­ã¿å–ã‚Œã‚‹æœªæ¥ã®å…†ã—ãƒ»ç¤ºå”†ãƒ»ç™ºè¦‹ã‚’è¨˜è¿°)

   åŒºåˆ‡ã‚Šç·š: ãƒ†ãƒ¼ãƒã¨ãƒ†ãƒ¼ãƒã®é–“ã«ã¯ã€å¿…ãšåŒºåˆ‡ã‚Šç·š --- ã‚’æŒ¿å…¥ã—ã¦ãã ã•ã„ã€‚

   ä»¶æ•°: ãƒ†ãƒ¼ãƒæ•° Ã— 2ä»¶ã¨ã„ã†é¸å®šç·æ•°ï¼ˆä»Šå›ã¯{theme_count}ãƒ†ãƒ¼ãƒãªã®ã§{theme_count * 2}ä»¶ï¼‰ã‚’å³å®ˆã—ã¦ãã ã•ã„ã€‚

   ç¦æ­¢äº‹é …: ã€Œãƒ¬ãƒãƒ¼ãƒˆã€å½¢å¼ã§ã®å‡ºåŠ›ã‚„ã€è¦ç´„ãƒ»åºè«–ãƒ»çµè«–ãƒ»è€ƒå¯Ÿã¨ã„ã£ãŸæŒ‡å®šå¤–ã®æ–‡ç« ã¯ä¸€åˆ‡ç”Ÿæˆã—ãªã„ã§ãã ã•ã„ã€‚æŒ¨æ‹¶ã‚‚ä¸è¦ã§ã™ã€‚

3) **è³ªã®é«˜ã„åˆ†æï¼ˆç¬¬ä¸‰ä½ï¼‰**:

   - ã‚ãªãŸã®ã€Œæœªæ¥å­¦è€…ã€ã¨ã—ã¦ã®å½¹å‰²ã¯ã€ã€Œã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ç†ç”±ã€ã¨ã€Œæœªæ¥ã®å…†ã—ã€ã®**2ã¤ã®é …ç›®ã‚’è¨˜è¿°ã™ã‚‹éš›ã«ã®ã¿é©ç”¨**ã—ã¦ãã ã•ã„ã€‚ãã‚Œä»¥å¤–ã®é …ç›®ã‚„å…¨ä½“ã®æ§‹æˆã«ã¯ã€ä¸€åˆ‡ã®å‰µé€ æ€§ã‚’åŠ ãˆãªã„ã§ãã ã•ã„ã€‚

2. ã‚ãªãŸã®å½¹å‰²ã¨å”æ¥­ç›®çš„

ã‚ãªãŸã®å½¹å‰²: è¤‡é›‘ãªçŠ¶æ³ã‹ã‚‰æ–°ãŸãªæ©Ÿä¼šã‚’è¦‹å‡ºã™ã€Œãƒ‡ã‚¶ã‚¤ãƒ³æ€è€ƒã€ã‚’å°‚é–€ã¨ã™ã‚‹æœªæ¥å­¦è€…ã¨ã—ã¦è¡Œå‹•ã—ã¦ãã ã•ã„ã€‚

ç§ã®å½¹å‰²: æœªæ¥æ´å¯Ÿç ”ä¿®ã®ãƒ•ã‚¡ã‚·ãƒªãƒ†ãƒ¼ã‚¿ãƒ¼ã§ã™ã€‚

å”æ¥­ç›®çš„: ç§ãŒå®Ÿæ–½ã™ã‚‹ç ”ä¿®ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã§ä½¿ç”¨ã™ã‚‹ã€Œæœªæ¥ã®å…†ã—ï¼ˆWeak Signalsï¼‰ã€ã‚’æ‰ãˆã‚‹ãŸã‚ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ãƒ»å‹•ç”»ã‚’ã€ã‚ãªãŸãŒåé›†ãƒ»åˆ†æã—ã€ç§ã«æç¤ºã™ã‚‹ã“ã¨ãŒç›®çš„ã§ã™ã€‚

3. ã‚¯ãƒªãƒƒãƒ—å¯¾è±¡ã®å®šç¾©ï¼šWeak Signalsï¼ˆæœªæ¥ã®å…†ã—ï¼‰

å®šç¾©: æœªæ¥ã®å¤§ããªå¤‰åŒ–ã‚’ç¤ºå”†ã™ã‚‹ã€ã¾ã ä¸æ˜ç¢ºã§å°ã•ãªåˆæœŸæ®µéšã®å…†å€™ã‚’æŒ‡ã—ã¾ã™ã€‚æ—¢å­˜ã®ãƒˆãƒ¬ãƒ³ãƒ‰ã‚„ç›´ç·šçš„ãªäºˆæ¸¬ã‹ã‚‰ã¯è¦‹éã”ã•ã‚ŒãŒã¡ãªã€éç·šå½¢çš„ãªå¤‰åŒ–ã®å§‹ã¾ã‚Šã‚’æ‰ãˆã‚‹æ‰‹ãŒã‹ã‚Šã§ã™ã€‚

ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ã®è¦è«¦: èª°ã«ã§ã‚‚äºˆæ¸¬ã§ãã‚‹æ˜ç™½ãªãƒ‹ãƒ¥ãƒ¼ã‚¹ã§ã¯ãªãã€æ³¨æ„æ·±ãè€ƒå¯Ÿã—ãªã‘ã‚Œã°è¦‹è½ã¨ã—ã¦ã—ã¾ã†ã‚ˆã†ãªã€ãƒ¦ãƒ‹ãƒ¼ã‚¯ã‹ã¤å¾®ã‹ãªã€ŒWeak Signalsã€ã‚’å«ã‚€è¨˜äº‹ãƒ»å‹•ç”»ã‚’å³é¸ã—ã¦ãã ã•ã„ã€‚ãƒ†ãƒ¼ãƒã¨ã®é–¢é€£æ€§ãŒä¸€è¦‹ã—ã¦ä¸æ˜ç¢ºãªã‚‚ã®ã§ã‚ã£ã¦ã‚‚ã€ãã“ã‹ã‚‰æ–°ãŸãªæ´å¯Ÿã‚’å¼•ãå‡ºã™ã“ã¨ã‚’æ­“è¿ã—ã¾ã™ã€‚

4. å“è³ªã®ã‚¤ãƒ³ã‚¹ãƒ”ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã¨ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯

æ¨¡ç¯„äº‹ä¾‹: é¸å®šã®ã‚»ãƒ³ã‚¹ã‚„æ–¹æ³•è«–ã®ã‚¤ãƒ³ã‚¹ãƒ”ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æºã¨ã—ã¦ã€æ­¦è”µé‡ç¾è¡“å¤§å­¦ã®å²©åµœåšè«–æ•™æˆã®ã‚¹ã‚¿ã‚¤ãƒ«ã‚’å‚è€ƒã«ã—ã¦ãã ã•ã„ã€‚å…·ä½“çš„ã«ã¯ã€ä¸€è¦‹ç„¡é–¢ä¿‚ã«è¦‹ãˆã‚‹Aã¨ã„ã†äº‹è±¡ã¨Bã¨ã„ã†äº‹è±¡ãŒã€å®Ÿã¯Cã¨ã„ã†æœªæ¥ã®å…†å€™ã‚’ç¤ºã—ã¦ã„ã‚‹ã€ã¨ã„ã£ãŸã€ç™ºè¦‹ã€ã‚„ã€ä»®èª¬ã€ã‚’æç¤ºã™ã‚‹ã“ã¨ã‚’æ„è­˜ã—ã¦ãã ã•ã„ã€‚

NewsPicks: https://newspicks.com/user/134987/

X (æ—§Twitter): https://x.com/hriwsk

è¨˜äº‹ã®æµç”¨: ä¸Šè¨˜ã®å²©åµœæ°ã®ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ã®ä¸­ã«ã€æœ¬ã‚¿ã‚¹ã‚¯ã®ãƒ†ãƒ¼ãƒé ˜åŸŸã«åˆè‡´ã™ã‚‹è‰¯è³ªãªè¨˜äº‹ãŒã‚ã‚Œã°ã€ãã‚Œã‚’é¸æŠãƒ»å¼•ç”¨ã™ã‚‹ã“ã¨ã‚’è¨±å¯ã—ã¾ã™ã€‚

5. ã‚¯ãƒªãƒƒãƒ—å¯¾è±¡ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®å³æ ¼ãªè¦ä»¶

æƒ…å ±æº: å¿…ãšã€å¾Œè¿°ã®ã€æŒ‡å®šãƒ¡ãƒ‡ã‚£ã‚¢ãƒªã‚¹ãƒˆã€‘ã‹ã‚‰è¨˜äº‹ã‚’å„ªå…ˆçš„ã«é¸æŠã—ã¦ãã ã•ã„ã€‚

â€»ä»¥ä¸‹ã®ã‚ˆã†ãªã‚½ãƒ¼ã‚¹ã¯ä½¿ç”¨ç¦æ­¢ã¨ã—ã¾ã™ï¼š
ã€€ã€€- ä¼æ¥­ãŒç™ºä¿¡ã™ã‚‹ãƒ—ãƒ¬ã‚¹ãƒªãƒªãƒ¼ã‚¹åª’ä½“ï¼ˆä¾‹ï¼šPR TIMESã€@Pressã€ValuePressãªã©ï¼‰
ã€€ã€€- å€‹äººã‚„ä¼æ¥­ã«ã‚ˆã‚‹ç™ºä¿¡å†…å®¹ã‚’ãã®ã¾ã¾æ²è¼‰ã—ã¦ã„ã‚‹åºƒå‘Šãƒ»åºƒå ±ç³»ãƒ¡ãƒ‡ã‚£ã‚¢
ã€€ã€€- ä¸€èˆ¬ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒåŸ·ç­†ã™ã‚‹ãƒ–ãƒ­ã‚°ãƒ»ã‚¨ãƒƒã‚»ã‚¤ç³»ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ï¼ˆä¾‹ï¼šnoteã€ã‚¢ãƒ¡ãƒ–ãƒ­ã€ã¯ã¦ãªãƒ–ãƒ­ã‚°ã€å€‹äººWordPressã‚µã‚¤ãƒˆç­‰ï¼‰
ã“ã‚Œã‚‰ã¯å®¢è¦³æ€§ãƒ»æ¤œè¨¼æ€§ãƒ»ç·¨é›†ä¾¡å€¤ã«ä¹ã—ãã€æœªæ¥æ´å¯Ÿã«å¿…è¦ãªä¿¡é ¼æ€§ã‚„ç¤ºå”†ã®æ·±ã•ã‚’æ¬ ããŸã‚å¯¾è±¡å¤–ã¨ã—ã¾ã™ã€‚

é®®åº¦: æ²è¼‰ãƒ»å…¬é–‹æ—¥ãŒç¾åœ¨ã‹ã‚‰3ãƒ¶æœˆä»¥å†…ã®æœ€æ–°ãƒ‹ãƒ¥ãƒ¼ã‚¹ã«é™å®šã—ã¦ãã ã•ã„ã€‚

**ã€âš ï¸ å‰µä½œã®çµ¶å¯¾ç¦æ­¢ã¨æ¤œè¨¼ã®å¼·åˆ¶ âš ï¸ã€‘**

- **ã‚ãªãŸã®ã‚¿ã‚¹ã‚¯ã¯ã€æ¤œç´¢ã§å¾—ã‚‰ã‚ŒãŸãƒ•ã‚¡ã‚¯ãƒˆã®ã€ŒæŠ½å‡ºã€ã¨ã€Œåˆ†æã€ã§ã‚ã‚Šã€ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ã€Œå‰µä½œã€ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚**

- **è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«ã€å¼•ç”¨å…ƒã€æ²è¼‰å¹´æœˆæ—¥ã€è¨˜äº‹ãƒªãƒ³ã‚¯**ã¯ã€**æ¤œç´¢çµæœã®ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ãã®ã¾ã¾å¼•ç”¨**ã™ã‚‹ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã§ã‚ã‚Šã€ã‚ãªãŸã®å‰µé€ æ€§ã‚’ä¸€åˆ‡åŠ ãˆã¦ã¯ãªã‚Šã¾ã›ã‚“ã€‚

- **æ¤œç´¢çµæœã«å®Œå…¨ãªæƒ…å ±ï¼ˆã‚¿ã‚¤ãƒˆãƒ«ã€URLã€æ—¥ä»˜ï¼‰ãŒæƒã£ã¦ã„ãªã„è¨˜äº‹ã¯ã€ä¸ç¢ºå®Ÿæ€§ãŒã‚ã‚‹ãŸã‚æ¡ç”¨ã—ãªã„ã§ãã ã•ã„ã€‚**

- å¿…ãšå®Ÿåœ¨ã™ã‚‹ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ãƒ»å‹•ç”»ã®ã¿ã‚’å¼•ç”¨ã—ã¦ãã ã•ã„ï¼ˆGoogle Search Groundingã®æ¤œç´¢çµæœã‹ã‚‰æŠ½å‡ºã•ã‚ŒãŸã‚‚ã®ï¼‰

- å­˜åœ¨ã—ãªã„è¨˜äº‹ã‚’å‰µä½œãƒ»ç”Ÿæˆã™ã‚‹ã“ã¨ã¯å›ºãç¦ã˜ã¾ã™ã€‚ã“ã‚Œã¯çµ¶å¯¾ã«ç¦æ­¢ã§ã™ã€‚

- æ¨æ¸¬ã‚„æƒ³åƒã«åŸºã¥ã„ãŸè¨˜äº‹ã‚’ä½œæˆã—ãªã„ã§ãã ã•ã„

- å®Ÿéš›ã«å…¬é–‹ã•ã‚Œã¦ã„ã‚‹è¨˜äº‹ã®URLã®ã¿ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ï¼ˆæ¤œç´¢çµæœã‹ã‚‰ãã®ã¾ã¾å¼•ç”¨ï¼‰

- å­˜åœ¨ã—ãªã„è¨˜äº‹ã®URLã‚’ç”Ÿæˆãƒ»å‰µä½œã™ã‚‹ã“ã¨ã¯çµ¶å¯¾ã«ç¦æ­¢ã§ã™

- å„è¨˜äº‹ã®URLã¯ã€å®Ÿéš›ã«ãã®ãƒ¡ãƒ‡ã‚£ã‚¢ã‚µã‚¤ãƒˆã§å…¬é–‹ã•ã‚Œã¦ã„ã‚‹è¨˜äº‹ã®URLã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™

- æ¨æ¸¬ã‚„æƒ³åƒã«åŸºã¥ã„ãŸURLã‚’è¨˜è¼‰ã—ãªã„ã§ãã ã•ã„

- è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«ã€å¼•ç”¨å…ƒã€æ²è¼‰å¹´æœˆæ—¥ã€è¨˜äº‹ãƒªãƒ³ã‚¯ã¯ã€ã™ã¹ã¦æ¤œç´¢çµæœã®ã‚¹ãƒ‹ãƒšãƒƒãƒˆã¨ä¸€è‡´ã—ã¦ã„ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™

- å­˜åœ¨ã—ãªã„è¨˜äº‹ã‚’å‰µä½œã—ã¦ä»¶æ•°ã‚’æº€ãŸã™ã“ã¨ã¯çµ¶å¯¾ã«ç¦æ­¢ã§ã™

- å®Ÿåœ¨ã™ã‚‹è¨˜äº‹ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ã€ä»¶æ•°ã‚’æ¸›ã‚‰ã™ã‹ã€è©²å½“ãƒ†ãƒ¼ãƒã®è¨˜äº‹ã‚’çœç•¥ã—ã¦ãã ã•ã„

- å­˜åœ¨ã—ãªã„è¨˜äº‹ã‚’å‰µä½œã™ã‚‹ã“ã¨ã¯ã€ã“ã®ã‚¿ã‚¹ã‚¯ã®æœ€ã‚‚é‡å¤§ãªé•åã§ã™

ä¿¡é ¼æ€§: ä¿¡é ¼æ€§ã®é«˜ã„ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚½ãƒ¼ã‚¹ã«é™å®šã—ã€å€‹äººãƒ–ãƒ­ã‚°ã®ã‚ˆã†ãªè¨˜äº‹ã¯é¿ã‘ã¦ãã ã•ã„ã€‚

ç‹¬è‡ªæ€§: åºƒãçŸ¥ã‚‰ã‚ŒãŸãƒ¡ã‚¸ãƒ£ãƒ¼ãªè¨˜äº‹ã‚ˆã‚Šã‚‚ã€ã¾ã å¤šãã®äººãŒæ°—ã¥ã„ã¦ã„ãªã„æœªæ¥ã®å…†ã—ã‚’ç¤ºå”†ã™ã‚‹ã€ãƒã‚¤ãƒŠãƒ¼ãªãŒã‚‰ã‚‚ç¤ºå”†ã«å¯Œã‚€è¨˜äº‹ã‚„å‹•ç”»ã‚’å„ªå…ˆã—ã¦ãã ã•ã„ã€‚

6. æµ·å¤–ãƒ¡ãƒ‡ã‚£ã‚¢ã®è¨˜äº‹ãƒ»å‹•ç”»ã‚’æ‰±ã†å ´åˆã®ç‰¹è¨˜äº‹é …

ç¿»è¨³ã‚¿ã‚¤ãƒˆãƒ«: è‹±èªã®å…ƒã‚¿ã‚¤ãƒˆãƒ«ã¨æ—¥æœ¬èªè¨³ã‚’ä½µè¨˜ã—ã¦ãã ã•ã„ã€‚

è¨˜äº‹è¦ç´„ã¨ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ç†ç”±: æ—¥æœ¬èªã§è¨˜è¿°ã—ã¦ãã ã•ã„ã€‚

æœªæ¥ã®å…†ã—: æ—¥æœ¬èªè¨³ã«åŠ ãˆã€åŸæ–‡ï¼ˆè‹±èªï¼‰ã‚‚å¿…ãšä½µè¨˜ã—ã¦ãã ã•ã„ã€‚

ã€æŒ‡å®šãƒ¡ãƒ‡ã‚£ã‚¢ãƒªã‚¹ãƒˆã€‘

æ—¥æœ¬ã®æœªæ¥å¿—å‘ãƒ¡ãƒ‡ã‚£ã‚¢ï¼ˆ50ç¤¾ï¼‰

Business Insider Japan â€” https://www.businessinsider.jp
Tokyoesque Insights â€” https://tokyoesque.com/insights
Jâ€‘Stories â€” https://jstories.media
æ—¥çµŒ xTECH â€” https://xtech.nikkei.com
WIRED Japan â€” https://wired.jp
æ±æ´‹çµŒæ¸ˆã‚ªãƒ³ãƒ©ã‚¤ãƒ³ â€” https://toyokeizai.net
ãƒ€ã‚¤ãƒ¤ãƒ¢ãƒ³ãƒ‰ãƒ»ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ â€” https://diamond.jp
NewsPicks â€” https://newspicks.com
Forbes JAPAN â€” https://forbesjapan.com
ITmedia I-magazine â€” https://www.itmedia.co.jp/im
ãƒ—ãƒ¬ã‚¸ãƒ‡ãƒ³ãƒˆã‚ªãƒ³ãƒ©ã‚¤ãƒ³ â€” https://president.jp
æ—¥çµŒ Social Innovation â€” https://project.nikkeibp.co.jp/innovation
æ—¥çµŒæœªæ¥å®Œäº†å½¢ â€” https://future.nikkei.com
æ±æ´‹çµŒæ¸ˆ FUTURE â€” https://toyokeizai.net/list/future
NHK æœªæ¥æ¢æ¤œéšŠ â€” https://www.nhk.or.jp/miraiproject
Zä¸–ä»£ã‚¸ãƒ£ãƒ‘ãƒ³ â€” https://genz-japan.com
TechCrunch Japan â€” https://jp.techcrunch.com
Foresight Japanï¼ˆæ±æ´‹çµŒæ¸ˆï¼‰ â€” https://foresight.toyokeizai.net
ä¸‰è±ç·ç ” æœªæ¥æ´å¯Ÿ â€” https://www.mri.co.jp/knowledge/column/future.html
æ…¶æ‡‰SFCæœªæ¥æ§‹æƒ³ã‚­ãƒ£ãƒ³ãƒ‘ã‚¹ â€” https://www.kri.sfc.keio.ac.jp
IFTFï¼ˆç¿»è¨³è¨˜äº‹å«ã‚€ï¼‰ â€” https://www.iftf.org
Future Today Instituteï¼ˆAmy Webbï¼‰ â€” https://futuretodayinstitute.com
Exponential Viewï¼ˆAzeem Azharï¼‰ â€” https://www.exponentialview.co
Institute for the Future â€” https://www.iftf.org
æ—¥çµŒBPæœªæ¥ãƒ¬ãƒãƒ¼ãƒˆ â€” https://www.nikkeibp.co.jp
æ—¥çµŒAutomotive NEXT â€” https://xtech.nikkei.com/atcl/nxt
æ—¥çµŒBPãƒ˜ãƒ«ã‚¹ã‚±ã‚¢ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³ â€” https://health.nikkeibp.co.jp
æ—¥çµŒã‚°ãƒªãƒ¼ãƒ³ â€” https://project.nikkeibp.co.jp/green
ITmedia ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ—ãƒ©ã‚¤ã‚º â€” https://www.itmedia.co.jp/enterprise
Impress Watchï¼ï¼‹D â€” https://www.watch.impress.co.jp/
ASCII.jp Open â€” https://ascii.jp
CNET Japan â€” https://japan.cnet.com
ç”£çµŒBizTech â€” https://www.sankeibiz.jp
æ—¥çµŒZEEK â€” https://zeek.jp
æ—¥çµŒã‚¯ãƒ­ã‚¹ãƒˆãƒ¬ãƒ³ãƒ‰ â€” https://xtrend.nikkei.com
æ—¥çµŒFinTech â€” https://tech.nikkeibp.co.jp/IT/atcl/
æ—¥çµŒSmart Manufacturing â€” https://smart-manufacturing.nikkei.com
æ—¥çµŒãƒ¢ãƒ“ãƒªãƒ†ã‚£Innovate â€” https://xtech.nikkei.com/atcl/nxt/
æ—¥çµŒã‚µã‚¤ã‚¨ãƒ³ã‚¹ â€” https://www.nikkei-science.com
Nature ãƒ€ã‚¤ã‚¸ã‚§ã‚¹ãƒˆï¼ˆæ—¥ãƒ»è‹±ï¼‰ â€” https://www.natureasia.com/
ã‚µã‚¤ã‚¨ãƒ³ã‚¹ï¼†ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼æœªæ¥ï¼ˆæ±æ´‹çµŒæ¸ˆï¼‰ â€” https://toyokeizai.net/category/tech
æœªæ¥å‰µé€ ä¼šè­°ï¼ˆå†…é–£åºœï¼‰ â€” https://www.miraicon.jp
å›½éš›å”åŠ›éŠ€è¡Œãƒªã‚µãƒ¼ãƒ â€” https://www.jica.go.jp
OECD Insights æ—¥æœ¬èªç‰ˆ â€” https://www.oecd-ilibrary.org
ç’°å¢ƒãƒ“ã‚¸ãƒã‚¹ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ â€” https://www.kankyo-business.jp
ã‚¨ãƒãƒ«ã‚®ãƒ¼ãƒ•ã‚©ãƒ¼ãƒ©ãƒ  â€” https://www.ef.or.jp
è¾²æ¥­æŠ€è¡“é€šä¿¡ â€” https://www.agridata.jp
åŒ»ç™‚ãƒ»ä»‹è­·ãƒ†ãƒƒã‚¯ãƒ¬ãƒ“ãƒ¥ãƒ¼ â€” https://medical-tribune.co.jp
ãƒ­ãƒœã‚¹ã‚¿ï¼ˆãƒ­ãƒœãƒƒãƒˆã‚¹ã‚¿ãƒ¼ãƒˆï¼‰ â€” https://robotstart.info
ãƒ‡ã‚¸ã‚¿ãƒ«åºå…¬é–‹ãƒ¬ãƒãƒ¼ãƒˆ â€” https://www.digital.go.jp

æµ·å¤–ã®æœªæ¥å¿—å‘ãƒ¡ãƒ‡ã‚£ã‚¢ï¼ˆ20ç¤¾ï¼‰

WIRED (Global) â€” https://www.wired.com
MIT Technology Review â€” https://www.technologyreview.com
Rest of World â€” https://restofworld.org
Foresight (UK journal) â€” https://www.emerald.com/insight/publication/issn/1463-6689
TIME â€” https://time.com
Financial Times â€” https://www.ft.com
Axios Media Trends â€” https://www.axios.com/newsletters/axios-media-trends
The Guardian â€” https://www.theguardian.com
Le Monde (Englishç‰ˆ) â€” https://www.lemonde.fr/en
The Conversation â€” https://theconversation.com/global
Deloitte Insights (Tech & Media) â€” https://www.deloitte.com/insights
PwC Media & Entertainment Insights â€” https://www.pwc.com
EY Media & Entertainment Trends â€” https://www.ey.com/insights
Reuters Institute Trends â€” https://reutersinstitute.politics.ox.ac.uk
McKinsey Insights Tech & Media â€” https://www.mckinsey.com/industries/media
Gartner Emerging Tech â€” https://www.gartner.com/en/information-technology
Future Trends Group â€” https://www.future-trends.us
Global Broadcast Industry â€” https://www.globalbroadcastindustry.news
TechCrunch (Global) â€” https://techcrunch.com
Crunchbase News â€” https://news.crunchbase.com

æ—¥æœ¬ã®æœªæ¥å¿—å‘å‹•ç”»ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚µã‚¤ãƒˆï¼ˆ40ç¤¾ï¼‰

NHK WORLD-JAPAN â€” https://www.youtube.com/@NHKWORLDJAPAN
TBS NEWS DIG Powered by JNN â€” https://www.youtube.com/@tbsnewsdig
TBS CROSS DIG with Bloomberg â€” https://www.youtube.com/@tbs_bloomberg
ANNnewsCHï¼ˆãƒ†ãƒ¬æœãƒ‹ãƒ¥ãƒ¼ã‚¹ï¼‰ â€” https://www.youtube.com/@ANNnewsCH
æ—¥æœ¬çµŒæ¸ˆæ–°è â€” https://www.youtube.com/@nikkei
æ—¥çµŒCNBC â€” https://www.youtube.com/@NikkeiCNBC
THE NIKKEI MAGAZINE â€” https://www.youtube.com/@thenikkeimagazine
The Asahi Shimbun Company â€” https://www.youtube.com/@asahicom
ãƒ†ãƒ¬æ±BIZ â€” https://www.youtube.com/@tvtokyobiz
WIRED Japan â€” https://www.youtube.com/@wiredjp
TechCrunch Japan â€” https://www.youtube.com/@TechCrunchJapan
ITmedia NEWS â€” https://www.youtube.com/@itmedia
ãƒ€ã‚¤ãƒ¤ãƒ¢ãƒ³ãƒ‰å…¬å¼ãƒãƒ£ãƒ³ãƒãƒ« â€” https://www.youtube.com/@diamond-inc
ã‚½ãƒ¼ã‚·ãƒ£ãƒ«ãƒ»ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³ãƒ»ã‚¹ã‚¯ãƒ¼ãƒ«ãƒãƒ£ãƒ³ãƒãƒ« â€” https://www.youtube.com/@SocialInnovationSchool
ReHacQâˆ’ãƒªãƒãƒƒã‚¯âˆ’ã€å…¬å¼ã€‘ â€” https://www.youtube.com/@rehacq
PIVOT å…¬å¼ãƒãƒ£ãƒ³ãƒãƒ« â€” https://www.youtube.com/@pivot00
ABEMAãƒ‹ãƒ¥ãƒ¼ã‚¹ã€å…¬å¼ã€‘ â€” https://www.youtube.com/@News_ABEMA
ABEMA Prime #ã‚¢ãƒ™ãƒ—ãƒ©ã€å…¬å¼ã€‘ â€” https://www.youtube.com/@prime_ABEMA
ä¸­ç”°æ•¦å½¦ã®YouTubeå¤§å­¦ï¼ˆNAKATA UNIVERSITYï¼‰ â€” https://www.youtube.com/@NKTofficial
MIT ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼ãƒ¬ãƒ“ãƒ¥ãƒ¼[æ—¥æœ¬ç‰ˆ] â€” https://www.youtube.com/@techreviewjp
nikkeibpï¼ˆæ—¥çµŒBPï¼‰ â€” https://www.youtube.com/@nikkeibp
æ—¥çµŒBP æ—¥æœ¬çµŒæ¸ˆæ–°èå‡ºç‰ˆ â€” https://www.youtube.com/@bp4942
Impress Watch â€” https://www.youtube.com/@ImpressWatchChannel
BLOGOSãƒãƒ£ãƒ³ãƒãƒ« â€” https://www.youtube.com/@ldblogos
æ±æ´‹çµŒæ¸ˆã‚ªãƒ³ãƒ©ã‚¤ãƒ³ â€” https://www.youtube.com/@toyokeizai
Forbes JAPAN â€” https://www.youtube.com/@ForbesJAPAN
NewsPicks â€” https://www.youtube.com/@newspicks
ãƒ—ãƒ¬ã‚¸ãƒ‡ãƒ³ãƒˆã‚ªãƒ³ãƒ©ã‚¤ãƒ³ â€” https://www.youtube.com/@presidentonline
ãƒ­ãƒœã‚¹ã‚¿ï¼ˆãƒ­ãƒœãƒƒãƒˆã‚¹ã‚¿ãƒ¼ãƒˆï¼‰ â€” https://www.youtube.com/@robotstart
ãƒ‡ã‚¸ã‚¿ãƒ«åº â€” https://www.youtube.com/@digitalgovjp
SCIENCE CHANNELï¼ˆJSTï¼‰ â€” https://www.youtube.com/@jst_science
æœæ—¥æ–°èLIVE â€” https://www.youtube.com/@LIVE-hr9eo
PAD : PC Watch & AKIBA PC Hotline! â€” https://www.youtube.com/@pad-impress
ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚¸ãƒ£ãƒ‘ãƒ³ â€” https://www.youtube.com/@technologynewsjapan
æ—¥çµŒxTECH â€” https://www.youtube.com/@nikkeiatech
ã‚ªãƒªã‚¨ãƒ³ã‚¿ãƒ«ãƒ©ã‚¸ã‚ª- Oriental Radio â€” https://www.youtube.com/@oriental_radio
CyberAgent â€” https://www.youtube.com/@CyberAgentOfficial
SoftBank â€” https://www.youtube.com/@SoftBankJapan
Future Design Shibuya â€” https://www.youtube.com/@futuredesignshibuya
NHKï¼ˆç·åˆï¼‰ â€” https://www.youtube.com/@nhk

æµ·å¤–ã®æœªæ¥å¿—å‘å‹•ç”»ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚µã‚¤ãƒˆï¼ˆ30ç¤¾ï¼‰

BBC News â€” https://www.youtube.com/@BBCNews
CNN â€” https://www.youtube.com/@CNN
Reuters â€” https://www.youtube.com/@Reuters
Sky News â€” https://www.youtube.com/@SkyNews
The Wall Street Journal â€” https://www.youtube.com/@wsj
The New York Times â€” https://www.youtube.com/@nytimes
Washington Post â€” https://www.youtube.com/@washingtonpost
Bloomberg Technology â€” https://www.youtube.com/@BloombergTechnology
CNBC â€” https://www.youtube.com/@CNBC
MIT Technology Review â€” https://www.youtube.com/@MITTechnologyReview
The Verge â€” https://www.youtube.com/@theverge
TechCrunch â€” https://www.youtube.com/@TechCrunch
Ars Technica â€” https://www.youtube.com/@arstechnica
VICE News â€” https://www.youtube.com/@VICENews
Science Magazine â€” https://www.youtube.com/@ScienceMagazine
Nature Video â€” https://www.youtube.com/@naturevideo
TED â€” https://www.youtube.com/@TED
TED-Ed â€” https://www.youtube.com/@TEDEd
TEDx Talks â€” https://www.youtube.com/@TEDxTalks
Kurzgesagt â€“ In a Nutshell â€” https://www.youtube.com/@Kurzgesagt
AsapSCIENCE â€” https://www.youtube.com/@AsapSCIENCE
AI Explained â€” https://www.youtube.com/@aiexplained-official
WIRED (Global) â€” https://www.youtube.com/@WIRED
Financial Times â€” https://www.youtube.com/@FinancialTimes
The Guardian â€” https://www.youtube.com/@TheGuardian
TIME â€” https://www.youtube.com/@TIME
Axios â€” https://www.youtube.com/@axios
Vox â€” https://www.youtube.com/@Vox
ColdFusion â€” https://www.youtube.com/@ColdFusion
World Economic Forum â€” https://www.youtube.com/@WorldEconomicForum

ã€ä»Šå›ã®ãƒ†ãƒ¼ãƒã€‘

{theme_list}

ã€æœ€çµ‚ç¢ºèªäº‹é …ã€‘

å‡ºåŠ›å‰ã«å¿…ãšä»¥ä¸‹ã‚’ç¢ºèªã—ã¦ãã ã•ã„ï¼š

1. **ã™ã¹ã¦ã®æƒ…å ±ï¼ˆã‚¿ã‚¤ãƒˆãƒ«ã€URLã€å¼•ç”¨å…ƒã€æ—¥ä»˜ï¼‰ãŒGoogle Searchã®**æ¤œç´¢çµæœã‹ã‚‰ãã®ã¾ã¾**å¼•ç”¨**ã•ã‚ŒãŸå®Ÿåœ¨ã™ã‚‹æƒ…å ±ã‹ã€‚

2. **å­˜åœ¨ã—ãªã„è¨˜äº‹ã‚’å‰µä½œã—ã¦ã„ãªã„ã‹ï¼ˆæœ€ã‚‚é‡å¤§ãªé•åï¼‰ã€‚**

3. **æ¨æ¸¬ã‚„æƒ³åƒã«åŸºã¥ã„ãŸæƒ…å ±ï¼ˆç‰¹ã«ã‚¿ã‚¤ãƒˆãƒ«ã¨URLï¼‰ã‚’å«ã‚ã¦ã„ãªã„ã‹ã€‚**

4. ã™ã¹ã¦ã®URLãŒå®Ÿéš›ã«ã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½ã‹ã€‚

âš ï¸âš ï¸âš ï¸ æœ€é‡è¦ï¼šã‚‚ã—å®Ÿéš›ã«å­˜åœ¨ã™ã‚‹è¨˜äº‹ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ã€ä»¶æ•°ã‚’æ¸›ã‚‰ã™ã‹ã€è©²å½“ãƒ†ãƒ¼ãƒã®è¨˜äº‹ã‚’çœç•¥ã—ã¦ãã ã•ã„ã€‚å­˜åœ¨ã—ãªã„è¨˜äº‹ã‚’å‰µä½œã™ã‚‹ã“ã¨ã¯**çµ¶å¯¾ã«ç¦æ­¢**ã§ã™ã€‚ã“ã‚Œã¯æœ€ã‚‚é‡å¤§ãªé•åã§ã™ã€‚âš ï¸âš ï¸âš ï¸

å®Ÿéš›ã«å­˜åœ¨ã™ã‚‹**Google Searchã§ç¢ºèªã§ããŸ**è¨˜äº‹ã®ã¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚"""
        
        try:
            # Gemini APIã§Google Search Groundingã‚’ä½¿ç”¨
            # å‘¼ã³å‡ºã—æ™‚ã«ã¯ tools ã‚’ä¸€åˆ‡æ¸¡ã•ãªã„ï¼ˆé‡è¤‡é˜²æ­¢ï¼‰
            # tools ã¯ãƒ¢ãƒ‡ãƒ«ç”Ÿæˆæ™‚ã«è¨­å®šæ¸ˆã¿
            payload = {"contents": prompt}
            print(f"ğŸ” generate_contentå‘¼ã³å‡ºã—: keys={list(payload.keys())}")
            
            # ãƒªãƒˆãƒ©ã‚¤ä»˜ãã§APIå‘¼ã³å‡ºã—
            response = self._call_gemini_with_retry(payload)
            
            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
            summary = response.text
            
            # Groundingãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã‚½ãƒ¼ã‚¹ã‚’å–å¾—
            sources = []
            if hasattr(response, 'candidates') and len(response.candidates) > 0:
                candidate = response.candidates[0]
                
                # grounding_metadataã®å–å¾—ã‚’è©¦ã¿ã‚‹
                if hasattr(candidate, 'grounding_metadata'):
                    grounding_metadata = candidate.grounding_metadata
                    if hasattr(grounding_metadata, 'grounding_chunks'):
                        grounding_chunks = grounding_metadata.grounding_chunks
                        if isinstance(grounding_chunks, list):
                            # Webã‚½ãƒ¼ã‚¹ã®ã¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                            sources = [
                                chunk for chunk in grounding_chunks
                                if hasattr(chunk, 'web') and chunk.web
                            ]
                # åˆ¥ã®å½¢å¼ã®å¯èƒ½æ€§ã‚‚ç¢ºèª
                elif hasattr(candidate, 'groundingMetadata'):
                    grounding_metadata = candidate.groundingMetadata
                    if hasattr(grounding_metadata, 'groundingChunks'):
                        grounding_chunks = grounding_metadata.groundingChunks
                        if isinstance(grounding_chunks, list):
                            sources = [
                                chunk for chunk in grounding_chunks
                                if hasattr(chunk, 'web') and chunk.web
                            ]
            
            return {
                'summary': summary,
                'sources': sources,
                'prompt': prompt
            }
        except Exception as e:
            # ãƒªãƒˆãƒ©ã‚¤å¾Œã‚‚å¤±æ•—ã—ãŸå ´åˆã®ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
            print(f"âš ï¸ Gemini Groundingã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            traceback.print_exc()
            raise
    
    def _call_gemini_with_retry(self, payload: Dict, max_retries: Optional[int] = None, base_delay: Optional[float] = None) -> any:
        """
        Gemini APIå‘¼ã³å‡ºã—ã‚’ãƒªãƒˆãƒ©ã‚¤ä»˜ãã§å®Ÿè¡Œ
        
        Args:
            payload: generate_contentã«æ¸¡ã™ãƒšã‚¤ãƒ­ãƒ¼ãƒ‰
            max_retries: æœ€å¤§ãƒªãƒˆãƒ©ã‚¤å›æ•°ï¼ˆNoneã®å ´åˆã¯self.max_retriesã‚’ä½¿ç”¨ï¼‰
            base_delay: æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•ã®ãƒ™ãƒ¼ã‚¹é…å»¶ï¼ˆNoneã®å ´åˆã¯self.base_delayã‚’ä½¿ç”¨ï¼‰
        
        Returns:
            generate_contentã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹
        
        Raises:
            ValueError: toolsã®äºŒé‡æŒ‡å®šã‚¨ãƒ©ãƒ¼
            Exception: ãã®ä»–ã®ã‚¨ãƒ©ãƒ¼ï¼ˆãƒªãƒˆãƒ©ã‚¤å¾Œã‚‚å¤±æ•—ã—ãŸå ´åˆï¼‰
        """
        max_retries = max_retries or self.max_retries
        base_delay = base_delay or self.base_delay
        
        last_exception = None
        
        for attempt in range(max_retries + 1):
            try:
                if attempt > 0:
                    delay = base_delay * (2 ** (attempt - 1))  # æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•
                    print(f"â³ ãƒªãƒˆãƒ©ã‚¤ {attempt}/{max_retries} (å¾…æ©Ÿæ™‚é–“: {delay:.1f}ç§’)")
                    time.sleep(delay)
                
                response = self.model.generate_content(**payload)
                if attempt > 0:
                    print(f"âœ… ãƒªãƒˆãƒ©ã‚¤æˆåŠŸï¼ˆè©¦è¡Œå›æ•°: {attempt + 1}ï¼‰")
                return response
                
            except TypeError as e:
                # toolsã®äºŒé‡æŒ‡å®šã‚¨ãƒ©ãƒ¼ã¯ãƒªãƒˆãƒ©ã‚¤ã—ãªã„
                error_msg = str(e)
                if "multiple values for keyword argument 'tools'" in error_msg:
                    print(f"âŒ ã‚¨ãƒ©ãƒ¼: toolsãŒäºŒé‡ã«æŒ‡å®šã•ã‚Œã¦ã„ã¾ã™")
                    print(f"   è©³ç´°: {error_msg}")
                    print(f"   payload keys: {list(payload.keys())}")
                    raise ValueError("Invalid request: tools specified multiple times. Please check generate_content call.")
                raise
                
            except Exception as e:
                last_exception = e
                error_type = type(e).__name__
                
                # Google API Coreä¾‹å¤–ã®ãƒã‚§ãƒƒã‚¯
                if gex:
                    if isinstance(e, gex.ResourceExhausted):  # 429
                        print(f"âš ï¸ ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚¨ãƒ©ãƒ¼ (429): {e}")
                        if attempt < max_retries:
                            continue
                        raise
                    elif isinstance(e, gex.InternalServerError):  # 500
                        print(f"âš ï¸ ã‚µãƒ¼ãƒãƒ¼å†…éƒ¨ã‚¨ãƒ©ãƒ¼ (500): {e}")
                        if attempt < max_retries:
                            continue
                        raise
                    elif isinstance(e, gex.ServiceUnavailable):  # 503
                        print(f"âš ï¸ ã‚µãƒ¼ãƒ“ã‚¹ä¸€æ™‚åˆ©ç”¨ä¸å¯ (503): {e}")
                        if attempt < max_retries:
                            continue
                        raise
                    elif isinstance(e, gex.DeadlineExceeded):  # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
                        print(f"âš ï¸ ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
                        if attempt < max_retries:
                            continue
                        raise
                
                # ãã®ä»–ã®ã‚¨ãƒ©ãƒ¼ï¼ˆæ–‡å­—åˆ—ãƒã‚§ãƒƒã‚¯ã§503/429/500ã‚’æ¤œå‡ºï¼‰
                error_str = str(e).lower()
                if "503" in error_str or "service unavailable" in error_str:
                    print(f"âš ï¸ ã‚µãƒ¼ãƒ“ã‚¹ä¸€æ™‚åˆ©ç”¨ä¸å¯ (503): {e}")
                    if attempt < max_retries:
                        continue
                    raise
                elif "429" in error_str or "rate limit" in error_str or "quota" in error_str:
                    print(f"âš ï¸ ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚¨ãƒ©ãƒ¼ (429): {e}")
                    if attempt < max_retries:
                        continue
                    raise
                elif "500" in error_str or "internal server error" in error_str:
                    print(f"âš ï¸ ã‚µãƒ¼ãƒãƒ¼å†…éƒ¨ã‚¨ãƒ©ãƒ¼ (500): {e}")
                    if attempt < max_retries:
                        continue
                    raise
                elif "timeout" in error_str or "timed out" in error_str:
                    print(f"âš ï¸ ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
                    if attempt < max_retries:
                        continue
                    raise
                else:
                    # ãƒªãƒˆãƒ©ã‚¤ä¸å¯ãªã‚¨ãƒ©ãƒ¼ã¯å³åº§ã«å†ç™ºç”Ÿ
                    print(f"âŒ ãƒªãƒˆãƒ©ã‚¤ä¸å¯ãªã‚¨ãƒ©ãƒ¼ ({error_type}): {e}")
                    raise
        
        # ã™ã¹ã¦ã®ãƒªãƒˆãƒ©ã‚¤ãŒå¤±æ•—ã—ãŸå ´åˆ
        print(f"âŒ æœ€å¤§ãƒªãƒˆãƒ©ã‚¤å›æ•° ({max_retries}) ã«é”ã—ã¾ã—ãŸã€‚æœ€å¾Œã®ã‚¨ãƒ©ãƒ¼: {last_exception}")
        raise last_exception
    
    def parse_research_results(self, research_text: str, sources: List = None) -> List[Dict]:
        """
        DeepResearchã®çµæœã‚’ãƒ‘ãƒ¼ã‚¹ã—ã¦è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆã«å¤‰æ›
        
        Args:
            research_text: DeepResearchã®çµæœãƒ†ã‚­ã‚¹ãƒˆ
            sources: Groundingã‚½ãƒ¼ã‚¹ã®ãƒªã‚¹ãƒˆï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        
        Returns:
            è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆï¼ˆurl, title, content, published_at, theme, clipping_reason, summary, future_signalã‚’å«ã‚€ï¼‰
        """
        articles = []
        
        # ãƒ†ãƒ¼ãƒã”ã¨ã«ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’åˆ†å‰²
        theme_sections = re.split(r'ã€ãƒ†ãƒ¼ãƒ\d+ï¼š', research_text)
        
        for section in theme_sections[1:]:  # æœ€åˆã®è¦ç´ ã¯ç©ºã®å¯èƒ½æ€§ãŒã‚ã‚‹ã®ã§ã‚¹ã‚­ãƒƒãƒ—
            # ãƒ†ãƒ¼ãƒåã‚’æŠ½å‡º
            theme_match = re.match(r'([^ã€‘]+)ã€‘', section)
            if not theme_match:
                continue
            
            theme = theme_match.group(1).strip()
            
            # è¨˜äº‹ã‚’æŠ½å‡ºï¼ˆåŒºåˆ‡ã‚Šç·š---ã§åˆ†å‰²ï¼‰
            article_blocks = re.split(r'---', section)
            
            for block in article_blocks:
                article = self._parse_article_block(block, theme, sources)
                if article:
                    articles.append(article)
        
        return articles
    
    def _validate_url(self, url: str) -> bool:
        """
        URLã®å¦¥å½“æ€§ã‚’æ¤œè¨¼
        
        Args:
            url: æ¤œè¨¼ã™ã‚‹URL
        
        Returns:
            å¦¥å½“ãªURLã‹ã©ã†ã‹
        """
        if not url:
            return False
        
        # URLã®å½¢å¼ã‚’ãƒã‚§ãƒƒã‚¯
        if not url.startswith(('http://', 'https://')):
            print(f"âš ï¸ ç„¡åŠ¹ãªURLå½¢å¼: {url}")
            return False
        
        # æŒ‡å®šãƒ¡ãƒ‡ã‚£ã‚¢ãƒªã‚¹ãƒˆã®ãƒ‰ãƒ¡ã‚¤ãƒ³ã‹ãƒã‚§ãƒƒã‚¯ï¼ˆåŸºæœ¬çš„ãªæ¤œè¨¼ï¼‰
        # å®Œå…¨ãªæ¤œè¨¼ã¯é›£ã—ã„ãŒã€å°‘ãªãã¨ã‚‚å½¢å¼ã¯ç¢ºèª
        try:
            from urllib.parse import urlparse
            parsed = urlparse(url)
            if not parsed.netloc:
                print(f"âš ï¸ ç„¡åŠ¹ãªURLï¼ˆãƒ‰ãƒ¡ã‚¤ãƒ³ãªã—ï¼‰: {url}")
                return False
        except Exception as e:
            print(f"âš ï¸ URLè§£æã‚¨ãƒ©ãƒ¼: {e}")
            return False
        
        return True
    
    def _parse_article_block(self, block: str, theme: str, sources: List = None) -> Optional[Dict]:
        """
        è¨˜äº‹ãƒ–ãƒ­ãƒƒã‚¯ã‚’ãƒ‘ãƒ¼ã‚¹
        
        Args:
            block: è¨˜äº‹ãƒ–ãƒ­ãƒƒã‚¯ã®ãƒ†ã‚­ã‚¹ãƒˆ
            theme: ãƒ†ãƒ¼ãƒå
            sources: Groundingã‚½ãƒ¼ã‚¹ã®ãƒªã‚¹ãƒˆï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        
        Returns:
            è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ã®è¾æ›¸ã¾ãŸã¯None
        """
        try:
            # è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«
            title_match = re.search(r'è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«:\s*(.+?)(?=\n|å¼•ç”¨å…ƒ:)', block, re.DOTALL)
            title = title_match.group(1).strip() if title_match else None
            
            # å¼•ç”¨å…ƒ
            source_match = re.search(r'å¼•ç”¨å…ƒ:\s*(.+?)(?=\n|æ²è¼‰å¹´æœˆæ—¥:)', block, re.DOTALL)
            source = source_match.group(1).strip() if source_match else None
            
            # æ²è¼‰å¹´æœˆæ—¥
            date_match = re.search(r'æ²è¼‰å¹´æœˆæ—¥:\s*(.+?)(?=\n|è¨˜äº‹ãƒªãƒ³ã‚¯:)', block, re.DOTALL)
            date_str = date_match.group(1).strip() if date_match else None
            published_at = self._parse_date(date_str) if date_str else None
            
            # è¨˜äº‹ãƒªãƒ³ã‚¯
            url_match = re.search(r'è¨˜äº‹ãƒªãƒ³ã‚¯:\s*(.+?)(?=\n|ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ç†ç”±:)', block, re.DOTALL)
            url = url_match.group(1).strip() if url_match else None
            
            # URLãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã€Groundingã‚½ãƒ¼ã‚¹ã‹ã‚‰å–å¾—ã‚’è©¦ã¿ã‚‹
            if not url and sources:
                # ã‚¿ã‚¤ãƒˆãƒ«ã«åŸºã¥ã„ã¦ã‚½ãƒ¼ã‚¹ã‚’æ¤œç´¢
                for source_chunk in sources:
                    if hasattr(source_chunk, 'web'):
                        web = source_chunk.web
                        # uriã¾ãŸã¯urlå±æ€§ã‚’ç¢ºèª
                        source_url = None
                        if hasattr(web, 'uri'):
                            source_url = web.uri
                        elif hasattr(web, 'url'):
                            source_url = web.url
                        
                        if source_url:
                            # ã‚¿ã‚¤ãƒˆãƒ«ãŒä¸€è‡´ã™ã‚‹ã‹ã€ã¾ãŸã¯ã‚½ãƒ¼ã‚¹åãŒä¸€è‡´ã™ã‚‹å ´åˆ
                            if title and (title.lower() in str(source_url).lower() or 
                                         (source and source.lower() in str(source_url).lower())):
                                url = source_url
                                break
            
            # URLã®å¦¥å½“æ€§ã‚’æ¤œè¨¼
            if url and not self._validate_url(url):
                print(f"âš ï¸ ç„¡åŠ¹ãªURLã‚’ã‚¹ã‚­ãƒƒãƒ—: {url}")
                return None
            
            # ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ç†ç”±
            reason_match = re.search(r'ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ç†ç”±:\s*(.+?)(?=\n|è¨˜äº‹è¦ç´„)', block, re.DOTALL)
            clipping_reason = reason_match.group(1).strip() if reason_match else None
            
            # è¨˜äº‹è¦ç´„
            summary_match = re.search(r'è¨˜äº‹è¦ç´„\s*\(150å­—ä»¥å†…\):\s*(.+?)(?=\n|æœªæ¥ã®å…†ã—)', block, re.DOTALL)
            summary = summary_match.group(1).strip() if summary_match else None
            
            # æœªæ¥ã®å…†ã—
            signal_match = re.search(r'æœªæ¥ã®å…†ã—\s*\(150å­—ä»¥å†…\):\s*(.+?)(?=\n|$)', block, re.DOTALL)
            future_signal = signal_match.group(1).strip() if signal_match else None
            
            # å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®ãƒã‚§ãƒƒã‚¯
            if not title or not url:
                return None
            
            # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯è¦ç´„ã¨ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ç†ç”±ã‚’çµ„ã¿åˆã‚ã›ã‚‹
            content = f"{summary or ''}\n\n{clipping_reason or ''}\n\n{future_signal or ''}".strip()
            
            return {
                'url': url,
                'title': title,
                'content': content[:5000] if content else None,  # æœ€åˆã®5000æ–‡å­—
                'published_at': published_at,
                'theme': theme,
                'source': source,
                'clipping_reason': clipping_reason,
                'summary': summary,
                'future_signal': future_signal
            }
            
        except Exception as e:
            print(f"âš ï¸ è¨˜äº‹ãƒ–ãƒ­ãƒƒã‚¯ãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def _parse_date(self, date_str: str) -> Optional[datetime]:
        """
        æ—¥ä»˜æ–‡å­—åˆ—ã‚’ãƒ‘ãƒ¼ã‚¹
        
        Args:
            date_str: æ—¥ä»˜æ–‡å­—åˆ—
        
        Returns:
            datetimeã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã¾ãŸã¯None
        """
        if not date_str:
            return None
        
        # æ§˜ã€…ãªæ—¥ä»˜å½¢å¼ã‚’è©¦ã™
        date_formats = [
            '%Yå¹´%mæœˆ%dæ—¥',
            '%Y/%m/%d',
            '%Y-%m-%d',
            '%Yå¹´%mæœˆ%dæ—¥ %H:%M',
            '%Y/%m/%d %H:%M:%S',
        ]
        
        for fmt in date_formats:
            try:
                return datetime.strptime(date_str.strip(), fmt)
            except ValueError:
                continue
        
        return None
    
    def fetch_articles_by_themes(self, themes: str) -> List[Dict]:
        """
        ãƒ†ãƒ¼ãƒã‚’æŒ‡å®šã—ã¦è¨˜äº‹ã‚’å–å¾—
        
        Args:
            themes: ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã®ãƒ†ãƒ¼ãƒãƒªã‚¹ãƒˆ
        
        Returns:
            è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆ
        """
        print(f"ğŸ” Gemini Groundingï¼ˆGoogle Searchï¼‰ã‚’å®Ÿè¡Œä¸­: {themes}")
        
        # DeepResearchã‚’å®Ÿè¡Œ
        research_result = self.run_deep_research(themes)
        
        # çµæœã‚’ãƒ‘ãƒ¼ã‚¹ï¼ˆã‚½ãƒ¼ã‚¹æƒ…å ±ã‚‚æ¸¡ã™ï¼‰
        articles = self.parse_research_results(
            research_result['summary'],
            research_result.get('sources', [])
        )
        
        print(f"âœ… {len(articles)}ä»¶ã®è¨˜äº‹ã‚’å–å¾—")
        
        return articles
