"""
è¨˜äº‹å–å¾—ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼ˆRSS/Webã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ï¼‰
"""
import os
import re
import feedparser
import requests
from bs4 import BeautifulSoup
from typing import List, Dict, Optional
from datetime import datetime
from urllib.parse import urljoin, urlparse
import time


class ArticleFetcher:
    """è¨˜äº‹å–å¾—ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, user_agent: str = None):
        """
        åˆæœŸåŒ–
        
        Args:
            user_agent: ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ™‚ã®User-Agent
        """
        self.user_agent = user_agent or "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': self.user_agent
        })
    
    def fetch_from_rss(self, rss_url: str, max_items: int = 10) -> List[Dict]:
        """
        RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰è¨˜äº‹ã‚’å–å¾—
        
        Args:
            rss_url: RSSãƒ•ã‚£ãƒ¼ãƒ‰ã®URL
            max_items: å–å¾—ã™ã‚‹æœ€å¤§è¨˜äº‹æ•°
        
        Returns:
            è¨˜äº‹ã®ãƒªã‚¹ãƒˆï¼ˆurl, title, content, published_atã‚’å«ã‚€ï¼‰
        """
        articles = []
        
        try:
            print(f"ğŸ“¡ RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‚’å–å¾—ä¸­: {rss_url}")
            feed = feedparser.parse(rss_url)
            
            if feed.bozo and feed.bozo_exception:
                print(f"âš ï¸ RSSè§£æã‚¨ãƒ©ãƒ¼: {feed.bozo_exception}")
                return articles
            
            entries = feed.entries[:max_items]
            
            for entry in entries:
                try:
                    # å…¬é–‹æ—¥æ™‚ã®å–å¾—
                    published_at = None
                    if hasattr(entry, 'published_parsed') and entry.published_parsed:
                        published_at = datetime(*entry.published_parsed[:6])
                    elif hasattr(entry, 'updated_parsed') and entry.updated_parsed:
                        published_at = datetime(*entry.updated_parsed[:6])
                    
                    # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å–å¾—
                    content = ""
                    if hasattr(entry, 'content'):
                        content = entry.content[0].value if entry.content else ""
                    elif hasattr(entry, 'summary'):
                        content = entry.summary
                    elif hasattr(entry, 'description'):
                        content = entry.description
                    
                    # HTMLã‚¿ã‚°ã‚’é™¤å»
                    content = self._clean_html(content)
                    
                    article = {
                        'url': entry.link,
                        'title': entry.title,
                        'content': content[:5000] if content else None,  # æœ€åˆã®5000æ–‡å­—
                        'published_at': published_at
                    }
                    articles.append(article)
                    print(f"  âœ“ {entry.title[:50]}...")
                    
                except Exception as e:
                    print(f"  âš ï¸ ã‚¨ãƒ³ãƒˆãƒªå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                    continue
            
            print(f"âœ… {len(articles)}ä»¶ã®è¨˜äº‹ã‚’å–å¾—")
            
        except Exception as e:
            print(f"âš ï¸ RSSå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        
        return articles
    
    def fetch_from_url(self, url: str) -> Optional[Dict]:
        """
        å˜ä¸€URLã‹ã‚‰è¨˜äº‹ã‚’å–å¾—ï¼ˆWebã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ï¼‰
        
        Args:
            url: è¨˜äº‹ã®URL
        
        Returns:
            è¨˜äº‹ã®è¾æ›¸ï¼ˆurl, title, content, published_atã‚’å«ã‚€ï¼‰ã¾ãŸã¯None
        """
        try:
            print(f"ğŸŒ è¨˜äº‹ã‚’å–å¾—ä¸­: {url}")
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            response.encoding = response.apparent_encoding
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ã‚¿ã‚¤ãƒˆãƒ«ã®å–å¾—
            title = None
            if soup.find('title'):
                title = soup.find('title').get_text().strip()
            elif soup.find('h1'):
                title = soup.find('h1').get_text().strip()
            elif soup.find('meta', property='og:title'):
                title = soup.find('meta', property='og:title').get('content', '').strip()
            
            # æœ¬æ–‡ã®å–å¾—
            content = None
            
            # articleã‚¿ã‚°ã‚’å„ªå…ˆ
            article_tag = soup.find('article')
            if article_tag:
                content = self._extract_text(article_tag)
            else:
                # mainã‚¿ã‚°
                main_tag = soup.find('main')
                if main_tag:
                    content = self._extract_text(main_tag)
                else:
                    # bodyå…¨ä½“ã‹ã‚‰ä¸è¦ãªè¦ç´ ã‚’é™¤å»
                    body = soup.find('body')
                    if body:
                        # ã‚¹ã‚¯ãƒªãƒ—ãƒˆã€ã‚¹ã‚¿ã‚¤ãƒ«ã€ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ãªã©ã‚’é™¤å»
                        for tag in body.find_all(['script', 'style', 'nav', 'header', 'footer', 'aside']):
                            tag.decompose()
                        content = self._extract_text(body)
            
            # å…¬é–‹æ—¥æ™‚ã®å–å¾—
            published_at = None
            time_tag = soup.find('time')
            if time_tag:
                datetime_attr = time_tag.get('datetime') or time_tag.get_text()
                try:
                    published_at = datetime.fromisoformat(datetime_attr.replace('Z', '+00:00'))
                except:
                    pass
            
            if not title:
                print(f"âš ï¸ ã‚¿ã‚¤ãƒˆãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {url}")
                return None
            
            if not content or len(content) < 100:
                print(f"âš ï¸ ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒçŸ­ã™ãã¾ã™: {url}")
                return None
            
            article = {
                'url': url,
                'title': title,
                'content': content[:5000],  # æœ€åˆã®5000æ–‡å­—
                'published_at': published_at
            }
            
            print(f"âœ… è¨˜äº‹å–å¾—å®Œäº†: {title[:50]}...")
            return article
            
        except Exception as e:
            print(f"âš ï¸ è¨˜äº‹å–å¾—ã‚¨ãƒ©ãƒ¼ ({url}): {e}")
            return None
    
    def fetch_from_urls(self, urls: List[str], delay: float = 1.0) -> List[Dict]:
        """
        è¤‡æ•°URLã‹ã‚‰è¨˜äº‹ã‚’å–å¾—
        
        Args:
            urls: URLã®ãƒªã‚¹ãƒˆ
            delay: ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“ã®é…å»¶ï¼ˆç§’ï¼‰
        
        Returns:
            è¨˜äº‹ã®ãƒªã‚¹ãƒˆ
        """
        articles = []
        
        for i, url in enumerate(urls):
            article = self.fetch_from_url(url)
            if article:
                articles.append(article)
            
            # æœ€å¾Œã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆä»¥å¤–ã¯é…å»¶
            if i < len(urls) - 1:
                time.sleep(delay)
        
        return articles
    
    def _extract_text(self, element) -> str:
        """
        HTMLè¦ç´ ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º
        
        Args:
            element: BeautifulSoupè¦ç´ 
        
        Returns:
            æŠ½å‡ºã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ
        """
        if not element:
            return ""
        
        # ã‚³ãƒ”ãƒ¼ã‚’ä½œæˆã—ã¦æ“ä½œ
        element = element.__copy__()
        
        # ä¸è¦ãªã‚¿ã‚°ã‚’é™¤å»
        for tag in element.find_all(['script', 'style', 'nav', 'header', 'footer', 'aside', 'iframe']):
            tag.decompose()
        
        # ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
        text = element.get_text(separator='\n', strip=True)
        
        # ä½™åˆ†ãªç©ºç™½ã‚’æ•´ç†
        text = re.sub(r'\n\s*\n', '\n\n', text)
        text = re.sub(r' +', ' ', text)
        
        return text.strip()
    
    def _clean_html(self, html: str) -> str:
        """
        HTMLã‚¿ã‚°ã‚’é™¤å»ã—ã¦ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ã‚’æŠ½å‡º
        
        Args:
            html: HTMLæ–‡å­—åˆ—
        
        Returns:
            ã‚¯ãƒªãƒ¼ãƒ³ãªãƒ†ã‚­ã‚¹ãƒˆ
        """
        if not html:
            return ""
        
        soup = BeautifulSoup(html, 'html.parser')
        return self._extract_text(soup)


class RSSFeedManager:
    """RSSãƒ•ã‚£ãƒ¼ãƒ‰ç®¡ç†ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.fetcher = ArticleFetcher()
        self.feeds = []  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆ
    
    def add_feed(self, rss_url: str, max_items: int = 10):
        """
        RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‚’è¿½åŠ 
        
        Args:
            rss_url: RSSãƒ•ã‚£ãƒ¼ãƒ‰ã®URL
            max_items: å–å¾—ã™ã‚‹æœ€å¤§è¨˜äº‹æ•°
        """
        self.feeds.append({
            'url': rss_url,
            'max_items': max_items
        })
    
    def fetch_all_feeds(self) -> List[Dict]:
        """
        ç™»éŒ²ã•ã‚Œã¦ã„ã‚‹ã™ã¹ã¦ã®RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰è¨˜äº‹ã‚’å–å¾—
        
        Returns:
            è¨˜äº‹ã®ãƒªã‚¹ãƒˆ
        """
        all_articles = []
        
        for feed_config in self.feeds:
            articles = self.fetcher.fetch_from_rss(
                feed_config['url'],
                feed_config['max_items']
            )
            all_articles.extend(articles)
            
            # ãƒ•ã‚£ãƒ¼ãƒ‰é–“ã®é…å»¶
            time.sleep(1.0)
        
        return all_articles


# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®RSSãƒ•ã‚£ãƒ¼ãƒ‰ï¼ˆä¾‹ï¼‰
DEFAULT_FEEDS = [
    # æŠ€è¡“ç³»ãƒ‹ãƒ¥ãƒ¼ã‚¹
    {'url': 'https://techcrunch.com/feed/', 'max_items': 5},
    {'url': 'https://www.theverge.com/rss/index.xml', 'max_items': 5},
    # æ—¥æœ¬èªæŠ€è¡“ç³»
    {'url': 'https://zenn.dev/feed', 'max_items': 5},
    {'url': 'https://qiita.com/feed', 'max_items': 5},
]


def get_default_feed_manager() -> RSSFeedManager:
    """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã®RSSãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã‚’å–å¾—"""
    manager = RSSFeedManager()
    for feed in DEFAULT_FEEDS:
        manager.add_feed(feed['url'], feed['max_items'])
    return manager

